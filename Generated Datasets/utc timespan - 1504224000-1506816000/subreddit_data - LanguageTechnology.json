{
    "active_user_count": 7, 
    "advertiser_category": null, 
    "audience_target": "", 
    "avg_comment_num_per_submission": 2, 
    "avg_submission_score": 6, 
    "collection_range_end_unix_timestamp": 1506816000, 
    "collection_range_end_utc": "2017-10-01 00:00:00", 
    "collection_range_start_unix_timestamp": 1504224000, 
    "collection_range_start_utc": "2017-09-01 00:00:00", 
    "description": "Articles on natural language processing.", 
    "display_name": "LanguageTechnology", 
    "domain_occurrences": {
        "allennlp.org": 1, 
        "approximatelycorrect.com": 1, 
        "arxiv.org": 2, 
        "blog.aylien.com": 1, 
        "catswhisker.xyz": 1, 
        "gfrison.com": 1, 
        "github.com": 1, 
        "mfstrategies.com": 1, 
        "nlp.stanford.edu": 1, 
        "nlposs.github.io": 1, 
        "self.LanguageTechnology": 31, 
        "stats.stackexchange.com": 1
    }, 
    "id": "2rkr2", 
    "num_external_website_posts": 12, 
    "num_text_posts": 31, 
    "public_description": "", 
    "submissions": [
        {
            "author": "hubbs5", 
            "created_utc": 1506775106.0, 
            "domain": "self.LanguageTechnology", 
            "id": "73eo7y", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/73eo7y/document_classification_and_satoshi_nakamoto/", 
            "score": 6, 
            "selftext": "I haven't done much with document classification before, so I decided to get my hands dirty with an interesting problem - who is Satoshi Nakamoto? To do that, I went ahead and scraped the sites of a handful of potential candidates for Nakamoto and put together a classifier to compare his written work with theirs (all the details can be found here: https://www.datahubbs.com/can-machine-learning-unmask-satoshi-nakamoto/). I tried to lay out my method as clearly as possible because I've never worked with text before and would like to get some feedback. I tried to highlight some of the weaknesses of the analysis in the post, but I'm sure I'm missing others. I'd like to follow up and make some improvements, so any thoughts would be appreciated!", 
            "subreddit": "LanguageTechnology", 
            "title": "Document Classification and Satoshi Nakamoto", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/73eo7y/document_classification_and_satoshi_nakamoto/"
        }, 
        {
            "author": "roemmele", 
            "created_utc": 1506722796.0, 
            "domain": "self.LanguageTechnology", 
            "id": "73b4bd", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/73b4bd/experimental_app_for_creative_language_generation/", 
            "score": 4, 
            "selftext": "Looking for people to try it out: https://fiction.ict.usc.edu/creativehelp/", 
            "subreddit": "LanguageTechnology", 
            "title": "experimental app for creative language generation", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/73b4bd/experimental_app_for_creative_language_generation/"
        }, 
        {
            "author": "joshman108", 
            "created_utc": 1506709849.0, 
            "domain": "self.LanguageTechnology", 
            "id": "739r3e", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/739r3e/replace_numbers_with_alpha_alias/", 
            "score": 1, 
            "selftext": "This is my first run at nlp. Is there an existing function in any language, preferably python or vba, that automatically replaces numbers with aliases? I could code something and I certainly may - my concern is simply whether I am missing any unique number contexts which I have not yet encountered. An existing function would help with that part. Below are some of my search terms and desired replacements. I had initially started finding/replacing my hand in excel but quickly realized how many possibilities there are. Something automatic would be nice.  Any ideas? \n\nby (number), you will have\u2026  where number is age and is followed by event.\n\n911\n\n(number) years\n\n(number) months \n\n(number) weeks \n\n(number) days \n\n\n100% \n\nxx% tip... where xx% is a tip amount\n\nin 2017\n\nby (number) where number is a time of day\n\nxx% where the percent is anything other than 100%, as 100% typically has a distinct meaning and context.\n\n$xx where x is some dollar amount and may be more or less than 2 digits and contains commas and decimals\n\n in (number) where number is year other than 2017\n\n(number) years old\n\nI have selected the corresponding replacement values for each instance in text:\n\n\u2018myage\u2019 (myage)\n\n\u2018nnn\u2019 (nine one one)\n\n\u2018sy\u2019 (some years)\n\n\u2018sm\u2019 (some months)\n\n\u2018sw\u2019 (some weeks)\n\n\u2018sd\u2019 (some days)\n\n\u2018hpct\u2019 (hundred percent)\n\n\u2018tpmt\u2019 (tip amount)\n\n\u2018crtyr\u2019 (current year)\n\n\u2018st\u2019  (some time)\n\n\u2018smp\u2019  (some percent)\n\n\u2018sdd\u2019 (some dollar)\n\n \u2018smcy\u2019 (some calendar year)\n\n \u2018sy\u2019 some years \n\n", 
            "subreddit": "LanguageTechnology", 
            "title": "Replace numbers with alpha alias?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/739r3e/replace_numbers_with_alpha_alias/"
        }, 
        {
            "author": "findandwrite", 
            "created_utc": 1506608843.0, 
            "domain": "self.LanguageTechnology", 
            "id": "730fb0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/730fb0/beautiful_soup_justtext_dragnet_not_super_happy/", 
            "score": 10, 
            "selftext": "I'm working on a project that pulls text from sites. At one point or another, I've used all of these packages and I feel like I find instances where they let me down. \n\nIs there any advice you guys can give me on a strong way pull text from pages?\n\nMaybe running them all and having a way to dynamically choose among their outputs???\n\nNot really sure tho", 
            "subreddit": "LanguageTechnology", 
            "title": "Beautiful soup? justtext? dragnet? Not super happy with any of these. Maybe I'm using them wrong? Advice on the best way to reliably pull text from a large number of varied websites. Beginner ooking for advice from the veteran data miners.", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/730fb0/beautiful_soup_justtext_dragnet_not_super_happy/"
        }, 
        {
            "author": "kuro-kuris", 
            "created_utc": 1506593567.0, 
            "domain": "self.LanguageTechnology", 
            "id": "72z5gc", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/72z5gc/explainable_document_classification/", 
            "score": 4, 
            "selftext": "Hi,\nI am currently working on identifying vulnerable customers (ill, learning challenges, addiction, etc.) from communications with companies. It is quite important that the models created are explainable.\nOther than decision trees what are my options for explainable models? I was thinking about trying a neural model with an attention mechanism and returning an interpretation of the attention mechanism as a justification of the decision.", 
            "subreddit": "LanguageTechnology", 
            "title": "Explainable document classification", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/72z5gc/explainable_document_classification/"
        }, 
        {
            "author": "adammathias", 
            "created_utc": 1506583041.0, 
            "domain": "approximatelycorrect.com", 
            "id": "72yj3f", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/72yj3f/a_random_walk_through_emnlp_2017/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "A Random Walk Through EMNLP 2017", 
            "url": "http://approximatelycorrect.com/2017/09/26/a-random-walk-through-emnlp-2017/"
        }, 
        {
            "author": "czechrepublic", 
            "created_utc": 1506494444.0, 
            "domain": "self.LanguageTechnology", 
            "id": "72qjer", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/72qjer/is_there_a_place_where_i_can_find_problem_sets/", 
            "score": 14, 
            "selftext": "I am taking an NLP course in my college, and I would like to find some problem sources related to basic n-gram probability, smoothing, POS tagging, and some other intro-level NLP topics.\n\nAny kind of source would be appreciated. Thanks!", 
            "subreddit": "LanguageTechnology", 
            "title": "Is there a place where I can find problem sets for basic NLP ?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/72qjer/is_there_a_place_where_i_can_find_problem_sets/"
        }, 
        {
            "author": "cognitivedemons", 
            "created_utc": 1506485374.0, 
            "domain": "arxiv.org", 
            "id": "72pw31", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/72pw31/snet_from_answer_extraction_to_answer_generation/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension", 
            "url": "https://arxiv.org/abs/1706.04815"
        }, 
        {
            "author": "iknowpeoplewhoknow", 
            "created_utc": 1506452590.0, 
            "domain": "self.LanguageTechnology", 
            "id": "72mlpa", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/72mlpa/this_text_viz_helps_you_see_top_issues_being/", 
            "score": 9, 
            "selftext": "This has been created using [force-directed graphs](https://bl.ocks.org/mbostock/4062045) and Google News. It's updated ~ realtime. We created it because you can search Google Trends for hadoop, but you won't get results when you drill down into eg: manufacturing. See top issues being discussed in a topic of interest at senseihub.com! Here are a few more examples: \n* [human capital](https://www.senseihub.com/human capital)\n* [biohacking](https://www.senseihub.com/biohacking)\n* [exoskeleton](https://www.senseihub.com/exoskeleton)", 
            "subreddit": "LanguageTechnology", 
            "title": "This text viz helps you see top issues being discussed in any topic from hundreds of google news results!", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/72mlpa/this_text_viz_helps_you_see_top_issues_being/"
        }, 
        {
            "author": "amirouche", 
            "created_utc": 1506189935.0, 
            "domain": "nlposs.github.io", 
            "id": "71zujt", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/71zujt/workshop_for_nlp_open_source_software/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Workshop for NLP Open Source Software", 
            "url": "https://nlposs.github.io/"
        }, 
        {
            "author": "theology_", 
            "created_utc": 1506009777.0, 
            "domain": "self.LanguageTechnology", 
            "id": "71k4ae", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/71k4ae/with_our_current_understanding_of_natural/", 
            "score": 22, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "With our current understanding of natural language processing, could we theoretically create a program that can read a wikipedia page and create a knowledge graph out of it (similar to wikidata)?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/71k4ae/with_our_current_understanding_of_natural/"
        }, 
        {
            "author": "2ndwoodsman", 
            "created_utc": 1505997225.0, 
            "domain": "github.com", 
            "id": "71it3o", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/71it3o/starspace_learning_embeddings_for_classification/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Starspace - Learning embeddings for classification, retrieval and ranking.", 
            "url": "https://github.com/facebookresearch/Starspace"
        }, 
        {
            "author": "TheUndead96", 
            "created_utc": 1505983560.0, 
            "domain": "self.LanguageTechnology", 
            "id": "71hu65", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/71hu65/question_nonwhitespace_language_recognition/", 
            "score": 3, 
            "selftext": "Hello everyone,\n\nI would like to know if there are any established methods for determining whether a collection of characters is a sentence in natural language, but with whitespace removed.\n\nThat is, if we had the sentence:\n\n\"thisisasentenceinenglish\"\n\nThis would be recognised as natural language, however the sentence:\n\n\"strinimaaluxtosoripo\"\n\nWould not (even though it might exhibit patterns of a natural language).\n\nI only need the algorithm to recognise English. I have thought of ways to do this using a dictionary, but my idea is essentially to interleave whitespace back into the string constructively. This would essentially entail adding spaces around the longest collections of characters that map to English words in a dictionary. This might, however, require backtracking, and seems like an inefficient/inelegant solution in general.\n\nThis obviously would not scale well with the length of the string. But is this as good as we can do?\n\nThanks in advance for any help!", 
            "subreddit": "LanguageTechnology", 
            "title": "Question: Non-whitespace Language Recognition", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/71hu65/question_nonwhitespace_language_recognition/"
        }, 
        {
            "author": "inejc", 
            "created_utc": 1505933036.0, 
            "domain": "self.LanguageTechnology", 
            "id": "71d7j7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/71d7j7/project_a_pytorch_implementation_of_paragraph/", 
            "score": 12, 
            "selftext": "I'm implementing a library for training paragraph vector models as proposed by Q. V. Le et al. (Distributed Representations of Sentences and Documents).\n\nThe code is available on GitHub: https://github.com/inejc/paragraph-vectors\n\nI would appreciate any kind of feedback. Contributions in any form are also more than welcome (I have already opened some issues regarding future work).", 
            "subreddit": "LanguageTechnology", 
            "title": "[Project] A PyTorch implementation of Paragraph Vectors (doc2vec)", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/71d7j7/project_a_pytorch_implementation_of_paragraph/"
        }, 
        {
            "author": "cs_on_detours", 
            "created_utc": 1505813532.0, 
            "domain": "self.LanguageTechnology", 
            "id": "711u7v", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/711u7v/question_cluster_conversational_text_based_on/", 
            "score": 5, 
            "selftext": "Hi guys,\nhow would one go about clustering conversational text by topic? The text is unlabeled but some have named entites.\n\nI've tried an average weighted TF-IDF with w2v and got ok results. My next step would be to try Doc2Vec. For now I've ignored the entities, didn't do anything special with them.\n\nBut I have one problem with it, let's say I have a long group chat and last week we've talked about the birthday of Tim and today we're talking about the birthday of Mike. With Doc2Vec and my TF-IDF approach I would put these two topics in the same cluster.\n\nIs there a way to separate my previous example? I was thinking about including a time factor, but I'm not sure this would be a good approach and how to go about it.", 
            "subreddit": "LanguageTechnology", 
            "title": "Question: Cluster conversational text based on topics", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/711u7v/question_cluster_conversational_text_based_on/"
        }, 
        {
            "author": "eggman-or-walrus", 
            "created_utc": 1505747060.0, 
            "domain": "self.LanguageTechnology", 
            "id": "70vkql", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/70vkql/what_else_can_we_do_to_improve_classification/", 
            "score": 7, 
            "selftext": "I come into a problem where there are certain features (words) that appear in all the classes and this seems to make my classifier confused on classifying the documents. As a result, when I use certain classifiers (Multinomial Naive Bayes, SVM, etc) the classifier fails to predict the right classes of the documents and instead it classifies all the documents to one certain class (which is very dominant in my case, it's quite imbalance dataset). Strangely, when I use something really simple like KNN, it can predict better (from the confusion matrix, I know all the data are no longer classified to one certain class anymore), albeit still low in accuracy (around 60%). I figure there must be some ways you can do to improve accuracy? \n\n- N-grams, I did bigrams and trigrams to my classifier, it doesn't improve the accuracy much.\n\n- Eliminating terms that only appear less than certain number in the documents. I limit it to min_df=5 and it reduces a great portion of features, from like 100k to only 2k, but with similar accuracy. I am quite impressed by this.\n\nWhat else can I do? I am actually thinking of something like feature selection (information gain, chi-square) but I haven't found any tutorial detailing about this on python. And I have no idea where to begin. Could anyone here suggest me some other methods? Thanks!", 
            "subreddit": "LanguageTechnology", 
            "title": "What else can we do to improve classification accuracy?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/70vkql/what_else_can_we_do_to_improve_classification/"
        }, 
        {
            "author": "yungfilleh", 
            "created_utc": 1505745865.0, 
            "domain": "blog.aylien.com", 
            "id": "70vg3s", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/70vg3s/highlights_of_emnlp_2017_exciting_datasets_return/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Highlights of EMNLP 2017 - Exciting Datasets, Return of the Clusters, and more!", 
            "url": "http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters"
        }, 
        {
            "author": "eggman-or-walrus", 
            "created_utc": 1505658426.0, 
            "domain": "self.LanguageTechnology", 
            "id": "70nrl8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/70nrl8/what_is_featuresets_can_someone_eli5_please/", 
            "score": 5, 
            "selftext": "I am trying to learn NLP with python on my own and now I feel I have been stuck and frustrated. Wherever I go, I will read the code about featuresets, from blogs or youtube tutorials on NLP. But they never really explain what it is and why it's important. What is it really? I don't usually use it and there hasn't been any problem so far. Usually I do the preprocessing stuff with NLTK then put the data straight to TfIdfVectorizer, then put it in a classifier. It works fine and I get my accuracy just fine. But now I bump into a case where my accuracy is really low and I wonder if not using this 'featuresets' is what causes it. What is the difference between having 'featuresets' and not? \n\nI hope this is clear enough, but if it is not, please tell me and I will edit the question. Thanks!", 
            "subreddit": "LanguageTechnology", 
            "title": "What is 'featuresets'? Can someone ELI5 please?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/70nrl8/what_is_featuresets_can_someone_eli5_please/"
        }, 
        {
            "author": "yungfilleh", 
            "created_utc": 1505490704.0, 
            "domain": "nlp.stanford.edu", 
            "id": "70apk2", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/70apk2/stanford_nlp_release_tacred_a_supervised_dataset/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Stanford NLP release TACRED: a supervised dataset for relation extraction (120k examples)", 
            "url": "https://nlp.stanford.edu/pubs/zhang2017tacred.pdf?utm_content=60249707&utm_medium=social&utm_source=twitter"
        }, 
        {
            "author": "Matisseio", 
            "created_utc": 1505435611.0, 
            "domain": "self.LanguageTechnology", 
            "id": "706f42", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/706f42/question_about_the_combinatorial_factor_for/", 
            "score": 4, 
            "selftext": "I'm doing some analysis on tweets, and I need a better way to create a custom list of stopwords.\n\nI came across this paper which shows an equation on the third page that I can't quite understand: https://arxiv.org/ftp/arxiv/papers/1205/1205.6396.pdf\n\nI understand what the end result is supposed to achieve, but I'm not well-versed in combinatorics, so it's a little confusing when I try to write it out and plug everything in by hand.\n\nThe result of using this computation is that it's supposed to give you an idea of stopwords within the domain you're looking at by evaluating words with lots of possible combinations.\n\nThe example the author uses is if you're looking at tweets (or any documents) about Linear Algebra, the words \"Linear Algebra\" and \"Complement Set\" probably have little variation, whereas words such as \"in the\" and \"that is\" have a lot of variation, and should be considered as stop words since they are less meaningful.\n\nHere's a quick screencap of the equation in question: https://imgur.com/a/ccjax\n\nI'm getting hung up on how the tweets would be broken down into words fed into the indicator function, and then summed to determine the variation of the word in question. I'm assuming we are looking for a high score here, but not sure how to derive it.\n\nPerhaps using these three examples tweets to explain:\n\n* Tweet 1 = \"of course the first class I accidentally sleep through was my first tutorial whelp, that's a full % of my linear algebra mark\"\n* Tweet 2 = \"Learning linear algebra: skipping class to read the textbook because you can't understand your professor.\"\n* Tweet 3 = \"Linear algebra is going to be the reason why I have a mental breakdown this semester.\"", 
            "subreddit": "LanguageTechnology", 
            "title": "Question about \"The Combinatorial Factor\" for analyzing stopwords in tweets", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/706f42/question_about_the_combinatorial_factor_for/"
        }, 
        {
            "author": "HD187123b", 
            "created_utc": 1505412530.0, 
            "domain": "self.LanguageTechnology", 
            "id": "70416e", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/70416e/question_matching_via_deep_learning/", 
            "score": 7, 
            "selftext": "Say you have two questions\n\u201cWhat are the best ways to lose weight?\u201d,\n\u201cWhat are effective weight loss plans?\u201d\n\nAnd you want to determine if these are in fact the same question just with different phrasing. What are some good papers / github repos that tackle this problem?\n\nI stumbled across this post by quora (quora engineering):\nhttps://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\n\nHowever I'm finding that their answer is not all that detailed.", 
            "subreddit": "LanguageTechnology", 
            "title": "Question matching via deep learning", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/70416e/question_matching_via_deep_learning/"
        }, 
        {
            "author": "czechrepublic", 
            "created_utc": 1505342845.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6zy7fo", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6zy7fo/question_about_doing_an_undergradlevel_nlp/", 
            "score": 5, 
            "selftext": "I am a senior in college and I got to do an NLP research with a professor. But he wants us to find the project by ourselves. I was wondering what could be a good NLP project to work on as an undergrad and a non-NLP expert. \n\nTwo specific questions I have are:\n\n1) How do we know what kinds of available projects are out there?\n\n2) How do we know if the project(or task) still has a space to improve?\n\n3) How do we know if it's an workable project for an undergrad student?\n\nAny kind of advice would be great. Thanks! ", 
            "subreddit": "LanguageTechnology", 
            "title": "Question about doing an undergrad-level NLP research.", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6zy7fo/question_about_doing_an_undergradlevel_nlp/"
        }, 
        {
            "author": "varmiss", 
            "created_utc": 1505313716.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6zuyvo", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6zuyvo/course_or_material_on_corpus_linguistics/", 
            "score": 1, 
            "selftext": "Hey, I'm searching for some resources for studying some basic principles of corpus linguistics since I'm probably going to need some in my study.\n\nDo you know any online courses or materials? Any books you recommend?", 
            "subreddit": "LanguageTechnology", 
            "title": "Course or material on corpus linguistics", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6zuyvo/course_or_material_on_corpus_linguistics/"
        }, 
        {
            "author": "DontVoteForMe", 
            "created_utc": 1505229644.0, 
            "domain": "mfstrategies.com", 
            "id": "6znmie", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6znmie/using_nlp_to_build_a_trustworthy_twitter_bot/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Using NLP to build a trustworthy Twitter bot", 
            "url": "http://www.mfstrategies.com/blog/2017/9/11/we-need-to-talk-about-kevin"
        }, 
        {
            "author": "racinecouscous", 
            "created_utc": 1505225674.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6zn7fo", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6zn7fo/data_set_for_french_canadian/", 
            "score": 1, 
            "selftext": "Do yo know if there is any dataset of texts written in \"spoken\" french canadian? I mean, texts using typical french canadian words, like \"chum\", \"chicanner\", etc. ", 
            "subreddit": "LanguageTechnology", 
            "title": "Data set for french canadian", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6zn7fo/data_set_for_french_canadian/"
        }, 
        {
            "author": "kuro-kuris", 
            "created_utc": 1505209496.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6zly84", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6zly84/state_of_the_art_named_entity_recognition/", 
            "score": 13, 
            "selftext": "What is the state of the art in named entity recogntion currently? I am looking in particular for time, date extraction and monetary value extraction through numbers and currencies. Without a large labelled data set how could you evaluate named entity recognition system? Just looking at papers?", 
            "subreddit": "LanguageTechnology", 
            "title": "State of the art named entity recognition?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6zly84/state_of_the_art_named_entity_recognition/"
        }, 
        {
            "author": "Molag_Balls", 
            "created_utc": 1505173757.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6zja3t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6zja3t/hybrid_code_networks/", 
            "score": 1, 
            "selftext": "Yes, like the rest of the world I'm looking to implement a (potentially rather complicated) chat bot. Sue me. \n\nI suffer from extreme analysis paralysis so I'm looking to feed my addiction to thinking too much with some academic inspiration.\n\nI recently read Williams' paper on [hybrid code networks](https://arxiv.org/abs/1702.03274) and thought it was really good, and I'm just wondering if anyone can recommend any similar work in the space of dialogue management.\n\nSpecifically as it relates to:\n\n* dialogue state tracking\n* reinforcement learning with imitation\n* wizard of oz simulations\n* pretty much any method of reducing the necessary amount of training data \n\nI'm particularly interested in ways one might start as a hand-coded expert system and transition into an RL/ML based end to end system as seamlessly as possible.\n\nObviously I don't expect any papers that address exactly all of these things, but some nudges in the right direction would be greatly appreciated.", 
            "subreddit": "LanguageTechnology", 
            "title": "Hybrid Code Networks", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6zja3t/hybrid_code_networks/"
        }, 
        {
            "author": "suriname0", 
            "created_utc": 1504921989.0, 
            "domain": "allennlp.org", 
            "id": "6yz57a", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6yz57a/allennlp_an_opensource_nlp_research_library_built/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "AllenNLP - An open-source NLP research library, built on PyTorch", 
            "url": "http://allennlp.org/"
        }, 
        {
            "author": "tocxy", 
            "created_utc": 1504894138.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6ywhw9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6ywhw9/word_length_prediction_via_ngram_analysis/", 
            "score": 4, 
            "selftext": "Hi!\n\nAssuming I have a long text without spacing, e.g. \"itwasaverylowfireindeednothingonsuchabitternight\". \n\nWithout using additional ressources (e.g., a large corpus), is it somehow possible to predict the word boundaries via n-gram analysis? For instance, to split this text into\"it\", \"was\", \"a\", \"very\", ...? \n\nedit:  To clarify, I assume a language model for an unknown language, without any training data. Just given a large text, is it somehow possible to segment/split the text into mostly meaningful words (e.g., based on n-grams)?", 
            "subreddit": "LanguageTechnology", 
            "title": "Word length prediction via ngram analysis", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6ywhw9/word_length_prediction_via_ngram_analysis/"
        }, 
        {
            "author": "cristoper", 
            "created_utc": 1504890649.0, 
            "domain": "catswhisker.xyz", 
            "id": "6yw4f2", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6yw4f2/a_first_excercise_in_natural_language_processing/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "A First Excercise in Natural Language Processing with Python: Counting Hapaxes", 
            "url": "http://catswhisker.xyz/log/2017/9/7/a_first_excercise_in_natural_language_processing_with_python_counting_hapaxes/"
        }, 
        {
            "author": "adammathias", 
            "created_utc": 1504789305.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6yn1v8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6yn1v8/how_to_get_the_similarity_between_languages/", 
            "score": 6, 
            "selftext": "Is there a good dataset or library or some trick for getting the similarity between languages, or n most similar languages?\n\n*Similarity* here means lexical and structural similarity for the purposes of translation and translation errors, especially into and out of English.\n\nFor example, one of the most similar languages to Basque would be Spanish, even though they are phylogenetically unrelated, because translation systems use similar training datasets for them, because they deal with things like English named entities in similar ways, because they have the same punctuation and casing rules, some flavour of T-V distinction, and use a similar character set.\n\nFor Kazakh I would put Russian, Turkish, Kyrgyz and Uzbek in the top 10 in terms of predictiveness for translation and translation quality. We should be biased a bit towards the big languages, so Uyghur and Turkmen are technically closer but it's really important to have Russian and Turkish in there, because those systems are working better and their data bleed into everything.  In fact, distance(a, b) need not be exactly equal to distance(b, a).\n\nOne solution would simply measures the distance (say, in km, or sq km of overlap) between the languages where their locations could be defined by centroids or multiple points or areas, because location is a really good proxy for these sorts of often areal features.\n\nAnother solution would give multiple dimensions, that can then be averaged, weighted or selected for the application.\n\nIt could also just be a hand-built dataset.\n\nOptions that will not work:\n\nword vectors  \nWe can get the word vector for the actual English string, like 'Ukrainian' or 'Ukrainian language' or even 'speaking Ukrainian' if our model was trained with word-level n-grams. This seems to work but is far from ideal. Some names are strictly for a language, others often refer to a country or nation.  'Persian' is also used for rugs and cats. Some languages have multiple names. 'Somali' and 'Indonesian' may be near to each other because they both often occur before 'pirates'.\n\nWALS  \nWALS - which is available as a Python lib: https://github.com/mayhewsw/wals - will not work out of the box, the results are simply nonsense for any kind of real-world application.\n\nFor example:\n\n`langsim.langsim('language.csv', 'rus', 0.5, False)`:\n\n> [(0.26315789473684215, 'Ladakhi:Bodic:Sino-Tibetan'), (0.26315789473684215, 'Latvian:Baltic:Indo-European'), (0.26315789473684215, 'Turkish:Turkic:Altaic'), (0.31578947368421051, 'Brahui:Northern Dravidian:Dravidian'), (0.31578947368421051, 'Burushaski:Burushaski:Burushaski'), (0.31578947368421051, 'Chuvash:Turkic:Altaic'), (0.31578947368421051, 'Greek (Modern):Greek:Indo-European'), (0.31578947368421051, 'Hindi:Indic:Indo-European'), (0.31578947368421051, 'Kannada:Southern Dravidian:Dravidian'), (0.31578947368421051, 'Khalkha:Mongolic:Altaic'), (0.31578947368421051, 'Malagasy:Barito:Austronesian'), (0.31578947368421051, 'Ndyuka:Creoles and Pidgins:other'), (0.36842105263157898, 'Abkhaz:Northwest Caucasian:Northwest Caucasian'), (0.36842105263157898, 'Amele:Madang:Trans-New Guinea'), (0.36842105263157898, 'Arabic (Egyptian):Semitic:Afro-Asiatic'), (0.36842105263157898, 'Basque:Basque:Basque'), (0.36842105263157898, 'Catalan:Romance:Indo-European'), (0.36842105263157898, 'Coos (Hanis):Coosan:Oregon Coast'), (0.36842105263157898, 'Hungarian:Ugric:Uralic'), (0.36842105263157898, 'Indonesian:Malayo-Sumbawan:Austronesian')]\n\n`langsim.langsim('language.csv', 'ukr', 0.5, False)`:\n\n>[(1.0, '!Xun (Ekoka):Ju-Kung:Kxa'), (1.0, '!X\u00f3\u00f5:Tu:Tu'), (1.0, \"'Are'are:Oceanic:Austronesian\"), (1.0, '//Ani:Khoe-Kwadi:Khoe-Kwadi'), (1.0, '/Xam:Tu:Tu'), (1.0, '=|Hoan:=|Hoan:Kxa'), (1.0, 'A-Pucikwar:Great Andamanese:Great Andamanese'), (1.0, 'Aari:South Omotic:Afro-Asiatic'), (1.0, 'Abau:Upper Sepik:Sepik'), (1.0, 'Abaza:Northwest Caucasian:Northwest Caucasian'), (1.0, 'Abenaki (Western):Algonquian:Algic'), (1.0, 'Abidji:Kwa:Niger-Congo'), (1.0, 'Abip\u00f3n:South Guaicuruan:Guaicuruan'), (1.0, 'Abkhaz:Northwest Caucasian:Northwest Caucasian'), (1.0, 'Abui:Greater Alor:Timor-Alor-Pantar'), (1.0, \"Abun:North-Central Bird's Head:West Papuan\"), (1.0, 'Acehnese:Malayo-Sumbawan:Austronesian'), (1.0, 'Achagua:Northern Arawakan:Arawakan'), (1.0, 'Achang:Burmese-Lolo:Sino-Tibetan'), (1.0, 'Acholi:Nilotic:Eastern Sudanic')]\n\n(x-post of https://linguistics.stackexchange.com/questions/25544/how-to-get-the-similarity-between-languages)", 
            "subreddit": "LanguageTechnology", 
            "title": "How to get the similarity between languages?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6yn1v8/how_to_get_the_similarity_between_languages/"
        }, 
        {
            "author": "ThomasAger", 
            "created_utc": 1504719487.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6yh5t7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6yh5t7/when_learning_a_vector_space_from_unstructured/", 
            "score": 3, 
            "selftext": "Hey, I'm wondering if I should be removing infrequent words (e.g. words that haven't occurred in at least 50 documents) from the [20 newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset before putting them into a [multi-dimensional scaling algorithm](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html) for dimensionality reduction. I haven't seen anyone mention removing infrequent terms in scientific papers as a preprocessing step, but in the scikit-learn package the [CountVectorizer] (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) (that converts text into term:frequency pairings) has these parameters on by default.\n\nShould I be removing infrequent words? If so, how many is reasonable for a dataset like the 20 newsgroups dataset?\n\nThanks for any help.", 
            "subreddit": "LanguageTechnology", 
            "title": "When learning a vector space from unstructured text using multi-dimensional scaling, should you remove infrequent words?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6yh5t7/when_learning_a_vector_space_from_unstructured/"
        }, 
        {
            "author": "sentient_machine", 
            "created_utc": 1504717046.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6ygw5j", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6ygw5j/anyone_here_familiar_with_document_frequency/", 
            "score": 5, 
            "selftext": "I think this might be the right sub, but if it's not I am sorry. It's still about NLP anyway. \n\nI'm still new to the topic and still trying to familiarize myself with a lot of things. Recently I got myself to learn about Tf-Idf, but apparently it's not enough for my research. I read about this feature selection technique called Df Thresholding. Basically, you use some kind of thresholds to decide whether certain features (terms) really contribute to the classification or not by looking at its document frequency (Df). I've tried to google it but every search result only points to Tf-Idf article and not the Df thresholding I am trying to learn. I wonder if you guys could link me to some articles or shed me some light on it. Thanks.  ", 
            "subreddit": "LanguageTechnology", 
            "title": "Anyone here familiar with Document Frequency Thresholding?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6ygw5j/anyone_here_familiar_with_document_frequency/"
        }, 
        {
            "author": "ahmed0627", 
            "created_utc": 1504674652.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6ydmmn", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6ydmmn/recommendation_best_book_to_learn_nlp_from/", 
            "score": 11, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Recommendation: Best book to learn NLP from", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6ydmmn/recommendation_best_book_to_learn_nlp_from/"
        }, 
        {
            "author": "VerySecretCactus", 
            "created_utc": 1504564024.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6y3okv", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6y3okv/is_plagiarism_detection_still_a_topic_of_serious/", 
            "score": 15, 
            "selftext": "If not plagiarism detection in general, is cross-language plagiarism detection still topical?", 
            "subreddit": "LanguageTechnology", 
            "title": "Is plagiarism detection still a topic of serious research, or has it mostly been solved to satisfaction?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6y3okv/is_plagiarism_detection_still_a_topic_of_serious/"
        }, 
        {
            "author": "iamjkdn", 
            "created_utc": 1504548677.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6y21qn", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6y21qn/parsing_multiple_questions_contained_in_one/", 
            "score": 4, 
            "selftext": "If a single query from the user contains multiple questions belonging to different categories, how can they be identified, split and parsed?\n\nEg -   \n\n    User - what is the weather now and tell me my next meeting  \n    Parser - {:weather => \"what is the weather\", :schedule => \"tell me my next meeting\"}  \n\n*Parser identifies the parts of sentences where the question belongs to two different categories*\n\n    User - show me hotels in san francisco for tomorrow that are less than $300 but not less than $200 are pet friendly have a gym and a pool with 3 or 4 stars staying for 2 nights and dont include anything that doesnt have wifi  \n    Parser - {:hotels => [\"show me hotels in san francisco\",  \n              \"for tomorrow\", \"less than $300 but not less than $200\",\n              \"pet friendly have a gym and a pool\",\n              \"with 3 or 4 stars\", \"staying for 2 nights\", \"with wifi\"]}\n\n*Parser identifies the question belonging to only one category but has additional steps for fine tuning the answer and created an array ordered according to the steps to take*\n\n               \n\n\nFrom what I can understand this requires a **sentence segmenter**, **multi-label classifier** and **co-reference resolution**\n\nBut the *sentence segementer* I have come across depend heavily on grammar, punctuations.   \n\n*Multi-label classifiers*, like a good trained naive bayes classifier works in most cases but since they are multi-label, most times output multiple categories for sentences which clearly belong to one class. Depending solely on the array outputs to check the labels present would fail.  \n\nIf used a multi-class classifier, that is also good to check the array output of probable categories but obviously they dont tell the different parts of the sentence much accurately, much less in what fashion to proceed with the next step.\n\n**As a first step, how can I tune sentence segmenter to correctly split the sentence without any strict grammar rules.** Good accuracy of this would help a lot in classification.\n\n*Please pardon me if this question comes across naive. It would be helpful if you could point out where to be correct. I have only started using reddit and trying to making sure I conform the the guidelines*", 
            "subreddit": "LanguageTechnology", 
            "title": "Parsing multiple questions contained in one single user query", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6y21qn/parsing_multiple_questions_contained_in_one/"
        }, 
        {
            "author": "mundada", 
            "created_utc": 1504545429.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6y1onc", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6y1onc/how_is_the_error_at_each_time_step_backpropagated/", 
            "score": 5, 
            "selftext": "My understanding:\n\nAssume we have encoded a sequence using RNN(lstm or gru). While decoding, decoder (RNN) will output some word at every time step. So at every time step we will have some error (since we have y_pred and y_true at time step t, let's say cross-entropy). If our output has length \"k\", then we have have \"k\" different error terms for a single training example with single pass through network.\n\nIt would be great if someone could explain from this point. From here on, I am not able to understand how the error is backpropagated\n\nSome more specific doubts :\nAt what time step does the backpropagation starts ?\nHow many times weights of encoder and decoder are updated for a single pass, if we have \"k\" error terms corresponding to \"k\" time steps ?  ", 
            "subreddit": "LanguageTechnology", 
            "title": "How is the error at each time step backpropagated in RNN (LSTMs, GRUs) encoder-decoder architecture for machine translation?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6y1onc/how_is_the_error_at_each_time_step_backpropagated/"
        }, 
        {
            "author": "gfrison", 
            "created_utc": 1504510824.0, 
            "domain": "gfrison.com", 
            "id": "6xyy3x", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6xyy3x/convolutional_neural_networks_for_text/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Convolutional Neural Networks for Text Classification outperforms Api.ai", 
            "url": "https://gfrison.com/2017/09/01/deeplearning-in-text-classification/"
        }, 
        {
            "author": "koormoosh", 
            "created_utc": 1504479306.0, 
            "domain": "stats.stackexchange.com", 
            "id": "6xwfba", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6xwfba/why_parametrize_dot_products_in_neural_networks_a/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "why parametrize dot products in neural networks (a paper specific question)", 
            "url": "https://stats.stackexchange.com/questions/301258/why-to-parametrize-dot-products-in-neural-networks"
        }, 
        {
            "author": "findandwrite", 
            "created_utc": 1504363536.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6xmd37", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6xmd37/i_have_a_basic_nlp_script_using_gensim_but_i/", 
            "score": 3, 
            "selftext": "So the purpose of the program is to get 15 documents from 15 different urls. Compute tf-idf + LDA for them, then grab a new document and compare it to the original 15 and display the results.\n\nTwo of the biggest requirements as I see them..\n\n1. I want to build a website interface for this program. I'd also like to be able to experiment with different stop word lists and lemmatizers as well add word2vec at some point. I want to be able to somewhat easily compare the results and performance of different NLP methods/libraries without needing to go out and get the original 15 documents again. \n\n2. I want to have a 'user login' function so that different users can pick up where they left off and see the all the results of their personal previous queries. \n\nThis indicates to me that I'll need some type of database - so I guess the first question would be, what database to do you guys prefer for storing raw text, preprocessed text, dictionaries, corpora and word vectors???\n\n(to be clear, if it was just raw text, i'd go with elasticsearch but with the addition of dictionaries and corpora, I'm not really sure how that changes the problem)\n\nGensim has a function that helps with saving corpora and dictionaries but I'm not sure how this should best be integrated with a database??\n\nThe other big question is about object oriented best practices.\n\nIn my head, I'm imagining I should make a class object like '15_document_corpus', and then give it attributes like 'raw_txt', 'preproccessed_text', etc, etc. \n\nAs well at methods like 'make_dict', 'make_corpus', 'compute_tfidt', etc.\n\nI feel like the benefit of that is that I can save and load the whole object when I want to visualize the data. I imagine this will also help with the user login feature when someone wants to load a previously processed corpus + data visulization.\n\n\n\nAgain though, I've never done anything like this before so these are all just wild guesses from someone totally inexperienced. \n\nAny help here is greatly appreciated!", 
            "subreddit": "LanguageTechnology", 
            "title": "I have a basic NLP script using gensim but I think it's written pretty poorly. Can someone advise me from the 10,000' view of how this should be designed? Ive never done this before so I'm really just taking shots in the dark here and would really appreciate some guidance.", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6xmd37/i_have_a_basic_nlp_script_using_gensim_but_i/"
        }, 
        {
            "author": "_murphys_law_", 
            "created_utc": 1504322044.0, 
            "domain": "arxiv.org", 
            "id": "6xjhdx", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6xjhdx/transfer_learning_across_lowresource_related/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation", 
            "url": "https://arxiv.org/abs/1708.09803v1"
        }, 
        {
            "author": "danieljj", 
            "created_utc": 1504294497.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6xgw37", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6xgw37/framenet_parser_in_java/", 
            "score": 1, 
            "selftext": "Does anyone know of any good FrameNet parser **written java**? ", 
            "subreddit": "LanguageTechnology", 
            "title": "FrameNet parser in Java?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6xgw37/framenet_parser_in_java/"
        }, 
        {
            "author": "adammathias", 
            "created_utc": 1504267897.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6xe3zj", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6xe3zj/rfc_on_language_python_package/", 
            "score": 6, 
            "selftext": "I'm opening up some very basic low-level building blocks that I use for working with text data.\n\nIt's intended to be used by code that processes text at scale and across many languages, so it doesn't pull in dependencies or large data files or do things that only work for English.  Often it's simple, but it is not something that should be implemented inside a machine learning pipeline.\n\nFor example, extracting n-grams from a string.  The usefulness of char-level n-grams is appreciated more and more, especially for morphologically rich languages, which is most of them, especially for languages with fewer data, which is most of them, for all types of applications.\n\nAnd self-contained code for extracting char-level n-grams is easy to hack together:\n\n> ng = lambda s, n: list(zip(*[s[i:] for i in range(n)]))  \n> ng('This is not a test.', 3)\n\n\nBut in practice we often only want n-grams within words, not across words.  And it makes sense for that kind of code to be open and common, to have more features and more eyes on it, and fewer names and conventions.\n\nSo I've put out the n-grams code in an initial release of the *language* Python package ([GitHub](https://github.com/SignalN/language) | [PyPI](https://pypi.python.org/pypi/language)).\n\n> pip install language\n\nI am seeking feedback on what belongs in there, what does not, what the structure should be and any other tips on developing and maintaining an open-source NLP library.\n\nFor now the basic structure is:  \n\n`language.languages` (coming soon)    \nBasic information on languages, for example equivalent and related ISO codes, and which chars are in which language.\n\n`language.chars`  \nMostly I want to expose and build on the unicodedata categories.  For example, finer-grained categories for things like quotation marks, and some conservative canonicalisations.\n\n`language.tokens`  \nDumb but robust language-agnostic tokenisation.  It should also take good guesses about so-called shapes - `\"3d\"`, `\"1st\"`, `\"https://youtube.com\"`, `\"don't\"`, `\"NamedEnt\"`, `\"ACRNYM\"`.\n\n`language.ngrams`  \nWord- and char-level n-grams and associated operations like diffing and matching.\n\nEach submodule depends only on the previous ones.\n", 
            "subreddit": "LanguageTechnology", 
            "title": "RFC on language Python package", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6xe3zj/rfc_on_language_python_package/"
        }
    ], 
    "subreddit_creation_utc": 1268268150.0, 
    "subscribers": 8179, 
    "title": "Natural Language Processing", 
    "title_word_count_occurrences": {
        "deep learning": 1, 
        "google": 1, 
        "java": 1, 
        "python": 2, 
        "tex": 5, 
        "twitter": 1
    }, 
    "top_score_submissions": [
        {
            "author": "theology_", 
            "created_utc": 1506009777.0, 
            "domain": "self.LanguageTechnology", 
            "id": "71k4ae", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/71k4ae/with_our_current_understanding_of_natural/", 
            "score": 22, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "With our current understanding of natural language processing, could we theoretically create a program that can read a wikipedia page and create a knowledge graph out of it (similar to wikidata)?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/71k4ae/with_our_current_understanding_of_natural/"
        }, 
        {
            "author": "suriname0", 
            "created_utc": 1504921989.0, 
            "domain": "allennlp.org", 
            "id": "6yz57a", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6yz57a/allennlp_an_opensource_nlp_research_library_built/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "AllenNLP - An open-source NLP research library, built on PyTorch", 
            "url": "http://allennlp.org/"
        }, 
        {
            "author": "VerySecretCactus", 
            "created_utc": 1504564024.0, 
            "domain": "self.LanguageTechnology", 
            "id": "6y3okv", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/6y3okv/is_plagiarism_detection_still_a_topic_of_serious/", 
            "score": 15, 
            "selftext": "If not plagiarism detection in general, is cross-language plagiarism detection still topical?", 
            "subreddit": "LanguageTechnology", 
            "title": "Is plagiarism detection still a topic of serious research, or has it mostly been solved to satisfaction?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/6y3okv/is_plagiarism_detection_still_a_topic_of_serious/"
        }, 
        {
            "author": "czechrepublic", 
            "created_utc": 1506494444.0, 
            "domain": "self.LanguageTechnology", 
            "id": "72qjer", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/72qjer/is_there_a_place_where_i_can_find_problem_sets/", 
            "score": 14, 
            "selftext": "I am taking an NLP course in my college, and I would like to find some problem sources related to basic n-gram probability, smoothing, POS tagging, and some other intro-level NLP topics.\n\nAny kind of source would be appreciated. Thanks!", 
            "subreddit": "LanguageTechnology", 
            "title": "Is there a place where I can find problem sets for basic NLP ?", 
            "url": "https://www.reddit.com/r/LanguageTechnology/comments/72qjer/is_there_a_place_where_i_can_find_problem_sets/"
        }, 
        {
            "author": "yungfilleh", 
            "created_utc": 1505745865.0, 
            "domain": "blog.aylien.com", 
            "id": "70vg3s", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/LanguageTechnology/comments/70vg3s/highlights_of_emnlp_2017_exciting_datasets_return/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "LanguageTechnology", 
            "title": "Highlights of EMNLP 2017 - Exciting Datasets, Return of the Clusters, and more!", 
            "url": "http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters"
        }
    ], 
    "total_submissions": 43, 
    "utc_of_data_collection_completion": "2017-10-17 18:43:23", 
    "utc_of_data_collection_start": "2017-10-17 18:43:22"
}