{
    "active_user_count": 12, 
    "advertiser_category": null, 
    "audience_target": "", 
    "avg_comment_num_per_submission": 1, 
    "avg_submission_score": 1, 
    "collection_range_end_unix_timestamp": 1506816000, 
    "collection_range_end_utc": "2017-10-01 00:00:00", 
    "collection_range_start_unix_timestamp": 1504224000, 
    "collection_range_start_utc": "2017-09-01 00:00:00", 
    "description": "Topics about Apache's Hadoop cloud platform and its related ecosystem - Pig, Hive, Cassandra, ZooKeeper\n\n----\nRelated subreddits:\n\n/r/Database\n\n/r/Datamining\n\n/r/BigData\n\n/r/CloudComputing\n\n/r/Nosql\n\n/r/MongoDB\n\n/r/DataSets\n\n/r/Intel\n", 
    "display_name": "hadoop", 
    "domain_occurrences": {
        "agiratech.com": 1, 
        "blog.cloudera.com": 1, 
        "medium.com": 1, 
        "self.hadoop": 4, 
        "stackoverflow.com": 7, 
        "tech.marksblogg.com": 1, 
        "youtube.com": 1
    }, 
    "id": "2r1e2", 
    "num_external_website_posts": 12, 
    "num_text_posts": 4, 
    "public_description": "", 
    "submissions": [
        {
            "author": "adanoopdixith", 
            "created_utc": 1506719335.0, 
            "domain": "youtube.com", 
            "id": "73arke", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": {
                "oembed": {
                    "author_name": "anoop dixith", 
                    "author_url": "https://www.youtube.com/user/dixithanoop", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/l_Wm6rIUNsQ?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/l_Wm6rIUNsQ/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Tech Like A Story: Big Data", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/73arke/over_the_past_few_months_a_lot_of_people_from/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "Over the past few months, a lot of people from non-technical background asked me questions like 'What are crypto-currencies?', 'How does SEO work?', 'How does MapReduce work?' etc. So I started this 'Tech, Like A Story' series to explain tech like a story! First one on Hadoop/MapReduce.", 
            "url": "https://www.youtube.com/watch?v=l_Wm6rIUNsQ"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1506687587.0, 
            "domain": "stackoverflow.com", 
            "id": "737feg", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/737feg/hadoop_job_formulation_passing_arrayliststring_to/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "Hadoop job formulation, passing ArrayList<String> to Mapper()", 
            "url": "https://stackoverflow.com/questions/46488701/hadoop-job-formulation-passing-arrayliststring-to-mapper"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1506545615.0, 
            "domain": "stackoverflow.com", 
            "id": "72v8yp", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/72v8yp/ioutils_issue_for_cat_of_data_from_hdfs_in_java/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "IOUtils issue for 'cat' of data from HDFS in Java program", 
            "url": "https://stackoverflow.com/questions/46457040/ioutils-issue-for-cat-of-data-from-hdfs-in-java-program"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1506439357.0, 
            "domain": "stackoverflow.com", 
            "id": "72l27p", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/72l27p/mapreduce_java_program_to_read_in_files_generate/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "mapreduce java program to read in files, generate redundant (word countable) terminal output", 
            "url": "https://stackoverflow.com/questions/46430337/mapreduce-java-program-to-read-in-files-generate-redundant-word-countable-ter"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1506386843.0, 
            "domain": "stackoverflow.com", 
            "id": "72gugu", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/72gugu/hadoop_job_to_issue_terminal_command_adapted_to/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "hadoop job to issue terminal command, adapted to hdfs files, reduce over console output", 
            "url": "https://stackoverflow.com/questions/46416179/hadoop-job-to-issue-terminal-command-adapted-to-hdfs-files-reduce-over-console"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1506370819.0, 
            "domain": "stackoverflow.com", 
            "id": "72f6m2", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/72f6m2/rewrite_java_program_as_a_hadoop_job/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "rewrite java program as a hadoop job", 
            "url": "https://stackoverflow.com/questions/46413537/rewrite-java-program-as-a-hadoop-job"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1506182884.0, 
            "domain": "stackoverflow.com", 
            "id": "71z4v0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/71z4v0/job_hangs_in_hadoop_compiled_submitted_accepted/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "job hangs in hadoop: compiled, submitted, accepted --> never terminates. wordcount example", 
            "url": "https://stackoverflow.com/questions/46381603/job-hangs-in-hadoop-compiled-submitted-accepted-never-terminates-wordcou"
        }, 
        {
            "author": "Littleish", 
            "created_utc": 1506156939.0, 
            "domain": "self.hadoop", 
            "id": "71x8o7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/71x8o7/getting_up_to_speed_with_hive_quickly/", 
            "score": 5, 
            "selftext": "I'm fully aware that this is a ridiculous post...\n\nI have limited analytical experience with hive and I'm a SQL developer. \n\nI've been asked to work along side a new team at work from Monday because they are struggling with Hive.\n\nI'm not sure what struggling with Hive means and they've been reluctant to give any information. I don't exactly work with stupid people so if a team has brought in outside resource they are likely going to be struggling with something fairly advanced.\n\nI'd like to do my best to be prepared but obviously it's hard to be prepared for a situation I know nothing about.\n\nI've looked at the syntax differences and I'm doing a hive course from udemy. It seems unlikely that my general SQL knowledge will be able to help.\n\nAny pointers or general resources it'd be good to get up to speed with?? Impossible question I know! ", 
            "subreddit": "hadoop", 
            "title": "Getting up to speed with hive quickly?", 
            "url": "https://www.reddit.com/r/hadoop/comments/71x8o7/getting_up_to_speed_with_hive_quickly/"
        }, 
        {
            "author": "GhenghisK", 
            "created_utc": 1506008522.0, 
            "domain": "self.hadoop", 
            "id": "71jz5t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/71jz5t/changing_hostname_in_working_cloudera_stack/", 
            "score": 1, 
            "selftext": "Does anyone have a working procedure to do this successfully. We've tried numerous links, with no success.. Using CDH5.7.x, with Accumulo, zookeeper, kafka, SOLR... Accumulo in particular doesnt work well with any attempted changes... ", 
            "subreddit": "hadoop", 
            "title": "Changing Hostname in working Cloudera Stack", 
            "url": "https://www.reddit.com/r/hadoop/comments/71jz5t/changing_hostname_in_working_cloudera_stack/"
        }, 
        {
            "author": "klieber", 
            "created_utc": 1505929313.0, 
            "domain": "self.hadoop", 
            "id": "71cr9h", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/71cr9h/security_patching_best_practices_for_os/", 
            "score": 3, 
            "selftext": "I'm not a hadoop expert, but rather work on the security side of things.  We've run into a situation where our hadoop team is stating they cannot apply monthly patches to our Red Hat environment that supports hadoop because of the way Red Hat releases patches.  (basically, they're bundled vs. individual, so they have to go through full regression testing which takes extensive time)\n\nI don't know enough about hadoop to really be of much help.  So, can anyone here offer some insights on OS patching for a ~500 node cluster?", 
            "subreddit": "hadoop", 
            "title": "Security patching best practices for OS vulnerabilities on hadoop servers", 
            "url": "https://www.reddit.com/r/hadoop/comments/71cr9h/security_patching_best_practices_for_os/"
        }, 
        {
            "author": "marklit", 
            "created_utc": 1505800079.0, 
            "domain": "tech.marksblogg.com", 
            "id": "710zla", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/710zla/11_billion_taxi_rides_with_spark_22_3_raspberry/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "1.1 Billion Taxi Rides with Spark 2.2 & 3 Raspberry Pi 3 Model Bs", 
            "url": "http://tech.marksblogg.com/billion-nyc-taxi-rides-spark-raspberry-pi.html"
        }, 
        {
            "author": "blueeyes44", 
            "created_utc": 1505743412.0, 
            "domain": "blog.cloudera.com", 
            "id": "70v6qm", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/70v6qm/how_to_predict_icu_mortality_with_digital_health/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "How To Predict ICU Mortality with Digital Health Data, DL4J, Apache Spark and Cloudera", 
            "url": "http://blog.cloudera.com/blog/2017/09/how-to-predict-icu-mortality-with-digital-health-data-dl4j-apache-spark-and-cloudera/"
        }, 
        {
            "author": "AgiraTechnologies", 
            "created_utc": 1505381117.0, 
            "domain": "agiratech.com", 
            "id": "7014ux", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/7014ux/how_to_setup_hadoop_280_single_node_cluster_on/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "How to Setup Hadoop 2.8.0 (Single Node Cluster) on CentOS", 
            "url": "http://www.agiratech.com/setup-hadoop-2-8-0-single-node-cluster-centos/"
        }, 
        {
            "author": "yeahilikefantasy", 
            "created_utc": 1504745880.0, 
            "domain": "stackoverflow.com", 
            "id": "6yjw05", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/6yjw05/any_help_with_a_tezhive_storage_config_question/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "Any help with a Tez/Hive storage config question?", 
            "url": "https://stackoverflow.com/questions/46086078/how-can-i-add-additional-libraries-to-tez-to-support-a-different-hdfs-backend"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1504666005.0, 
            "domain": "medium.com", 
            "id": "6ycwv4", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/6ycwv4/working_thru_hadoop_examples/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "Working Thru Hadoop Examples", 
            "url": "https://medium.com/@s.matthew.english/working-thru-hadoop-examples-449840469259"
        }, 
        {
            "author": "yanks09champs", 
            "created_utc": 1504495430.0, 
            "domain": "self.hadoop", 
            "id": "6xxt70", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/6xxt70/most_lightweight_hadoop_sandbox_out_there/", 
            "score": 2, 
            "selftext": "I am aware of the hortonworks and cloudera sandboxes each required >8gb of ram some even more. Was wondering do you of a light hadoop distro where I can get say hadoop spark hive and would take less than 4gb of ram .\n\nThanks", 
            "subreddit": "hadoop", 
            "title": "Most lightweight hadoop sandbox out there?", 
            "url": "https://www.reddit.com/r/hadoop/comments/6xxt70/most_lightweight_hadoop_sandbox_out_there/"
        }
    ], 
    "subreddit_creation_utc": 1246447083.0, 
    "subscribers": 4499, 
    "title": "All about the yellow elephant that powers the cloud", 
    "title_word_count_occurrences": {
        "apache": 1, 
        "hadoop": 9, 
        "java": 3
    }, 
    "top_score_submissions": [
        {
            "author": "adanoopdixith", 
            "created_utc": 1506719335.0, 
            "domain": "youtube.com", 
            "id": "73arke", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": {
                "oembed": {
                    "author_name": "anoop dixith", 
                    "author_url": "https://www.youtube.com/user/dixithanoop", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/l_Wm6rIUNsQ?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/l_Wm6rIUNsQ/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Tech Like A Story: Big Data", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/73arke/over_the_past_few_months_a_lot_of_people_from/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "Over the past few months, a lot of people from non-technical background asked me questions like 'What are crypto-currencies?', 'How does SEO work?', 'How does MapReduce work?' etc. So I started this 'Tech, Like A Story' series to explain tech like a story! First one on Hadoop/MapReduce.", 
            "url": "https://www.youtube.com/watch?v=l_Wm6rIUNsQ"
        }, 
        {
            "author": "Littleish", 
            "created_utc": 1506156939.0, 
            "domain": "self.hadoop", 
            "id": "71x8o7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/71x8o7/getting_up_to_speed_with_hive_quickly/", 
            "score": 5, 
            "selftext": "I'm fully aware that this is a ridiculous post...\n\nI have limited analytical experience with hive and I'm a SQL developer. \n\nI've been asked to work along side a new team at work from Monday because they are struggling with Hive.\n\nI'm not sure what struggling with Hive means and they've been reluctant to give any information. I don't exactly work with stupid people so if a team has brought in outside resource they are likely going to be struggling with something fairly advanced.\n\nI'd like to do my best to be prepared but obviously it's hard to be prepared for a situation I know nothing about.\n\nI've looked at the syntax differences and I'm doing a hive course from udemy. It seems unlikely that my general SQL knowledge will be able to help.\n\nAny pointers or general resources it'd be good to get up to speed with?? Impossible question I know! ", 
            "subreddit": "hadoop", 
            "title": "Getting up to speed with hive quickly?", 
            "url": "https://www.reddit.com/r/hadoop/comments/71x8o7/getting_up_to_speed_with_hive_quickly/"
        }, 
        {
            "author": "AgiraTechnologies", 
            "created_utc": 1505381117.0, 
            "domain": "agiratech.com", 
            "id": "7014ux", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/7014ux/how_to_setup_hadoop_280_single_node_cluster_on/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "How to Setup Hadoop 2.8.0 (Single Node Cluster) on CentOS", 
            "url": "http://www.agiratech.com/setup-hadoop-2-8-0-single-node-cluster-centos/"
        }, 
        {
            "author": "s-matthew-english", 
            "created_utc": 1506370819.0, 
            "domain": "stackoverflow.com", 
            "id": "72f6m2", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/72f6m2/rewrite_java_program_as_a_hadoop_job/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "hadoop", 
            "title": "rewrite java program as a hadoop job", 
            "url": "https://stackoverflow.com/questions/46413537/rewrite-java-program-as-a-hadoop-job"
        }, 
        {
            "author": "klieber", 
            "created_utc": 1505929313.0, 
            "domain": "self.hadoop", 
            "id": "71cr9h", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/hadoop/comments/71cr9h/security_patching_best_practices_for_os/", 
            "score": 3, 
            "selftext": "I'm not a hadoop expert, but rather work on the security side of things.  We've run into a situation where our hadoop team is stating they cannot apply monthly patches to our Red Hat environment that supports hadoop because of the way Red Hat releases patches.  (basically, they're bundled vs. individual, so they have to go through full regression testing which takes extensive time)\n\nI don't know enough about hadoop to really be of much help.  So, can anyone here offer some insights on OS patching for a ~500 node cluster?", 
            "subreddit": "hadoop", 
            "title": "Security patching best practices for OS vulnerabilities on hadoop servers", 
            "url": "https://www.reddit.com/r/hadoop/comments/71cr9h/security_patching_best_practices_for_os/"
        }
    ], 
    "total_submissions": 16, 
    "utc_of_data_collection_completion": "2017-10-16 18:50:46", 
    "utc_of_data_collection_start": "2017-10-16 18:50:46"
}