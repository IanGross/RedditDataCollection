{
    "active_user_count": 630, 
    "advertiser_category": "Technology", 
    "audience_target": "technology", 
    "avg_comment_num_per_submission": 13, 
    "avg_submission_score": 39, 
    "collection_range_end_unix_timestamp": 1506816000, 
    "collection_range_end_utc": "2017-10-01 00:00:00", 
    "collection_range_start_unix_timestamp": 1504224000, 
    "collection_range_start_utc": "2017-09-01 00:00:00", 
    "description": "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n--------\n+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n--------\n+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n--------\n+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n--------\n+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ANews)\n--------\n\n***[@slashML on Twitter](https://twitter.com/slashML)***\n\n--------\n**AMAs:**\n\n[Google Brain Team (9/17/2017)](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/)\n\n[Google Brain Team (8/11/2016)]\n(https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/)\n\n[The MalariaSpot Team (2/6/2016)](https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/)\n\n[OpenAI Research Team (1/9/2016)](http://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/)\n\n[Nando de Freitas (12/26/2015)](http://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/)\n\n[Andrew Ng and Adam Coates (4/15/2015)](http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/)\n\n[J\u00fcrgen Schmidhuber (3/4/2015)](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)\n\n[Geoffrey Hinton (11/10/2014)]\n(http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/)\n\n[Michael Jordan (9/10/2014)](http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)\n\n[Yann LeCun (5/15/2014)](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/)\n\n[Yoshua Bengio (2/27/2014)](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)\n\n--------\n**Beginners:**\n\nPlease have a look at [our FAQ and Link-Collection](http://www.reddit.com/r/MachineLearning/wiki/index)\n\n[Metacademy](http://www.metacademy.org) is a great resource which compiles lesson plans on popular machine learning topics.\n\nFor Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/\n\n--------\n\n[Advanced Courses](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses?st=isz2lqdk&sh=56c58cd6)\n\n--------\nRelated Subreddit :\n\n* [LearnMachineLearning](http://www.reddit.com/r/LearnMachineLearning)\n\n* [Statistics](http://www.reddit.com/r/statistics)\n\n* [Computer Vision](http://www.reddit.com/r/computervision)\n\n* [Compressive Sensing](http://www.reddit.com/r/CompressiveSensing/)\n\n* [NLP] (http://www.reddit.com/r/LanguageTechnology)\n\n* [ML Questions] (http://www.reddit.com/r/MLQuestions)\n\n* /r/MLjobs and /r/BigDataJobs\n\n* /r/datacleaning\n\n* /r/DataScience\n\n* /r/scientificresearch\n\n* /r/artificial", 
    "display_name": "machinelearning", 
    "domain_occurrences": {
        "abidlabs.github.io": 1, 
        "adamashwal.com": 1, 
        "allennlp.org": 1, 
        "alphagomovie.com": 1, 
        "appliedmldays.org": 1, 
        "artem.sobolev.name": 1, 
        "arxiv.org": 36, 
        "axios.com": 1, 
        "azure.microsoft.com": 1, 
        "bair.berkeley.edu": 1, 
        "bam4d.github.io": 1, 
        "blog.acolyer.org": 2, 
        "blog.dataversioncontrol.com": 1, 
        "blog.deepsense.ai": 1, 
        "blog.openai.com": 1, 
        "blog.qure.ai": 1, 
        "blog.sicara.com": 1, 
        "blog.statsbot.co": 2, 
        "blogs.nvidia.com": 1, 
        "blogs.unity3d.com": 1, 
        "bonaccorso.eu": 3, 
        "clclp.me": 1, 
        "cloudplatform.googleblog.com": 1, 
        "cognitivedemons.wordpress.com": 1, 
        "cs231n.stanford.edu": 1, 
        "datasciencecentral.com": 1, 
        "datawhatnow.com": 1, 
        "demos.algorithmia.com": 1, 
        "dev.to": 1, 
        "devblogs.nvidia.com": 1, 
        "developers.google.com": 1, 
        "displayr.com": 2, 
        "docs.google.com": 1, 
        "docs.microsoft.com": 1, 
        "dutchess.ai": 1, 
        "elen.ucl.ac.be": 1, 
        "eng.uber.com": 2, 
        "engineering.linkedin.com": 1, 
        "engineering.pivotal.io": 1, 
        "engineering.siftscience.com": 1, 
        "eurekalert.org": 2, 
        "experiments.adversarial.network": 1, 
        "faculty.ucmerced.edu": 1, 
        "fast.ai": 1, 
        "fastcompany.com": 1, 
        "gauthamzz.github.io": 1, 
        "ghrecommender.io": 1, 
        "gist.github.com": 1, 
        "github.com": 50, 
        "gmarti.gitlab.io": 2, 
        "groups.google.com": 1, 
        "hackernoon.com": 2, 
        "hameddaily.blogspot.de": 1, 
        "i.redd.it": 1, 
        "ihes.fr": 1, 
        "jasdeep06.github.io": 1, 
        "jungle-ml.com": 1, 
        "kdd.org": 1, 
        "kernels.io": 1, 
        "ku.cloud.panopto.eu": 1, 
        "lanl.gov": 1, 
        "learndatasci.com": 1, 
        "learningai.io": 1, 
        "lyrebird.ai": 1, 
        "machinelearning.apple.com": 1, 
        "math.stackexchange.com": 1, 
        "medium.com": 13, 
        "mfstrategies.com": 1, 
        "microsoft.com": 1, 
        "ml-showcase.com": 1, 
        "momixa.com": 1, 
        "myurasov.github.io": 1, 
        "nadbordrozd.github.io": 1, 
        "nih.gov": 1, 
        "nips.cc": 4, 
        "nips2017creativity.github.io": 1, 
        "nlml.github.io": 1, 
        "nlp.town": 1, 
        "nvdla.org": 1, 
        "nypost.com": 1, 
        "ofir.io": 1, 
        "papers.cumincad.org": 1, 
        "phys.org": 1, 
        "predictscript.com": 1, 
        "psyarxiv.com": 1, 
        "pyimagesearch.com": 1, 
        "quantamagazine.org": 1, 
        "queirozf.com": 2, 
        "rajpurkar.github.io": 1, 
        "reddit.com": 1, 
        "rickyhan.com": 1, 
        "robot-learning.org": 1, 
        "samcoope.com": 1, 
        "self.MachineLearning": 176, 
        "sentiance.com": 1, 
        "sigmoidal.io": 1, 
        "signifiedorigins.wordpress.com": 1, 
        "sites.google.com": 2, 
        "softwaremill.com": 1, 
        "statnews.com": 1, 
        "summerofhpc.prace-ri.eu": 1, 
        "tech.finn.no": 1, 
        "techcrunch.com": 1, 
        "techdirt.com": 1, 
        "theorangeduck.com": 1, 
        "thestrongai.com": 1, 
        "timdettmers.com": 1, 
        "twitter.com": 2, 
        "vanhavel.github.io": 1, 
        "venturebeat.com": 1, 
        "vicarious.com": 1, 
        "vimeo.com": 1, 
        "wired.co.uk": 1, 
        "xkcd.com": 1, 
        "youtu.be": 2, 
        "youtube.com": 9, 
        "ytaigman.github.io": 1
    }, 
    "id": "2r3gv", 
    "num_external_website_posts": 236, 
    "num_text_posts": 176, 
    "public_description": "", 
    "submissions": [
        {
            "author": "c0cky_", 
            "created_utc": 1506802057.0, 
            "domain": "medium.com", 
            "id": "73h61l", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73h61l/p_our_data_science_workflow/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Our Data Science Workflow", 
            "url": "https://medium.com/@shanif/our-data-science-workflow-b974f30a124d"
        }, 
        {
            "author": "obsoletelearner", 
            "created_utc": 1506799258.0, 
            "domain": "self.MachineLearning", 
            "id": "73gw36", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73gw36/d_cybernetics_researchers_and_companys/", 
            "score": 1, 
            "selftext": "Hi, \n\nI gained an intense interest in recent times for cybernetics, however i myself do not know anything much about this field,  can you please recommend any good researchers to follow? are there any companies that would standout in this area? I'd atleast like to start with an internship. \n\nThanks, \nOL.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Cybernetics: Researchers and Companys", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/73gw36/d_cybernetics_researchers_and_companys/"
        }, 
        {
            "author": "ElPresidente408", 
            "created_utc": 1506792684.0, 
            "domain": "self.MachineLearning", 
            "id": "73g8lh", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73g8lh/d_applying_cnns_to_spatialgeographic_data/", 
            "score": 3, 
            "selftext": "I'm working on a project where I want to predict whether a geographic area is likely to observe a particular effect I'm studying. For inputs, I have multiple layers of US Census variables which have been rasterized into 10x10mi squares. My targets are the same format.\n\nNot all areas exhibit the effect so I'm only training based on cells where there has been an observed value and using that to predict the rates for the other cells.  I've initially tried flattening the entire set so that each cell becomes a pair of independent inputs-target values and using a standard fully connected network. Results seem pretty good. Practically speaking, there is *some* non-zero chance of it occurring everywhere so I think this approach works.\n\nBut that had me thinking that there's definitely a neighboring component to this problem since we are talking about people and geography. Convolutions seem like a good idea to grab neighboring cells while handling the depth (features) too. But I'm having a hard time thinking how it could practically be applied.\n\nThere's only one United States to train on. Maybe grab patches and train that way, but then how do I handle those with no observed effect? Wondering if anyone has applied CNNs in practice against geographic data?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Applying CNNs to spatial/geographic data", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/73g8lh/d_applying_cnns_to_spatialgeographic_data/"
        }, 
        {
            "author": "themathstudent", 
            "created_utc": 1506777424.0, 
            "domain": "github.com", 
            "id": "73eu9q", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73eu9q/p_sequence_to_sequence_tensorflow_tutorial/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Sequence to Sequence Tensorflow tutorial (translation)", 
            "url": "https://github.com/sachinruk/deepschool.io/blob/master/Lesson%2019%20-%20Seq2Seq%20-%20Date%20translator%20-%20Solutions.ipynb"
        }, 
        {
            "author": "Reiinakano", 
            "created_utc": 1506777222.0, 
            "domain": "nih.gov", 
            "id": "73etpe", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73etpe/p_nih_clinical_center_provides_one_of_the_largest/", 
            "score": 149, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community", 
            "url": "https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community"
        }, 
        {
            "author": "xingdongrobotics", 
            "created_utc": 1506772936.0, 
            "domain": "self.MachineLearning", 
            "id": "73ej3r", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73ej3r/d_will_it_be_useful_to_have_an_idea_bank_website/", 
            "score": 11, 
            "selftext": "In this way, it might accelerate the development of algorithms, or not ? Since anyone can propose new idea in real-time and get feedback/collaboration/criticize also in real-time. ", 
            "subreddit": "MachineLearning", 
            "title": "[D] Will it be useful to have an 'idea bank' website like polymath project ? People can post ideas and co-work projects/papers online.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/73ej3r/d_will_it_be_useful_to_have_an_idea_bank_website/"
        }, 
        {
            "author": "longinglove", 
            "created_utc": 1506752623.0, 
            "domain": "github.com", 
            "id": "73dfrh", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73dfrh/p_noisy_labels_and_label_smoothing/", 
            "score": 31, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Noisy Labels and Label Smoothing", 
            "url": "https://github.com/Kyubyong/label_smoothing"
        }, 
        {
            "author": "Kaixhin", 
            "created_utc": 1506702425.0, 
            "domain": "blog.openai.com", 
            "id": "738xli", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 66, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/738xli/r_nonlinear_computation_in_deep_linear_networks/", 
            "score": 128, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Nonlinear Computation in Deep Linear Networks", 
            "url": "https://blog.openai.com/nonlinear-computation-in-linear-networks/"
        }, 
        {
            "author": "evc123", 
            "created_utc": 1506702022.0, 
            "domain": "youtube.com", 
            "id": "738vy6", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": {
                "oembed": {
                    "author_name": "Siraj Raval", 
                    "author_url": "https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/HAC6sqq7_-U?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/HAC6sqq7_-U/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "How do we Democratize Access to Data?", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/738vy6/d_how_do_we_democratize_access_to_data_openmined/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] How do we Democratize Access to Data? <--OpenMined tutorial", 
            "url": "https://www.youtube.com/watch?v=HAC6sqq7_-U"
        }, 
        {
            "author": "bronzestick", 
            "created_utc": 1506698988.0, 
            "domain": "self.MachineLearning", 
            "id": "738jtp", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/738jtp/d_good_tutorial_or_introduction_to_pomdps/", 
            "score": 14, 
            "selftext": "Is there a text with a good mathematical treatment of POMDPs and on the approximate/exact methods of solving them? \n\nI did a prelim google search, but I couldn't find a comprehensive one.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Good tutorial or introduction to POMDPs", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/738jtp/d_good_tutorial_or_introduction_to_pomdps/"
        }, 
        {
            "author": "Renton3", 
            "created_utc": 1506694709.0, 
            "domain": "self.MachineLearning", 
            "id": "7382tx", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7382tx/pnews_clustering_python/", 
            "score": 9, 
            "selftext": "I want to know good strategies or algorithms to solve the following problem:\n\nI have a news clustering problem. I don't know the quantity of clusters. The only thing I know about the amount of clusters is that it should be controlled by one parameter. Parameter should regulate the measure of the connectivity between the documents of the cluster, with \"amplification\" I should receive clusters of very similar documents, with \"easing\" - the reverse situation. In ideal case, the algorithm should automatically determine the optimum value of this parameter.\n", 
            "subreddit": "MachineLearning", 
            "title": "[P]News clustering Python", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7382tx/pnews_clustering_python/"
        }, 
        {
            "author": "Daemonix00", 
            "created_utc": 1506682649.0, 
            "domain": "papers.cumincad.org", 
            "id": "7372iu", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7372iu/d_paper_publication_with_a_network_trained_with/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Paper publication with a network trained with 600 images and 40/20 epochs? or something...", 
            "url": "http://papers.cumincad.org/data/works/att/ecaade2017_009.pdf"
        }, 
        {
            "author": "FantasyBorderline", 
            "created_utc": 1506666955.0, 
            "domain": "self.MachineLearning", 
            "id": "73645t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/73645t/r_gender_recognition_using_gil_levis_caffe_model/", 
            "score": 10, 
            "selftext": "As of now, I've managed to use Gil Levi's Caffe model for Gender Recognition in OpenCV. Basically using said model with `Dnn.readNetFromCaffe()`. My problem is that processing one frame takes 1.6-1.7 seconds, basically unusable in a live camera preview.\n\nSo, which do I optimize, OpenCV (especially its tiny-dnn) itself, or the model? And how? Do I also have to implement face tracking and face ID-ing?", 
            "subreddit": "MachineLearning", 
            "title": "[R] Gender Recognition using Gil Levi's Caffe model in a Live Camera Preview - speed problems and asking for possible optimizations in both OpenCV and the model itself", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/73645t/r_gender_recognition_using_gil_levis_caffe_model/"
        }, 
        {
            "author": "Fe_man_", 
            "created_utc": 1506660599.0, 
            "domain": "self.MachineLearning", 
            "id": "735oex", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/735oex/dlooking_for_a_medium_difficulty_ml_project/", 
            "score": 5, 
            "selftext": "I am looking for a semester long project in ML with a group size of 2-4 people preferably with Neural Networks. We looked into gaming (flappy bird) but turns out the scope is pretty small for a final year under-grad project.\nWe also looked at self playing Mario. Could you please tell me whether the scope of Mario is good enough for this project.\nAlso, please suggest other project ideas in ML.", 
            "subreddit": "MachineLearning", 
            "title": "[D]Looking for a medium difficulty ML project...", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/735oex/dlooking_for_a_medium_difficulty_ml_project/"
        }, 
        {
            "author": "longinglove", 
            "created_utc": 1506644444.0, 
            "domain": "github.com", 
            "id": "7349o3", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7349o3/p_pytorch_exercises/", 
            "score": 38, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Pytorch exercises", 
            "url": "https://github.com/Kyubyong/pytorch_exercises"
        }, 
        {
            "author": "thatguydr", 
            "created_utc": 1506640935.0, 
            "domain": "arxiv.org", 
            "id": "733xpc", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/733xpc/r_neural_optimizer_search_with_reinforcement/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Neural Optimizer Search with Reinforcement Learning", 
            "url": "https://arxiv.org/abs/1709.07417"
        }, 
        {
            "author": "mamaosamallama", 
            "created_utc": 1506639721.0, 
            "domain": "self.MachineLearning", 
            "id": "733thd", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 29, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/733thd/d_what_skills_would_you_like_to_see_in_a/", 
            "score": 12, 
            "selftext": "I'm about to graduate and was wondering what technical skill set I should pursue in order to land an entry level Machine Learning position. I'm learning Python and I'm currently enrolled in Andrew Ng's course on Coursera. Any help would be appreciated. ", 
            "subreddit": "MachineLearning", 
            "title": "[D] What skills would you like to see in a candidate if you were interviewing them for a Machine Learning position?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/733thd/d_what_skills_would_you_like_to_see_in_a/"
        }, 
        {
            "author": "weirdML", 
            "created_utc": 1506635153.0, 
            "domain": "vimeo.com", 
            "id": "733cs7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": {
                "oembed": {
                    "author_name": "Amii", 
                    "author_url": "https://vimeo.com/amiithinks", 
                    "description": "Abstract: This talk will present an overview of our recent research on distributional reinforcement learning. In a recent paper we argued for the fundamental importance of the value distribution: the distribution of random returns received by a reinforcement learning agent. This is in contrast to the common approach, which models the expectation of this return, or value.", 
                    "height": 338, 
                    "html": "<iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F235922311&dntp=1&url=https%3A%2F%2Fvimeo.com%2F235922311&image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F657889004_1280.jpg&key=522baf40bd3911e08d854040d3dc5c07&type=text%2Fhtml&schema=vimeo\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "Vimeo", 
                    "provider_url": "https://vimeo.com/", 
                    "thumbnail_height": 720, 
                    "thumbnail_url": "https://i.vimeocdn.com/video/657889004_1280.jpg", 
                    "thumbnail_width": 1280, 
                    "title": "Marc Bellemare - \"A Distributional Perspective on Reinforcement Learning\"", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "vimeo.com"
            }, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/733cs7/r_a_distributional_perspective_on_reinforcement/", 
            "score": 27, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] \"A Distributional Perspective on Reinforcement Learning\" - Marc Bellemare", 
            "url": "https://vimeo.com/235922311"
        }, 
        {
            "author": "libreland", 
            "created_utc": 1506629763.0, 
            "domain": "groups.google.com", 
            "id": "732rxz", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 126, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/732rxz/d_theanos_dead/", 
            "score": 533, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Theano's Dead", 
            "url": "https://groups.google.com/forum/#!topic/theano-users/7Poq8BZutbY"
        }, 
        {
            "author": "OccamsNuke", 
            "created_utc": 1506626306.0, 
            "domain": "adamashwal.com", 
            "id": "732dxq", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/732dxq/d_teaching_a_catapult_to_shoot_down_a_missile/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Teaching a Catapult to Shoot Down a Missile: First impressions with Unity's reinforcement learning SDK", 
            "url": "http://adamashwal.com/catapult"
        }, 
        {
            "author": "bbernhard1", 
            "created_utc": 1506622817.0, 
            "domain": "self.MachineLearning", 
            "id": "731zwb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/731zwb/p_imagemonkey_a_public_open_source_image_database/", 
            "score": 31, 
            "selftext": "Hi reddit,\n\nI am really happy to finally get a chance to post something to https://www.reddit.com/r/SideProject/. \n\nThe last three weeks I was working on ImageMonkey - check it out here: https://imagemonkey.io/. \n\nThe idea originated while I was working on another project where at some point I wanted to integrate Machine Learning into my application. With all the great Machine Learning frameworks out there, it's really easy to get your foot into the door quickly. You can easily download a pre-trained model and re-train the last layer with your own image dataset.\n\nBut while I was playing a little bit with the frameworks, I somehow realized that it's really hard to get some good training data. If you are lucky then there is some (annotated) training data online, if not...well, then you have to get your hands dirty and do the tedious work yourself. So I thought to myself: Wouldn't it be cool to create a image dataset that's publicly, open source and easily \naccessible? That's the point ImageMonkey was born.\n\nWhile working on ImageMonkey I realized one thing quickly: It should be easy to get data in there and easy to get data out there. If it's too complicated to donate, annotate or validate pictures nobody will do it. That's why there is a mobile app, a soon to be released Google Chrome Extension (Validate a picture every time you open a new tab in your browser) and a REST API. I am sure there are lot of improvements, but I think for a start it's not too bad. \n\n**Whats next?**\n\nAs this post already is pretty long (and I am not sure if anyone still is reading ;-)), I will keep this one short. (If you are interested, there will be more detailed blog post available soon)\n\n* Hardware Upgrade + Caching: Currently everything is running on one really small machine\n\n* Online Playground: It would be cool to actually test how good/bad our image dataset is classifying images. So it would be nice to have regular (Tensorflow?) builds which are based on the current image dataset.\n\n* API abuse prevention: As the API is public there needs to be some sort of API abuse trigger that goes off when someone tries to destroy datasets on purpose by maliciously voting for the wrong images.\n\n* add some sort of gamification to attract also non-tech related people\n\n* API improvements \n\n* Bugfixes (there are always bugs to fix)\n\n\nIf you are still with me - thanks for reading! If you have questions, ideas, suggestions or feedback - please feel free to do so. \nAny input is greatly appreciated.\n\n\n\n**Questions**\n\nIn /r/SideProject I got a pretty interesting question from /u/benscar1 which I want post here as well, as I think it's a great topic for a discussion. \n\n\n> How are you going to convince people to annotate or label the images for no compensation?\n\n\nThat's a really good question, that I also thought a lot about lately. Do you think that there is compensation needed in order to convince people to contribute or will they feel the need to contribute when they are actively using the service in order to give something back? Honestly, I don't really know. What motivates people to contribute to Wikipedia? (serious question, as I haven't contributed anything to Wikipedia yet I really don't know what people motivates to do so)\n\nI personally think that simplicity is key here. If there are people out there that want to contribute something they should be able to do it with just one click. Ideally without any registration. People should be able to contribute without any additional hassle.\n\nI could imagine that some data, that already has a high propability to be valid, can also be used in child's games. e.q: \"Mark all apples on the picture\". Due to this, it might be possible further increase the dataset's quality. Of couse we have to be fully transparent at this point that the data is used for something, so that users can opt-out if they don't want to participate.. And yeah...the game should be fun for kids :D\n\nGiven that we have categories with a lot of training images, it might be possible to let the neural network do the initial annotation/validation.\n\nWhat I could also imagine is to build a captcha like service that people can implement in their webpages . E.q: \"select all pictures with an apple\"\n\n\nWhat do you think? What motivates you to contribute something to the public? \n\nCheers,\n\nBernhard ", 
            "subreddit": "MachineLearning", 
            "title": "[P] ImageMonkey - A public open source image database (x-post from /r/SideProject)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/731zwb/p_imagemonkey_a_public_open_source_image_database/"
        }, 
        {
            "author": "nefertum", 
            "created_utc": 1506619333.0, 
            "domain": "self.MachineLearning", 
            "id": "731lgk", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 36, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/731lgk/ddeep_learning_with_little_data/", 
            "score": 10, 
            "selftext": "Sorry, but I am a complete newbie on machine learning/deep learning. I need to build a deep learning model in tensorflow with human genome data to classify cancer. My data is too limited. (at most 1000 sample) and it became even smaller when I divided it for train, test, and validation. (input matrix 1060 * 20206 )\nEven though, it didn't seem logical for me to use deep learning for such a small amount of data that is what I need to do ...\nCan you recommend to some structure names or methods for feature selection or anything to give me an idea ?", 
            "subreddit": "MachineLearning", 
            "title": "[D]Deep learning with little data", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/731lgk/ddeep_learning_with_little_data/"
        }, 
        {
            "author": "RedditEpiphanySnail", 
            "created_utc": 1506614730.0, 
            "domain": "self.MachineLearning", 
            "id": "7312ow", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 24, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7312ow/d_information_theory_101_and_how_it_helps_to/", 
            "score": 98, 
            "selftext": "Hey guys, I stumbled upon an article that I think will interest anyone discovering Machine Learning.\n\n### Information Theory 101\nThis informative and let's be honest, [quite dense post](https://blog.recast.ai/introduction-information-theory-care/), goes back to the roots of information theory, giving the spotlight to [Claude E. Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) who layed out the basics of modern information theory in his paper Mathematical Theory of Communication published in 1948.\nBefore Shanon, information theory was already studied but much different from what we know today. Essentially, most observations, rules and theorems were channel-specific. The only understanding everybody agreed upon was the tradeoff between rate and reliability. In this paper, Shanon proved the tradeoff invalid.\n\n### Why should I care about Information Theory for Machine Learning?\nThe goal of machine learning is to have a computer able to learn by himself, without being explictly programmed. So, to put it back in our terms, ML goals is to maximizes mutual information. \n\nDiverse Information Theory paradigm are now frequently used for Machine Learning such as:\n\n* **Clustering by Compression (ZIP/RAR/GIF/JPEG...)**: Instead of saving twice (or more) a redundant information, only the reference to the second iteration of the information (and the nth one) are saved to reduce the overall file size without loosing quality.\n\n* **Classification**: Both supervised and unsupervised classifications methods relie on paradigms coming from Information Theory. They are useful to sort large set of data: classifying a music library by genre for instance.\n\n---\n\nYou can find the article [here](https://blog.recast.ai/introduction-information-theory-care/). It's quite a read but it's definitely worth it to anyone who wanna beef or freshen up one's knowledge on Information Theory and one's understanding of Machine Learning.\n\nSo have you read it? What did you learn?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Information Theory 101 and how it helps to understand machine learning", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7312ow/d_information_theory_101_and_how_it_helps_to/"
        }, 
        {
            "author": "fixedrl", 
            "created_utc": 1506614248.0, 
            "domain": "self.MachineLearning", 
            "id": "7310nw", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7310nw/d_what_might_be_the_impacts_of_relusigmoid_for/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] What might be the impacts of ReLU/Sigmoid for training one-step dynamics model in RL ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7310nw/d_what_might_be_the_impacts_of_relusigmoid_for/"
        }, 
        {
            "author": "vadim878", 
            "created_utc": 1506613497.0, 
            "domain": "self.MachineLearning", 
            "id": "730xjh", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/730xjh/d_decent_implementation_of_pointer_network/", 
            "score": 1, 
            "selftext": "Since couple days, I'm looking for a pointer network implementation in python. \n\nDoes somebody can recommend me anything? Preferable Keras, but in general I don't care. \n\nI would like to use it for a list sorting application, f.i.:\ninput:\n[1,1,2,2,0] \noutput:\n[1,2,2,1,0]\n\nImportant number of internal elements should be the same for input and output list. That's why I think the pointer net is a suitable solution... ", 
            "subreddit": "MachineLearning", 
            "title": "[D] decent implementation of Pointer Network?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/730xjh/d_decent_implementation_of_pointer_network/"
        }, 
        {
            "author": "HowDeepisYourLearnin", 
            "created_utc": 1506611353.0, 
            "domain": "self.MachineLearning", 
            "id": "730owz", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/730owz/d_how_does_facebookgoogleapple_do_cluster_images/", 
            "score": 4, 
            "selftext": "The problem is grouping images of faces into groups where each group corresponds to one identity, but you get receive the faces one by one as the user uploads photos. In almost all literature I have read it is assumed that you have all images available at once, make some latent representation of each face, and then cluster the representations by defining some distance metric or looking at the structure of the latent space. \n\nI'm curious how one would go about clustering faces incrementally as you receive them. If your embedding is accurate enough you could just compare an incoming image with an example from all current clusters. If you get a match to add to that cluster or else you just add a new cluster. This would require a ridiculously accurate classifier, but I wouldn't put it past the companies mentioned to make something like that.\nDoing incremental clustering seems like an extremely complicated and/or expensive operation.\n\nDoes anyone here have experience or knowledge on this topic (or papers on exactly this that I've missed)?", 
            "subreddit": "MachineLearning", 
            "title": "[D] How does facebook/google/apple do cluster images of people into different identities?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/730owz/d_how_does_facebookgoogleapple_do_cluster_images/"
        }, 
        {
            "author": "drinkmenot", 
            "created_utc": 1506609666.0, 
            "domain": "self.MachineLearning", 
            "id": "730icq", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/730icq/d_is_there_any_literature_on_the_impact_of_the/", 
            "score": 23, 
            "selftext": "Specifically, I'm interested in a log-scale curve \"number of hours vs word error rate\" in speech recognition, but I appreciate any non-ASR pointers as well.\n\nI'm interested in finding the point at which the data \"arms race\" will not get a fixed model further.\n\nedit:\nhere are some older papers that have the numbers I'm looking for, but for non-neural models:\n\n* http://www.dcs.shef.ac.uk/~roger/publications/Eurospeech03%20Comparison%20of%20Data%20Requirements.pdf\n\n* http://mi.eng.cam.ac.uk/~ky219/papers/evermann-icassp05.pdf", 
            "subreddit": "MachineLearning", 
            "title": "[D] Is there any literature on the impact of the amount of training data on the performance of recurrent neural networks?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/730icq/d_is_there_any_literature_on_the_impact_of_the/"
        }, 
        {
            "author": "LF_78", 
            "created_utc": 1506605964.0, 
            "domain": "self.MachineLearning", 
            "id": "7304ml", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7304ml/d_simple_and_basic_python_script_implementing/", 
            "score": 3, 
            "selftext": "I'm struggling to find a **simple** working example of reinforcement learning (Proximal Policy Optimization) written with [TensorForce](https://github.com/reinforceio/tensorforce), to understand the general approach and start tinkering with it.\n\nI'm thinking about TensorForce because it seems the most high level library focused on RL, but any other library (like Keras) would do.\n\nAll the examples I found are pretty complex (at least for me) and involving OpenAI Gym: I would need a simple script defining the environment, the network, the training operations and inference operations, that I could use as a starting point.\n\nDo you have any suggestion?\n\nThanks in advance!", 
            "subreddit": "MachineLearning", 
            "title": "[D] Simple and basic Python script implementing reinforcement learning (PPO) with TensorForce?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7304ml/d_simple_and_basic_python_script_implementing/"
        }, 
        {
            "author": "henry-e", 
            "created_utc": 1506596874.0, 
            "domain": "arxiv.org", 
            "id": "72zdf7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72zdf7/r_generating_sentences_by_editing_prototypes/", 
            "score": 72, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Generating Sentences by Editing Prototypes", 
            "url": "https://arxiv.org/abs/1709.08878"
        }, 
        {
            "author": "Jojanzing", 
            "created_utc": 1506590146.0, 
            "domain": "github.com", 
            "id": "72yy49", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72yy49/p_opensource_sleepstage_classifier_python/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Open-Source Sleep-Stage Classifier - Python", 
            "url": "https://github.com/skjerns/AutoSleepScorer"
        }, 
        {
            "author": "jaesik", 
            "created_utc": 1506586568.0, 
            "domain": "github.com", 
            "id": "72yqq2", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72yqq2/p_tensorflow_implementation_of_programmable_agents/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Tensorflow implementation of Programmable Agents", 
            "url": "https://github.com/jaesik817/programmable-agents_tensorflow"
        }, 
        {
            "author": "lleewwiiss", 
            "created_utc": 1506569014.0, 
            "domain": "self.MachineLearning", 
            "id": "72xic3", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72xic3/d_training_lstm_networks_with_multiple_timeseries/", 
            "score": 6, 
            "selftext": "I have a dataset containing usage and maintenance information (3000 unique events) for ~100,000 hardware components, over a 2 year period. I am trying to predict component failure in the next 3 months based on this data (0/1).\n\nI am looking to investigate LSTM networks but am confused as to how to train it over multiple samples. Do I need to train a separate network for each component and somehow combine them, or is it possible to have each components time-series as an input and build the model that way?\n\nI am going to extract features on monthly basis for each component.\n\nIs it possible to use these windows to predict the final label or should I be labeling each window and trying to predict that? but would I be creating very unbalanced data as only the last windows would have a 1 label?\n\nIt is also possible LSTM style networks may not be ideal for this problem, thanks for any help!", 
            "subreddit": "MachineLearning", 
            "title": "[D] Training LSTM Networks with multiple time-series", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72xic3/d_training_lstm_networks_with_multiple_timeseries/"
        }, 
        {
            "author": "george_lul", 
            "created_utc": 1506542346.0, 
            "domain": "self.MachineLearning", 
            "id": "72uvnv", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72uvnv/d_whats_iclr_like/", 
            "score": 7, 
            "selftext": "Going there first time.\n\nSubmitting a paper there. \n\nhelp", 
            "subreddit": "MachineLearning", 
            "title": "[D] Whats ICLR like?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72uvnv/d_whats_iclr_like/"
        }, 
        {
            "author": "dirac-hatt", 
            "created_utc": 1506539631.0, 
            "domain": "github.com", 
            "id": "72uki8", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72uki8/p_tensorflow_implementation_of_enet_semantic/", 
            "score": 30, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] TensorFlow Implementation of ENet - Semantic Segmentation for Autonomous Driving", 
            "url": "https://github.com/fregu856/segmentation"
        }, 
        {
            "author": "xingdongrobotics", 
            "created_utc": 1506537868.0, 
            "domain": "self.MachineLearning", 
            "id": "72ud9s", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72ud9s/d_often_heard_messy_deadline_code_yet_still/", 
            "score": 6, 
            "selftext": "----------Update---------------------\n\nWhy many people not release their code on GitHub at the same time the paper pushed on ArXiv ? Even though the code might looks messy, but the open community can polish it together (issues/PR), and it also make others to understand technical details faster.\nAre there some reasons why this is not common ?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Often heard 'messy deadline code', yet still guaranteeing the good result in the paper not caused by 'nice-bug' ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72ud9s/d_often_heard_messy_deadline_code_yet_still/"
        }, 
        {
            "author": "cherls", 
            "created_utc": 1506537597.0, 
            "domain": "machinelearning.apple.com", 
            "id": "72uc35", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 20, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72uc35/r_realtime_recognition_of_handwritten_chinese/", 
            "score": 100, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Real-Time Recognition of Handwritten Chinese Characters Spanning a Large Inventory of 30,000 Characters - Apple", 
            "url": "https://machinelearning.apple.com/2017/09/12/handwriting.html"
        }, 
        {
            "author": "yurtaev", 
            "created_utc": 1506534705.0, 
            "domain": "ghrecommender.io", 
            "id": "72tzxj", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72tzxj/p_ghrecommender_personalized_recommendations_for/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] GHRecommender - personalized recommendations for GitHub projects based on information about repositories starred by the user", 
            "url": "https://ghrecommender.io"
        }, 
        {
            "author": "fixedrl", 
            "created_utc": 1506532859.0, 
            "domain": "self.MachineLearning", 
            "id": "72ts98", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72ts98/d_debug_with_rl_policy_network_tends_to_generate/", 
            "score": 2, 
            "selftext": "The environment is OpenAI-Gym, CartPole-v0, I made it to be continuous action space `[-1, 1]`. The policy network is 1-layer MLP with 50 hidden neurons (ReLU). Actions generated near initial state is okay, but when rolling out trajectories, it generates actions e.g. -0.2, 0.7, 1.x, 2.x, 3.x, 4.x, ...", 
            "subreddit": "MachineLearning", 
            "title": "[D] Debug with RL: Policy network tends to generate larger and larger invalid action ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72ts98/d_debug_with_rl_policy_network_tends_to_generate/"
        }, 
        {
            "author": "say_wot_again", 
            "created_utc": 1506527180.0, 
            "domain": "twitter.com", 
            "id": "72t4la", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72t4la/d_nice_twitter_thread_on_reproducibility_in_ml/", 
            "score": 18, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Nice Twitter thread on reproducibility in ML", 
            "url": "https://twitter.com/nsaphra/status/912973595038683136"
        }, 
        {
            "author": "RocketApplianceAdmin", 
            "created_utc": 1506520529.0, 
            "domain": "self.MachineLearning", 
            "id": "72sep4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72sep4/d_how_relevant_is_data_transfer_speed_in_ml/", 
            "score": 4, 
            "selftext": "In your current ML computer, what is the connection from your data storage to the GPUs? Are the data transfer speeds with these connection sufficient for computations, or would a faster rate of data transfer allow for faster training? Essentially, is there a performance bottle neck in training caused by the data transfer? For example, would these speeds be different when using network attached storage (via Ethernet) in comparison to a local drive with SATA connection to the motherboard?  Would these different speeds affect the overall performance?", 
            "subreddit": "MachineLearning", 
            "title": "[D] How relevant is data transfer speed in ML?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72sep4/d_how_relevant_is_data_transfer_speed_in_ml/"
        }, 
        {
            "author": "asobolev", 
            "created_utc": 1506518902.0, 
            "domain": "youtu.be", 
            "id": "72s90w", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": {
                "oembed": {
                    "author_name": "Oleg Ivanov", 
                    "author_url": "https://www.youtube.com/channel/UCAKtuIJzTMmM09iOBZwBpDQ", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/9VtPt33MKK0?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/9VtPt33MKK0/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "AdaGAN: Boosting Generative Models, Iliya Tolstikhin, bayesgroup.ru", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72s90w/r_adagan_boosting_generative_models_iliya/", 
            "score": 47, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] AdaGAN: Boosting Generative Models, Iliya Tolstikhin (NIPS2017 paper)", 
            "url": "https://youtu.be/9VtPt33MKK0"
        }, 
        {
            "author": "zsdh123", 
            "created_utc": 1506515946.0, 
            "domain": "github.com", 
            "id": "72rz0a", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72rz0a/p_improved_cyclegan_with_resizeconvolution/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Improved CycleGAN with Resize-Convolution", 
            "url": "https://github.com/luoxier/CycleGAN_Tensorlayer"
        }, 
        {
            "author": "swentso", 
            "created_utc": 1506512448.0, 
            "domain": "self.MachineLearning", 
            "id": "72romz", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72romz/d_what_are_most_interesting_mldldata_conferences/", 
            "score": 0, 
            "selftext": "Hey !\nI'm trying to figure out if I could attend some interesting event' on the applications of ML/DL/Data in businesses (I'm not talking about not academic confs). What are the ones you know, especially in Europe.\n\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] What are most interesting ML/DL/Data Conferences in Europe In your op' ? (About applications in business, not academic ones)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72romz/d_what_are_most_interesting_mldldata_conferences/"
        }, 
        {
            "author": "hardmaru", 
            "created_utc": 1506499895.0, 
            "domain": "developers.google.com", 
            "id": "72qvgy", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72qvgy/p_machine_learning_glossary/", 
            "score": 231, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Machine Learning Glossary", 
            "url": "https://developers.google.com/machine-learning/glossary/"
        }, 
        {
            "author": "statmlsn", 
            "created_utc": 1506497546.0, 
            "domain": "arxiv.org", 
            "id": "72qqgs", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72qqgs/r_autoencoder_by_forest/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] AutoEncoder by Forest", 
            "url": "https://arxiv.org/abs/1709.09018"
        }, 
        {
            "author": "LazyOptimist", 
            "created_utc": 1506495897.0, 
            "domain": "arxiv.org", 
            "id": "72qmqo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72qmqo/r_muon_trigger_for_mobile_phones/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Muon Trigger for Mobile Phones", 
            "url": "https://arxiv.org/abs/1709.08605v1"
        }, 
        {
            "author": "3amm0R", 
            "created_utc": 1506489103.0, 
            "domain": "self.MachineLearning", 
            "id": "72q6dy", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72q6dy/p_video_pixel_networks_in_tensorflow/", 
            "score": 17, 
            "selftext": "After DeepMind has released the Video Pixel Networks paper I started working on a tensorflow implementation of it on this [github repository ](https://github.com/3ammor/Video-Pixel-Networks). I finished it so feel free to check it out. And I also wrote a medium post about the architecture and my implementation details. You can find it here: [VPN in TF](https://medium.com/@omarabdulrahman/video-pixel-networks-in-tensorflow-5a8644a044a9)", 
            "subreddit": "MachineLearning", 
            "title": "[P] Video Pixel Networks in Tensorflow", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72q6dy/p_video_pixel_networks_in_tensorflow/"
        }, 
        {
            "author": "Phylliida", 
            "created_utc": 1506465394.0, 
            "domain": "self.MachineLearning", 
            "id": "72o1p5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72o1p5/d_have_we_solved_the_animal_turing_test_for_any/", 
            "score": 7, 
            "selftext": "What I mean is, have we made any physical robot type thing that interacts with animals of a specific species and those animals accept the robot as one of their own?\n\nI have chickens and they like to follow each other around (get scared and scream when they can\u2019t find each other) and do dumb stuff all day that seems like it would be pretty easy to teach a chicken robot to do.\n\nThis is a little tricky because lots of animals are pretty accepting of weird things as long as they get reasonable interactions and/or food from them. For example, we were worried at first that our dog might eat our chickens but the first time she came out a chicken pecked her on the nose and she ran away and is now scared of the chickens. She will still follow them around and if they are eating something (like birdseed) she will get jealous and try to eat it too, then get confused cause it\u2019s gross. That has happened so many times lol\n\nThe hardware design might be tricky but I think making a little robot ant that is accepted into an ant colony would also be impressive and less susceptible to this problem and currently is very hard to do. Perhaps a termite or bee colony might work better since they are bigger idk but yea passing the \u201cAnimal Turing Test\u201d I think would be really cool and interesting for studying animal behavior in its own right", 
            "subreddit": "MachineLearning", 
            "title": "[D] Have we solved the \u201cAnimal Turing Test\u201d for any specific animal species?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72o1p5/d_have_we_solved_the_animal_turing_test_for_any/"
        }, 
        {
            "author": "Gurung88", 
            "created_utc": 1506463143.0, 
            "domain": "self.MachineLearning", 
            "id": "72nt5a", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72nt5a/d_what_are_some_differences_between_how_deep/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] What are some differences between how deep learning works and how neurons in a network in the brain work?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72nt5a/d_what_are_some_differences_between_how_deep/"
        }, 
        {
            "author": "thumbsdrivesmecrazy", 
            "created_utc": 1506456822.0, 
            "domain": "blog.dataversioncontrol.com", 
            "id": "72n3m5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72n3m5/r_best_practices_of_orchestrating_python_and_r/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Best practices of orchestrating Python and R code in ML projects", 
            "url": "https://blog.dataversioncontrol.com/best-practices-of-orchestrating-python-and-r-code-in-ml-projects-f28f3a879484"
        }, 
        {
            "author": "darkconfidantislife", 
            "created_utc": 1506453420.0, 
            "domain": "nvdla.org", 
            "id": "72mp9y", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72mp9y/news_nvidia_open_sources_deep_learning_processor/", 
            "score": 37, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[News] nVidia open sources (!) deep learning processor hardware (RTL and Verilog)", 
            "url": "http://nvdla.org"
        }, 
        {
            "author": "galapag0", 
            "created_utc": 1506451089.0, 
            "domain": "github.com", 
            "id": "72mfdc", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72mfdc/p_code_from_the_classsplitting_generative/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Code from the \"Class-Splitting Generative Adversarial Networks\" paper", 
            "url": "https://github.com/CIFASIS/splitting_gan"
        }, 
        {
            "author": "Pfohlol", 
            "created_utc": 1506442329.0, 
            "domain": "arxiv.org", 
            "id": "72lejx", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72lejx/r_170805894_an_improved_multioutput_gaussian/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] [1708.05894] An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis Detection", 
            "url": "https://arxiv.org/abs/1708.05894"
        }, 
        {
            "author": "MG2033", 
            "created_utc": 1506440901.0, 
            "domain": "github.com", 
            "id": "72l8ix", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72l8ix/project_shufflenet_implementation_in_tensorflow/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[Project] ShuffleNet implementation in TensorFlow", 
            "url": "https://github.com/MG2033/ShuffleNet"
        }, 
        {
            "author": "q914847518", 
            "created_utc": 1506439971.0, 
            "domain": "github.com", 
            "id": "72l4oi", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 74, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72l4oi/pfinally_managed_to_paint_on_anime_sketch_with/", 
            "score": 420, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[p]FINALLY MANAGED to paint on anime sketch WITH REFERENCE!!", 
            "url": "https://github.com/lllyasviel/style2paints"
        }, 
        {
            "author": "desku", 
            "created_utc": 1506439468.0, 
            "domain": "arxiv.org", 
            "id": "72l2n6", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72l2n6/r_a_survey_of_machine_learning_for_big_code_and/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] A Survey of Machine Learning for Big Code and Naturalness", 
            "url": "https://arxiv.org/abs/1709.06182"
        }, 
        {
            "author": "illiterate_gorillas", 
            "created_utc": 1506439178.0, 
            "domain": "self.MachineLearning", 
            "id": "72l1gy", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72l1gy/doptions_for_real_time_object_detection_and/", 
            "score": 4, 
            "selftext": "Hello back prop experts,\n\nI recently started looking into object detection for a project of mine and was wondering if am missing something to get stuff off the ground.\n\nI want to implement a real time object detection system on a raspberry pi 3 for surveillance of an open spaces for eg a garden. I have already tried a few available solutions. I don't need to detect many classes(only 3 person,dog, bicycle) so maybe the fastest option can be retrained with fewer filters and parameters thereby decreasing the total compute time.\n\n**Darknet(YOLO)** [https://github.com/pjreddie/darknet] \nInstalled default darknet tested YOLOv2 and YOLO runs on a raspberry pi3 each frames runs for approx `450 secs` for each image.\nTiny YOLO had run for `40 seconds` per image.\n\n**Tensorflow Google Object Detection (API)**[https://github.com/tensorflow/models/blob/master/object_detection/g3doc/installation.md]:\nI had tried all available networks.\nThe best performing one was SSD inception network which runs at `26 secs` per image.\n\n**Microsoft Embedded Learning Library (ELL)**[https://github.com/Microsoft/ELL]:\nI could not get this to work for some compilation reasons but will try to check it out again later.\nPlease let me know If this worked for you and how it performs in Object detection tasks.\n\n**Darknet-NNPACK**[https://github.com/thomaspark-pkj/darknet-nnpack]:\nHere the darknet was optimized for arm processors and had convolutions implemented with some kind of FFT implementations with speeds up stuff a lot.\n\nI have achieved most promise from this but it has its problems.\n\nInstalled darknet tested YOLO(full v1) runs on a Raspberry Pi3 each image requires \napprox `45 secs` which is 10x faster than default YOLO network.\nTiny YOLO had run for `1.5 seconds` per frame but gives `no results`.\n\nThis is possible bug reported due to version conflicts between the models and the cfg files. I have opened an github (issue)[https://github.com/thomaspark-pkj/darknet-nnpack/issues/13] a while back and yet to receive a response.\n\n**MXnet (SSD)**[https://github.com/zhreshold/mxnet-ssd]:\nPort of SSD in Mxnet (not compiled with NNPACK)\nMXnet SSD resnet 50 per image `88 sec`\nMXnet SSD inceptionv3 per image `35 sec`\n\n\n**Caffe-YOLO**[https://github.com/yeahkun/caffe-yolo]:\nRunning caffe on yolo_small works  with `24 sec` per frame.\nRunning caffe on yolo_tiny works  with `5 sec` per frame.\nThis looks like the fastest of the one I have tried unless the darknet-nnpack issues could be solved.\n\n\n\nDid I miss any other framework/model that could get better results?\nFor now If I cant get any better results I would train the tiny yolo(nnpack) and maybe look for quantizing options later to speed it up.\n\n ", 
            "subreddit": "MachineLearning", 
            "title": "[D]Options for Real time object detection and localization on a raspberry pi 3 (ideal speed >10fps)?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72l1gy/doptions_for_real_time_object_detection_and/"
        }, 
        {
            "author": "zthoutt", 
            "created_utc": 1506438092.0, 
            "domain": "hackernoon.com", 
            "id": "72kx19", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72kx19/r_5_min_explanation_of_creative_adversarial/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] 5 min. Explanation of Creative Adversarial Networks", 
            "url": "https://hackernoon.com/what-are-creative-adversarial-networks-cans-bb81d09aa235"
        }, 
        {
            "author": "party-horse", 
            "created_utc": 1506434653.0, 
            "domain": "github.com", 
            "id": "72kjii", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72kjii/p_graph_attach_simple_computational_graph_library/", 
            "score": 15, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "\u200b [P] Graph Attach: Simple computational graph library for machine learning in pure Python.", 
            "url": "https://github.com/jgolebiowski/graphAttack"
        }, 
        {
            "author": "rx303", 
            "created_utc": 1506430689.0, 
            "domain": "self.MachineLearning", 
            "id": "72k5jq", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72k5jq/d_smart_teacher_for_active_learning/", 
            "score": 3, 
            "selftext": "So, I have a fact extraction task of time indication in text (like 'in ten minutes', 'from two to five hours', 'tomorrow after lunch' framed with some context words). I'm doing it with BiLSTM+CRF for NER (extracting places in text where it is said about time) + LSTM for classification (specific dates, hours, minutes and modes - is it interval or not, absolute or relative time,...) + some code to convert that classification to datetime values.\n\nOn the one hand, I don't have a training set. On the other, I can write a bunch of templates of time indication myself and then substitute a random hour\\minute. So, I've coded a function which generates a dictionary data structure with values randomly chosen from finite discrete sets and then builds a proper string. I use that data structure as a classification target.\n\nI find that architecture quite interesting - instead of solving direct problem of text analysis, I solve reverse, more simple problem of text synthesis and then unleash ML on it. And that makes me think that here I could somehow apply active learning because instead of randomly generating source data structure I could somehow analyze history of training and actively select values. And because data structure generation is sequential with some fields depending on another (for example, if we choose to output time interval then there will be 2 time values, otherwise only 1), that active learning probably should look like parameter tree exploration, where nodes are some global parameter selection and leaves are concrete hours\\minutes. For example, if teacher finds out that network is weak to some kind of examples = some branch in tree, it should be more inclined to palm off these templates but with other hours\\minutes.\n\nAre there any papers I could read on subject?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Smart teacher for active learning", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72k5jq/d_smart_teacher_for_active_learning/"
        }, 
        {
            "author": "andrewm4894", 
            "created_utc": 1506428469.0, 
            "domain": "self.MachineLearning", 
            "id": "72jypd", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72jypd/d_user2vec_representing_a_user_based_on_the_docs/", 
            "score": 7, 
            "selftext": "I'd like to form a representation of users based on the last N documents they have consumed. \n\nSo i'm planning on using doc2vec to form this representation of each document but i'm just trying to figure out what would be a good way to essentially place users in the same space. \n\nSomething as simple as averaging the vectors of their last 5 documents they consumed springs to mind but am not sure if this might be a bit silly as wash a lot away in the average. Maybe some sort of knn approach in the space might be possible. \n\nThen i'm wondering - the same way we just use a doc_id in doc2vec, how crazy would it be to just add in a user_id token and try that way to get a representation of a user in much the same way as a document. \n\nI've not been able to find much on ways to use word2vec type embeddings to come up with both document vectors and user vectors that can then be used in a sort of vector space model approach. \n\nAnyone any pointers or suggestions?", 
            "subreddit": "MachineLearning", 
            "title": "[D] User2Vec? representing a user based on the docs they consume", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72jypd/d_user2vec_representing_a_user_based_on_the_docs/"
        }, 
        {
            "author": "spurra", 
            "created_utc": 1506422510.0, 
            "domain": "arxiv.org", 
            "id": "72ji91", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72ji91/r_guiding_infogan_with_semisupervision/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Guiding InfoGAN with Semi-Supervision", 
            "url": "https://arxiv.org/abs/1707.04487"
        }, 
        {
            "author": "Deep_Fried_Learning", 
            "created_utc": 1506421073.0, 
            "domain": "arxiv.org", 
            "id": "72jezx", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72jezx/r170903966_unsupervised_deep_homography_a_fast/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R][1709.03966] Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model", 
            "url": "https://arxiv.org/abs/1709.03966"
        }, 
        {
            "author": "visarga", 
            "created_utc": 1506419906.0, 
            "domain": "arxiv.org", 
            "id": "72jc5h", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72jc5h/r_film_visual_reasoning_with_a_general/", 
            "score": 32, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] FiLM: Visual Reasoning with a General Conditioning Layer", 
            "url": "https://arxiv.org/abs/1709.07871"
        }, 
        {
            "author": "gau_mar", 
            "created_utc": 1506413981.0, 
            "domain": "arxiv.org", 
            "id": "72izeu", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72izeu/r_clustering_financial_time_series_a_review/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Clustering Financial Time Series - a review", 
            "url": "https://arxiv.org/pdf/1703.00485.pdf"
        }, 
        {
            "author": "blakeh36", 
            "created_utc": 1506392348.0, 
            "domain": "self.MachineLearning", 
            "id": "72hdfb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72hdfb/d_what_is_the_path_to_ml_research/", 
            "score": 8, 
            "selftext": "I'm currently a freshman in computer science at a fairly respected university. I am very interested by the application and current progress of ML. I really want to eventually do research, and I want to know what steps I should be taking do that. Currently, I'm doing a freshman research program. Besides that, I'm not sure what needs to be done.\nI've looked it up and I understand that I need to get a phd in something related. Right now my plan is to double major in pure math, and do a 5-year master program my school has. Finally getting my phd somewhere relating to ML, but I'm not sure if that's correct\nWhat was your path to research? what do you recommend?", 
            "subreddit": "MachineLearning", 
            "title": "[D] What is the path to ML research?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72hdfb/d_what_is_the_path_to_ml_research/"
        }, 
        {
            "author": "code_kansas", 
            "created_utc": 1506392148.0, 
            "domain": "experiments.adversarial.network", 
            "id": "72hcra", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72hcra/p_visualization_of_three_different/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Visualization of three different biologically-grounded neuron models", 
            "url": "http://experiments.adversarial.network/biological-neurons"
        }, 
        {
            "author": "xternalz", 
            "created_utc": 1506390138.0, 
            "domain": "arxiv.org", 
            "id": "72h5zf", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 47, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72h5zf/r_the_consciousness_prior/", 
            "score": 85, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] The Consciousness Prior", 
            "url": "https://arxiv.org/abs/1709.08568"
        }, 
        {
            "author": "rubot9000", 
            "created_utc": 1506387403.0, 
            "domain": "arxiv.org", 
            "id": "72gwfx", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72gwfx/research_r_underwater_multirobot_convoying_using/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[Research] [R] Underwater Multi-Robot Convoying using Visual Tracking by Detection with Deep Learning", 
            "url": "https://arxiv.org/abs/1709.08292"
        }, 
        {
            "author": "NippedAtTheBud", 
            "created_utc": 1506377114.0, 
            "domain": "self.MachineLearning", 
            "id": "72fvc7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72fvc7/d_nips_student_volunteer_application/", 
            "score": 1, 
            "selftext": "Hi,\n\nApologies if this is the wrong place to ask this question.\n\nI'm a Masters student applying to be a student volunteer in this year's NIPS. \nAs a part of the application, my guide needs to send a recommendation letter. He's agreed to send the letter but not sure of what should be in the letter or how different it should be compared to, say a recommendation letter for admission into a graduate program.\n\nCould someone who has experience with the application process tell me what kind of a recommendation letter is expected? \n\nThanks!\n ", 
            "subreddit": "MachineLearning", 
            "title": "[D] NIPS Student Volunteer Application", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72fvc7/d_nips_student_volunteer_application/"
        }, 
        {
            "author": "madchops1", 
            "created_utc": 1506375441.0, 
            "domain": "dutchess.ai", 
            "id": "72fpc1", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72fpc1/p_crude_oil_futures_predictions_experiments_in/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Crude Oil Futures Predictions. Experiments in time-series data in AWS. - Dutchess.ai", 
            "url": "http://dutchess.ai"
        }, 
        {
            "author": "maximecb", 
            "created_utc": 1506373561.0, 
            "domain": "self.MachineLearning", 
            "id": "72fhsc", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72fhsc/d_creating_new_simulated_environments_for_rl/", 
            "score": 3, 
            "selftext": "I'm relatively new to machine learning and looking for ways I can leverage the expertise I do have to contribute something useful. I have some experience making games and simulations, and so I thought one possibility would be for me to create new simulated environments for reinforcement learning.\n\nBefore I go into this project, I thought I would come here and ask for ideas and suggestions as to what could be useful in this domain. Whether people think 2D environments are sufficient to perform interesting research, or would like to see 3D worlds (eg: pixels to actions). Another possibility is that I could make this into some kind of a library for building your own environments with minimal effort.\n\nIf you are an RL researcher, or are familiar with the fields, what do you think would be most useful? What do you think is lacking in current systems? What are current problems being studied in RL, and do you think environments could help your research? Would also like to know how people feel about OpenAI gym. I could possibly contribute to this project, or release something with a simple Python interface as a pip package.\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Creating new simulated environments for RL, looking for feedback", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72fhsc/d_creating_new_simulated_environments_for_rl/"
        }, 
        {
            "author": "Dakkudakkudakku", 
            "created_utc": 1506369267.0, 
            "domain": "self.MachineLearning", 
            "id": "72f043", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72f043/d_realtime_image_compression/", 
            "score": 1, 
            "selftext": "Hi /r/MachineLearning,\n\nI am developing a compression algorithm for real-time image compression. As the images are from a specific domain, i think i should be able to outperform general compression algorithms like JPEG. Note that I have a huge amount of training data. My idea is to use some sort of convolutional autoencoder, where i encode the bottlenet representation into binary to get the code. I plan to optimize for the reconstruction loss, maybe also to use adversarial training.\n\nBefore committing to this approach, i wanted to ask if you guys see any flaws in this? Maybe you know some recent research in machine learning based image compression? I found some papers on deconv nets, and RNN based approaches. What is the current \"way-to-go\"? In what direction would you point me?\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Real-time image compression", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72f043/d_realtime_image_compression/"
        }, 
        {
            "author": "VinayUPrabhu", 
            "created_utc": 1506363151.0, 
            "domain": "self.MachineLearning", 
            "id": "72eau7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72eau7/d_tishbys_opening_the_black_box_of_deep_neural/", 
            "score": 2, 
            "selftext": "I worked in Information Theory (IT) and Naftali is a well established guy.His work on the Information-Bottleneck framework is especially well known and is taught is most doctoral IT courses (at-least in India and US).\nIn the above mentioned paper, he posits that unreasonable reasonableness of DNNs (specifically MLPs) in terms of its generalization abilities can be reasonably explained via the IB framework.\nWhile sifting through the video lecture [ goo.gl/NkswVy ] of this paper (now popularized by the Quanta magazine article), I chanced upon his comments, that read:\n'(1) This work wasn't good enough for NIPS 2017...\ufeff\n(2) NIPS, ICML, and our similar conferences anonymous and superficial reviewing is apparently unable to deal with unusual papers, as this one.  I am very lucky this talk/work receives attention eventually, but this noisy review process is obviously inappropriate for such a dynamic field. I wish the creativity of the ML community can find better ways to judge and spread important contributions.\ufeff'\n\nThese is damning assessment of the review process which IMHO finds strong resonance from many quarters.\nIt would be helpful if the Reddit community here could share your opinions/experiences in this regard as well as your view about the paper.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Tishby's 'Opening the Black Box of Deep Neural Networks via Information' received a rejection from NIPS!", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72eau7/d_tishbys_opening_the_black_box_of_deep_neural/"
        }, 
        {
            "author": "xternalz", 
            "created_utc": 1506361707.0, 
            "domain": "arxiv.org", 
            "id": "72e4ju", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72e4ju/r_eraserelu_a_simple_way_to_ease_the_training_of/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural Networks", 
            "url": "https://arxiv.org/abs/1709.07634"
        }, 
        {
            "author": "gamahead", 
            "created_utc": 1506359358.0, 
            "domain": "self.MachineLearning", 
            "id": "72dus0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 96, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72dus0/discussion_serious_what_are_the_major_challenges/", 
            "score": 43, 
            "selftext": "I know that this sub is critical of discussions about things like AGI, but I want to hear some serious technical discussion about what the major challenges are that stand in the way of AGI. Even if you believe that it's too far away to take seriously, then I want to hear your technical reason for thinking so. \n\nEdit: Something like [Hilbert's problems](https://en.wikipedia.org/wiki/Hilbert%27s_problems) would be awesome", 
            "subreddit": "MachineLearning", 
            "title": "[Discussion] [Serious] What are the major challenges that need to be solved to progress toward AGI?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72dus0/discussion_serious_what_are_the_major_challenges/"
        }, 
        {
            "author": "_yoch_", 
            "created_utc": 1506358850.0, 
            "domain": "github.com", 
            "id": "72dsgb", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72dsgb/p_svmloader_a_fast_python_module_to_load_datasets/", 
            "score": 11, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] svmloader: a fast python module to load datasets in sparse CSR matrix", 
            "url": "https://github.com/yoch/svmloader"
        }, 
        {
            "author": "bl0wback_cat", 
            "created_utc": 1506354496.0, 
            "domain": "techcrunch.com", 
            "id": "72dad1", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72dad1/n_microsoft_launches_new_machine_learning_tools/", 
            "score": 75, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Microsoft launches new machine learning tools", 
            "url": "https://techcrunch.com/2017/09/25/microsoft-launches-new-machine-learning-tools/"
        }, 
        {
            "author": "hoaphumanoid", 
            "created_utc": 1506352096.0, 
            "domain": "azure.microsoft.com", 
            "id": "72d0pj", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72d0pj/n_diving_deep_into_whats_new_with_azure_machine/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Diving deep into what's new with Azure Machine Learning", 
            "url": "https://azure.microsoft.com/en-us/blog/diving-deep-into-what-s-new-with-azure-machine-learning/"
        }, 
        {
            "author": "Zahlii", 
            "created_utc": 1506350482.0, 
            "domain": "self.MachineLearning", 
            "id": "72cufk", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72cufk/d_imagebased_cnn_regression_advice_sought/", 
            "score": 0, 
            "selftext": "Hi everyone,\n\nas some of you might have read, I am working on an satellite image-based wind speed estimastion of tropical cyclones.\n\nI came up with an initial network architecture and training process with the following sourcecode:\nhttps://github.com/Zahlii/tropical-cyclone-speed-cnn/blob/master/process.py\n\nI am a bit unsure however, so I'd appreciate if you guys could have a look at the architecture and see whether there's any huge mistake with my approach.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Image-based CNN regression - advice sought", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72cufk/d_imagebased_cnn_regression_advice_sought/"
        }, 
        {
            "author": "XalosXandrez", 
            "created_utc": 1506349851.0, 
            "domain": "self.MachineLearning", 
            "id": "72cs23", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72cs23/d_model_compression_vs_training_from_scratch/", 
            "score": 7, 
            "selftext": "Hello all,\n\nModel compression for deep neural networks is a fairly popular research topic these days (it was much more popular an year or so ago). Does anyone know of any paper which compares performances of *compressed* models against those when the *compressed* models are trained from scratch?\n\nIn other words, we have two \"small\" models of the same architecture - one obtained from compressing a large model, another obtained by training the same small model from scratch. Have there been any studies which compare the relative performance of these two? \n\nThe only case I know of where these are compared are the knowledge distillation papers.\n\nThanks!", 
            "subreddit": "MachineLearning", 
            "title": "[D] Model compression vs Training from scratch", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72cs23/d_model_compression_vs_training_from_scratch/"
        }, 
        {
            "author": "saucysassy", 
            "created_utc": 1506349389.0, 
            "domain": "blog.qure.ai", 
            "id": "72cqbq", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72cqbq/p_visualizing_deep_learning_networks_a_summary/", 
            "score": 18, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Visualizing Deep Learning Networks - A Summary", 
            "url": "http://blog.qure.ai/notes/visualizing_deep_learning"
        }, 
        {
            "author": "jakn", 
            "created_utc": 1506343615.0, 
            "domain": "hameddaily.blogspot.de", 
            "id": "72c6pq", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72c6pq/p_imitation_learning_in_tensorflow_hopper_from/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Imitation Learning in Tensorflow (Hopper from openAI gym)", 
            "url": "http://hameddaily.blogspot.de/2017/09/imitation-learning-in-tensorflow-hopper.html"
        }, 
        {
            "author": "backpropper", 
            "created_utc": 1506342816.0, 
            "domain": "github.com", 
            "id": "72c492", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72c492/p_introducing_fabrik_build_visualize_and/", 
            "score": 157, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Introducing Fabrik: Build, visualize, and collaboratively design neural nets in the browser", 
            "url": "https://github.com/Cloud-CV/Fabrik"
        }, 
        {
            "author": "Reiinakano", 
            "created_utc": 1506339383.0, 
            "domain": "github.com", 
            "id": "72bu4z", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72bu4z/p_implementation_of_inbrowser_fast_style_transfer/", 
            "score": 34, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Implementation of in-browser Fast Style Transfer using Deeplearn.JS library (demo + code)", 
            "url": "https://github.com/reiinakano/fast-style-transfer-deeplearnjs/"
        }, 
        {
            "author": "TheNotKnower", 
            "created_utc": 1506336436.0, 
            "domain": "self.MachineLearning", 
            "id": "72bmgx", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72bmgx/d_question_how_to_solve_reinforcement_learning/", 
            "score": 6, 
            "selftext": "I apologize if this question is too simple for /r/ML (I'm not sure), but I [tried asking](https://www.reddit.com/r/MLQuestions/comments/7204d4/how_to_solve_reinforcement_learning_tasks_with/) on /r/MLQuestions and I didn't get an answer. \n\nAre there any machine learning techniques that can deal with input \"matrix\" of the form AxN and outputs of the form BxM, where A and B are constants but N and M very from time step to time step?\n\nExample task: There are a variable number of balls that fall from the top (Y=100) to the bottom (Y=0). On each time step new balls might appear. Each ball has a unique ID, a current Y coordinate, a vertical Velocity, an X coordinate between 0-9, and an X Target: [ID, Y, V, X, T] x N balls in play. On each time step, the AI can decide for each ball separately to push it to the left (X-=1) or right (X+=1): [ID,-/+] x M balls to push. The AI is \"punished\" with -1 for each pushed ball. Balls disappear when they hit each other or the bottom, and the AI gets a reward of +10 if the ball was in the right column (X=T).\n\nAre there any ML techniques that can *learn* this?\n\n(Bonus question: what if there are also different *kinds* of input objects? E.g. the AI can also observe floating platforms that should never be hit with coordinates for left, right and height, and a horizontal velocity.)\n\nThanks!", 
            "subreddit": "MachineLearning", 
            "title": "[D] [Question] How to solve (reinforcement learning) tasks with variable number of objects and actions on each timestep?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72bmgx/d_question_how_to_solve_reinforcement_learning/"
        }, 
        {
            "author": "esurior", 
            "created_utc": 1506335530.0, 
            "domain": "sentiance.com", 
            "id": "72bkfk", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72bkfk/p_deep_learning_on_raw_sensor_data_to_distinguish/", 
            "score": 24, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Deep learning on raw sensor data to distinguish drivers from passengers", 
            "url": "http://www.sentiance.com/2017/09/25/deep-learning-on-passenger-and-driver-behavior-analysis-using-sensor-data/"
        }, 
        {
            "author": "deeplyrandom", 
            "created_utc": 1506331602.0, 
            "domain": "myurasov.github.io", 
            "id": "72bba9", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72bba9/p_formulaless_explanation_and_keras/", 
            "score": 33, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Formula-less explanation and Keras implementation of Wasserstein GAN", 
            "url": "https://myurasov.github.io/2017/09/24/wasserstein-gan-keras.html?r"
        }, 
        {
            "author": "hardmaru", 
            "created_utc": 1506320570.0, 
            "domain": "arxiv.org", 
            "id": "72anue", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72anue/r_using_simulation_and_domain_adaptation_to/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping", 
            "url": "https://arxiv.org/abs/1709.07857"
        }, 
        {
            "author": "ML_WAYR_bot", 
            "created_utc": 1506315145.0, 
            "domain": "self.MachineLearning", 
            "id": "72ab5y", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 24, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72ab5y/d_machine_learning_wayr_what_are_you_reading_week/", 
            "score": 111, 
            "selftext": "This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\n\nPrevious weeks :\n\n|1-10|11-20|21-30|31-40|\n|----|-----|-----|-----|\n|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|[Week 11](https://www.reddit.com/r/MachineLearning/comments/57xw56/discussion_machine_learning_wayr_what_are_you/)|[Week 21](https://www.reddit.com/r/MachineLearning/comments/60ildf/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 31](https://www.reddit.com/r/MachineLearning/comments/6s0k1u/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|[Week 12](https://www.reddit.com/r/MachineLearning/comments/5acb1t/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 22](https://www.reddit.com/r/MachineLearning/comments/64jwde/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|[Week 13](https://www.reddit.com/r/MachineLearning/comments/5cwfb6/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 23](https://www.reddit.com/r/MachineLearning/comments/674331/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|[Week 14](https://www.reddit.com/r/MachineLearning/comments/5fc5mh/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 24](https://www.reddit.com/r/MachineLearning/comments/68hhhb/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|[Week 15](https://www.reddit.com/r/MachineLearning/comments/5hy4ur/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 25](https://www.reddit.com/r/MachineLearning/comments/69teiz/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|[Week 16](https://www.reddit.com/r/MachineLearning/comments/5kd6vd/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 26](https://www.reddit.com/r/MachineLearning/comments/6d7nb1/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 7](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|[Week 17](https://www.reddit.com/r/MachineLearning/comments/5ob7dx/discussion_machine_learning_wayr_what_are_you/)|[Week 27](https://www.reddit.com/r/MachineLearning/comments/6gngwc/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 8](https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/)|[Week 18](https://www.reddit.com/r/MachineLearning/comments/5r14yd/discussion_machine_learning_wayr_what_are_you/)|[Week 28](https://www.reddit.com/r/MachineLearning/comments/6jgdva/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 9](https://www.reddit.com/r/MachineLearning/comments/54kvsu/machine_learning_wayr_what_are_you_reading_week_9/)|[Week 19](https://www.reddit.com/r/MachineLearning/comments/5tt9cz/discussion_machine_learning_wayr_what_are_you/)|[Week 29](https://www.reddit.com/r/MachineLearning/comments/6m9l1v/d_machine_learning_wayr_what_are_you_reading_week/)||\n|[Week 10](https://www.reddit.com/r/MachineLearning/comments/56s2oa/discussion_machine_learning_wayr_what_are_you/)|[Week 20](https://www.reddit.com/r/MachineLearning/comments/5wh2wb/d_machine_learning_wayr_what_are_you_reading_week/)|[Week 30](https://www.reddit.com/r/MachineLearning/comments/6p3ha7/d_machine_learning_wayr_what_are_you_reading_week/)||\n\nMost upvoted papers two weeks ago:\n\n/u/olBaa: [most upvoted paper of the previous week](https://arxiv.org/abs/1707.05926)\n\n/u/PassiveAgressiveHobo: [Can GAN Learn Topological Features of a Graph?](https://arxiv.org/abs/1707.06197)\n\n/u/GChe: https://www.coursera.org/learn/machine-learning-projects\n\nBesides that, there are no rules, have fun.\n\n**Disclosure: I haven't updated this bot in nearly a month. I fixed a bug that prevented it from posting the last few weeks. I should probably post the source code on Github... Sorry for the holdup!**", 
            "subreddit": "MachineLearning", 
            "title": "[D] Machine Learning - WAYR (What Are You Reading) - Week 32", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/72ab5y/d_machine_learning_wayr_what_are_you_reading_week/"
        }, 
        {
            "author": "downtownslim", 
            "created_utc": 1506313428.0, 
            "domain": "wired.co.uk", 
            "id": "72a6iu", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/72a6iu/n_curiousai_we_have_solved_the_problem_we_are/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] CuriousAI: \"we have solved the problem. We are able to invoke neural networks that are able to generate speech\"", 
            "url": "http://www.wired.co.uk/article/harri-valpola-curious-ai-artificial-intelligence-third-wave"
        }, 
        {
            "author": "Neatzski", 
            "created_utc": 1506305034.0, 
            "domain": "self.MachineLearning", 
            "id": "729it1", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/729it1/d_learning_ml_for_the_latex_minded/", 
            "score": 7, 
            "selftext": "How should someone who's more interested in the math-heavy theory side of ML learn the field?\nAs it stands, I'm a handy with proof-based Linear Algebra and Analysis and have a smattering of Probability. Programming wise, I'm a functional programming nerd, so reading Chris Olah's blog made me interested in ML.\nIf I want to explore the \"purer\" side of ML so that I can learn the things I'd find in Olah's blog to full depth, what should I do? Should I just pick up the Deep Learning book and go from there? Does anyone have a set of resources to recommend?\nFor what it's worth, I'm not opposed to coding things, I just prefer generality and rigor (e.g. \"Backprop is just the chain rule.\").\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Learning ML for the LaTeX minded.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/729it1/d_learning_ml_for_the_latex_minded/"
        }, 
        {
            "author": "princealiiiii", 
            "created_utc": 1506290963.0, 
            "domain": "abidlabs.github.io", 
            "id": "728793", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/728793/d_3_ways_to_remove_noise_from_datasignal/", 
            "score": 23, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] 3 Ways to Remove Noise from Data/Signal", 
            "url": "https://abidlabs.github.io/removing-noise-from-signal/"
        }, 
        {
            "author": "harvey_slash", 
            "created_utc": 1506282325.0, 
            "domain": "self.MachineLearning", 
            "id": "7279un", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7279un/d_looking_for_a_collaborative_to_implement/", 
            "score": 0, 
            "selftext": "Theres this awesome paper which I think is going to be seminal in the field of style transfer. \nIt requires PatchMatch in order to make the whole pipeline work. \nRight now i have more or less finished PatchMatch, but there is a bug which is preventing me from moving further. \n\nIf you're interested in looking to collaborate on a really cool project, please respond. \nWe will discuss further, and if we both agree , i will add you as a collaborator. \n\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Looking for a Collaborative to implement awesome style transfer (Details in description)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7279un/d_looking_for_a_collaborative_to_implement/"
        }, 
        {
            "author": "FrigoCoder", 
            "created_utc": 1506260834.0, 
            "domain": "faculty.ucmerced.edu", 
            "id": "725441", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/725441/r_the_method_of_auxiliary_coordinates_mac/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] The method of auxiliary coordinates (MAC)", 
            "url": "http://faculty.ucmerced.edu/mcarreira-perpinan/MAC"
        }, 
        {
            "author": "Andome", 
            "created_utc": 1506258768.0, 
            "domain": "ml-showcase.com", 
            "id": "724yhc", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/724yhc/p_a_curated_collection_of_interesting_machine/", 
            "score": 181, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] A curated collection of interesting machine learning projects", 
            "url": "https://ml-showcase.com/"
        }, 
        {
            "author": "chubbyspartn", 
            "created_utc": 1506226094.0, 
            "domain": "self.MachineLearning", 
            "id": "7231jj", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7231jj/p_a_few_questions_on_building_a_linear_neural/", 
            "score": 1, 
            "selftext": "I am working on developing a neural network from scratch. Each of my nodes are linear and each layer is fully connected. The neurons are represented by a function\n\n    N = a1*x1 + . . . + an*xn + b,\n\nwhere a's are weights of the neurons, x's are inputs, and b is the bias. The neurons fire with Relu activation so the true value of N would be max{0, N}.\n\nAs of right now all of the framework is complete and I have a simple game that I am testing the neural network on. However I am not getting the results I expect.\n\n\nI believe my current problems lie in back propagation. As of now i am updating each weight, say ai by taking the partial of my output neuron, say O, respect to ai. So after each forward pass of the neural network I measure the outcome to get an error and then update each weight cj by \n\n    cj += dO/dcj * stepSize * error\nMy partial derivitevs are taken on the function \n\n    N = a1*x1 + . . . + an*xn + b  for each neuron\n\nrather than on the function \n\n    max{0, N}. \n\n\n   \n\nI was wondering if there is more that needs to be done for back propagation and if my techniques seem sound.\n\nI would also like to know if using linear functions is acceptable or if it might be better to choose other functions. I picked linear functions due to their partials being very easy to compute.\n\nLastly if you know of any good resources on neural networks of a similar design, I would be interested in seeing them. Many of the ones that I have found are either two basic, dealing with neural networks of 4 or so neurons. Or they are far to general that I have difficulty implementing the ideas to my specific framework.", 
            "subreddit": "MachineLearning", 
            "title": "[P] A few questions on building a linear neural network from scratch.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7231jj/p_a_few_questions_on_building_a_linear_neural/"
        }, 
        {
            "author": "wassname", 
            "created_utc": 1506219110.0, 
            "domain": "arxiv.org", 
            "id": "722iko", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/722iko/r_cyclical_learning_rates_for_training_neural/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Cyclical Learning Rates for Training Neural Networks", 
            "url": "https://arxiv.org/abs/1506.01186"
        }, 
        {
            "author": "dirac-hatt", 
            "created_utc": 1506195016.0, 
            "domain": "github.com", 
            "id": "720cwq", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/720cwq/p_tensorflow_implementation_of_squeezedet_object/", 
            "score": 94, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] TensorFlow Implementation of SqueezeDet (Object Detection for Autonomous Driving)", 
            "url": "https://github.com/fregu856/2D_detection"
        }, 
        {
            "author": "swentso", 
            "created_utc": 1506188354.0, 
            "domain": "self.MachineLearning", 
            "id": "71zonu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71zonu/d_did_you_guys_ever_tried_to_set_your_ml_model_as/", 
            "score": 14, 
            "selftext": "I want to be able to consume my model written in python (tensorflow / sklearn) as a REST API. What are the good ways to do it properly ? Any tools that might help ?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Did you guys ever tried to set your ML model as a REST API ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71zonu/d_did_you_guys_ever_tried_to_set_your_ml_model_as/"
        }, 
        {
            "author": "undefdev", 
            "created_utc": 1506174463.0, 
            "domain": "arxiv.org", 
            "id": "71ycs0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71ycs0/r_zhusuan_a_library_for_bayesian_deep_learning/", 
            "score": 74, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] ZhuSuan: A Library for Bayesian Deep Learning", 
            "url": "https://arxiv.org/abs/1709.05870v1"
        }, 
        {
            "author": "gauthamz", 
            "created_utc": 1506159835.0, 
            "domain": "gauthamzz.github.io", 
            "id": "71xe8h", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71xe8h/audio_style_transfer_using_cyclegan_project/", 
            "score": 25, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Audio Style transfer using CycleGAN \"[Project]\"", 
            "url": "https://gauthamzz.github.io/2017/09/23/AudioStyleTransfer/"
        }, 
        {
            "author": "DeepDeeperRIPgradien", 
            "created_utc": 1506158508.0, 
            "domain": "self.MachineLearning", 
            "id": "71xbof", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71xbof/d_something_like_artemissacred_for_custom_data/", 
            "score": 10, 
            "selftext": "We had discussions here about how people set up and organize their ML experiment. Tools like Artemis and Sacred were suggested, i.e. tools that save and track results of different parameters used in a model.\n\nNow my question is: are similar tools available for data used for these experiment? In my case, my images/videos are somewhere stored in folder and I have set up a database that contains annotations/labels as well as paths to the images, so I can query the database to get data for training/validation/testing. However, I'm not operating on fixed, public datasets, but on custom, self-recorded data that is collected and annotated over a time span of weeks to months. This means that models that were trained and tested on data from January will be re-trained and tested on additional data from March. \n\nI'm wondering if other people have a similar experiment strcuture and what tools or pipeline they use to keep track of the data they used at what time for which iteration of the model. ", 
            "subreddit": "MachineLearning", 
            "title": "[D] Something like Artemis/Sacred for (custom) data (sets)?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71xbof/d_something_like_artemissacred_for_custom_data/"
        }, 
        {
            "author": "LF_78", 
            "created_utc": 1506158315.0, 
            "domain": "self.MachineLearning", 
            "id": "71xbby", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71xbby/d_dnn_vs_cnn_vs_rnn_vs_lstm_for_time_series/", 
            "score": 33, 
            "selftext": "My input is a time series (*n* values for each data point), and my output is a layer of 3 neurons with boolean output. I plan to train the network with reinforcement learning.\n\n- Which is the suggested network architecture in this specific scenario? Other models than those listed in the subject are welcome as well of course.\n\n- Based on the answer to the first question: how do you suggest to feed the input, as a single data point, or as a moving window of *N* data points?\n\nAny pointer to documents or resources debating this topic is more than welcome.\n\nThanks!", 
            "subreddit": "MachineLearning", 
            "title": "[D] DNN vs. CNN vs. RNN vs. LSTM for time series?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71xbby/d_dnn_vs_cnn_vs_rnn_vs_lstm_for_time_series/"
        }, 
        {
            "author": "sw-padme-amidala", 
            "created_utc": 1506157082.0, 
            "domain": "self.MachineLearning", 
            "id": "71x8xs", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71x8xs/r_model_for_shape_classification/", 
            "score": 6, 
            "selftext": "Hi r/MachineLearning, I'm new into machine learning and looking for a model to classify shapes. To be more precise circles or round objects. I read a couple of articles and sources about the ImageNet classification models, but I'm uncertain if they suite. Shapes are more like attributes.\n\nI'm interested in Reddit's opinion. Can someone suggest a model or some best practices to create a model for shape (circle, round objects) classification or recognition? Thanks and have create day.", 
            "subreddit": "MachineLearning", 
            "title": "[R] Model for shape classification", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71x8xs/r_model_for_shape_classification/"
        }, 
        {
            "author": "luminousridge", 
            "created_utc": 1506147509.0, 
            "domain": "self.MachineLearning", 
            "id": "71wprg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71wprg/d_judging_quality_of_lstms_generated_text/", 
            "score": 7, 
            "selftext": "For an LSTM trained to generate text, is there a way to evaluate how well the network has learned to 'imitate' to write the style of text reflected in the training data? Or more simply, to show that the network learned something of value at all.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Judging quality of LSTM's generated text", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71wprg/d_judging_quality_of_lstms_generated_text/"
        }, 
        {
            "author": "soccer_account", 
            "created_utc": 1506128356.0, 
            "domain": "i.redd.it", 
            "id": "71vafj", 
            "is_reddit_media_domain": true, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71vafj/d_planning_ai_for_a_toddler/", 
            "score": 59, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Planning AI for a toddler! \ud83e\udd26\u200d\u2642\ufe0f", 
            "url": "https://i.redd.it/twsimpzc7jnz.jpg"
        }, 
        {
            "author": "phobrain", 
            "created_utc": 1506124872.0, 
            "domain": "self.MachineLearning", 
            "id": "71uzri", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71uzri/d_how_safe_is_all_this_code/", 
            "score": 0, 
            "selftext": "Just in case this is a leading-edge problem, I'm not sure where else to mention it: Has anyone heard of strange corruption in a router after an ubuntu+conda+tensorflow install? \n\nAfter a 12 hour install marathon, I have what might be the latest and greatest for Ubuntu 16.04, with a 1080 ti cranking away and warming my leg, on a wired connection. A few hours after leaving it to run, I found the router password no longer worked in wireless and console mode. Last I know it worked was a few days ago. After power cycling the router, it worked again, but there was no record of the password-failed login attempts according to the net provider, or records of activity from unexpected sources, or any sign of error. A lot of nvidia and other software downloaded slowly at ~300K over fiber in console mode during the install, not sure if that's a symptom of a router with internal problems preventing logging.. 10+ hours to do that setup, I hope it's not corrupt.\n\nCompiled tf after running w/ a built pkg and wanting the speedup and lack of SSE/AVX messages, so went through 2 versions of that plus 3 versions of cuda drivers, default 375.82, 381.22, and 384.69.", 
            "subreddit": "MachineLearning", 
            "title": "[D] How safe is all this code?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71uzri/d_how_safe_is_all_this_code/"
        }, 
        {
            "author": "SerpentAI", 
            "created_utc": 1506124087.0, 
            "domain": "github.com", 
            "id": "71uxa5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 34, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71uxa5/p_serpentai_game_agent_framework_turn_any_video/", 
            "score": 439, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Serpent.AI - Game Agent Framework. Turn ANY video game in a sandbox environment for AI & Bot programming. (Beta Release)", 
            "url": "https://github.com/SerpentAI/SerpentAI"
        }, 
        {
            "author": "throwAwayObama", 
            "created_utc": 1506110675.0, 
            "domain": "self.MachineLearning", 
            "id": "71tlby", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 17, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71tlby/d_what_hardware_setup_do_you_need_to_do_decent/", 
            "score": 9, 
            "selftext": "What sort of hardware setup do you need in order to work on the data sets they have in Kaggle competitions? \n\nHow important it is to have a GPU? Solid state drive? 16 Gigs ram? \n\nWhat are the most import hardware set ups? ", 
            "subreddit": "MachineLearning", 
            "title": "[D] What hardware setup do you need to do decent ML/DS work? Lets say being able to compete in Kaggle competitions.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71tlby/d_what_hardware_setup_do_you_need_to_do_decent/"
        }, 
        {
            "author": "napster226", 
            "created_utc": 1506108846.0, 
            "domain": "self.MachineLearning", 
            "id": "71te4e", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71te4e/d_how_can_ml_be_applied_to_political_campaigns/", 
            "score": 0, 
            "selftext": "From an ML novice: It seems like political campaigns are a bit behind when it comes to implementing data science, likely because of the short timeframe and limited resources on a campaign. Still though, how could machine learning be applied to political campaigning?", 
            "subreddit": "MachineLearning", 
            "title": "[D] How can ML be applied to political campaigns?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71te4e/d_how_can_ml_be_applied_to_political_campaigns/"
        }, 
        {
            "author": "george_lul", 
            "created_utc": 1506103486.0, 
            "domain": "self.MachineLearning", 
            "id": "71ssfr", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 25, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71ssfr/d_does_anyone_find_the_current_research_trend_in/", 
            "score": 27, 
            "selftext": "The time to come up with a new network architecture is much, much less than the time for results to get out.\n\nFor example, there is highway network, then resnet as its offspring and then DenseNet which is a trivial modification to resnet. I'm talking about 1 day to come up and code the idea, then  1 or more months to get the results.\n\nThere is nothing to prevent anyone from putting a new architecture on arXiv without experiment. On the other hand, the experiments take too long to do. It may hurt someone who can come up with a really well thought-out architecture because the idea will get scooped by bigger labs who have way more GPUs. (oh no you didn't do imagenet dataset 10 times)\n\nHow should we solve this? Should we have a standard on how big the network is? Should we just reject any architecture that doesn't provide any reason why it will work?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Does anyone find the current research trend in deep learning a bit pathological?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71ssfr/d_does_anyone_find_the_current_research_trend_in/"
        }, 
        {
            "author": "singham", 
            "created_utc": 1506092219.0, 
            "domain": "self.MachineLearning", 
            "id": "71rigz", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 23, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71rigz/d_what_features_do_you_expect_machine_learning_to/", 
            "score": 12, 
            "selftext": "What features do you expect Machine Learning to add to next generation of IDEs ? There is tonne of open source code available on Github. Can abstractions learned by an AI system be applied to writing code given that programs are far more restrictive and less ambiguous than natural languages?\n\nIs it possible for ML to learn common patterns across different languages and suggest them to users if there is partial match ? \nCan ML understand the current structure of the function and suggest ways to implement it better?  ", 
            "subreddit": "MachineLearning", 
            "title": "[D] What features do you expect Machine Learning to add to modern IDEs ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71rigz/d_what_features_do_you_expect_machine_learning_to/"
        }, 
        {
            "author": "BrianOcor", 
            "created_utc": 1506092196.0, 
            "domain": "phys.org", 
            "id": "71rie3", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71rie3/n_new_software_turns_mobilephone_accessory_into/", 
            "score": 14, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] New software turns mobile-phone accessory into breathing monitor", 
            "url": "https://phys.org/news/2017-09-software-mobile-phone-accessory.html"
        }, 
        {
            "author": "brunoalano", 
            "created_utc": 1506091716.0, 
            "domain": "self.MachineLearning", 
            "id": "71rgib", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71rgib/d_encode_coordinates_in_rnn/", 
            "score": 1, 
            "selftext": "Actually I started thinking about how can I efficiently encode coordinates (0, 1) map into my RNN.\n\nIn this model I have a nested relation between a **Event** and the sequence of coordinates of this Event. Kinda like: **User** *has many* **Events** that *has many* **Coordinates**.\n\nIn this way, first of all, I have actually a RNN-LSTM that encodes each User's Event at timestep, and a parallel CNN-LSTM that encodes User's Coordinates at each timestep with a CNN encoding it.\n\nFirst implementation:\nhttps://imgur.com/a/dVOiG\n\n**The Problem**\n\nSince using a time distributed CNN is expensive, I'm looking a better way to encode my coordinates. Any thoughts about how can I do that?\n\nSorry if something is not clear.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Encode Coordinates in RNN", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71rgib/d_encode_coordinates_in_rnn/"
        }, 
        {
            "author": "whateverr123", 
            "created_utc": 1506089590.0, 
            "domain": "github.com", 
            "id": "71r8kj", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71r8kj/p_pix2code/", 
            "score": 82, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] pix2code", 
            "url": "https://github.com/tonybeltramelli/pix2code"
        }, 
        {
            "author": "PaperTapir", 
            "created_utc": 1506089582.0, 
            "domain": "self.MachineLearning", 
            "id": "71r8jb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71r8jb/d_idea_for_estimating_future_workload_at_a/", 
            "score": 3, 
            "selftext": "Hey all! I work in manufacturing and have recently taken an interest in machine learning and all the cool things you can do with it. I have an idea for a potential use at my job but would need input to see if it makes sense at all! \n\nSo basically, I have been trying to come up with better ways to predict how many workers my company would require at any given time for the next month or so. This would allow us to avoid scheduling conflicts and *foresee* any instances where we would need to call in workers for overtime to continue running at maximal efficiency, or approve more PTO if workers are not needed.\n\nSome background on the manufacturing:\n\nOur company manufactures chemicals and we run multiple batches (as opposed to a continuous line of production) of different products at any given time on many pieces of equipment. To start a process, Chemical A will need to be sucked into a Reactor A using Equipment A, then Chemical B is added using Equipment B and they react. While Equipment B is in use, another process using Reactor C may be at the stage where it is ready to also use Equipment B, but needs to wait for first process to finish using it. Each of these 'steps' can typically be classified into some kind of 'unit operation' that more or less will always require the same amount of workers (if run in vacuum). There are maybe 3 or 4 categories of solids additions, 2 categories of liquids additions, 12 different types of reactions, and so on and so forth. For the sake of simplicity, we can also say that each category of unit operation will take the same amount of time to perform once started.\n\nAt this point it would be fairly simple to calculate how many workers are needed at a given time just by adding up whatever our process timeline would spit out. \"If process A is already using equipment B, then push back process B to wait for completion.\" What complicates the situation is that the same workers will sometimes run process A and process B at the same time if their respective pieces of equipment are in the same area. This will also affect how long each step takes. This is entirely dependent on the unique combination of equipment in use and other unit operations running at the time. If we have the manpower available, we are usually pretty good at assigning the right amount of workers to a process in the moment.\n\nIf we could track the real time data for what unit operations were running at a given time and how many workers were needed in actuality to do the work, we could come up with some kind of model to predict number of workers. Naturally, we have recently created tracking software that does exactly this. We can see how long each step took in reality, and we can see how many workers participated in completing it. Based off the same tracking software, we can also tell what other pieces of equipment are in use at that time and what unit operation they are in use with.\n\nThe machine learning algorithm I am envisioning would predict the amount of workers needed and the amount of time needed for each step to complete, based off the input of what unit operations are running on which pieces of equipment. We could then feed the algorithm with the number of workers actually assigned to a particular area and how long things actually took. We could include confounding factors like plant shutdowns or outages of particular equipment in the model to make it more representative of reality. Over time, the model will learn to recognize patterns and tell us what we want to know.\n\nHas anything like this been done before? How far removed is this from what machine learning is normally used for? Could I come close to the amount of data needed to train my machine learning model? Am I completely bonkers for thinking this would work?\n\nInput would be much appreciated! Really curious to see if something like this would be within the realm of possibility.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Idea for estimating future workload at a company based on predefined work operations", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71r8jb/d_idea_for_estimating_future_workload_at_a/"
        }, 
        {
            "author": "xingdongrobotics", 
            "created_utc": 1506070439.0, 
            "domain": "self.MachineLearning", 
            "id": "71prec", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71prec/d_can_someone_use_pytorch_if_they_work_in/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Can someone use PyTorch if they work in Deepmind or OpenAI ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71prec/d_can_someone_use_pytorch_if_they_work_in/"
        }, 
        {
            "author": "andyzth", 
            "created_utc": 1506068019.0, 
            "domain": "arxiv.org", 
            "id": "71pmna", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71pmna/r_n2n_learning_network_to_network_compression_via/", 
            "score": 17, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning", 
            "url": "https://arxiv.org/abs/1709.06030v1"
        }, 
        {
            "author": "Mussky", 
            "created_utc": 1506067990.0, 
            "domain": "github.com", 
            "id": "71pmko", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71pmko/p_simple_svm_classification_rest_api_with_python/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Simple SVM Classification REST API with Python", 
            "url": "https://github.com/mg52/svmapi"
        }, 
        {
            "author": "vkazemi", 
            "created_utc": 1506064932.0, 
            "domain": "quantamagazine.org", 
            "id": "71pfwn", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 68, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71pfwn/d_new_theory_cracks_open_the_black_box_of_deep/", 
            "score": 234, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] New Theory Cracks Open the Black Box of Deep Learning", 
            "url": "https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/"
        }, 
        {
            "author": "seann999", 
            "created_utc": 1506062328.0, 
            "domain": "self.MachineLearning", 
            "id": "71p9qg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71p9qg/d_paper_recommendation_system/", 
            "score": 2, 
            "selftext": "I was wondering if there was a recommender system for papers based on similar researchers liking similar topics/papers.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Paper recommendation system?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71p9qg/d_paper_recommendation_system/"
        }, 
        {
            "author": "ryches", 
            "created_utc": 1506055774.0, 
            "domain": "self.MachineLearning", 
            "id": "71otgj", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71otgj/d_what_is_included_in_the_various_nips_events/", 
            "score": 5, 
            "selftext": "I was very interested in going to the NIPS Conference since it will be somewhat near my area this year but see that the \"conference\" is already sold out but they say that the tutorial and workshops are still available but that workshops are selling out soon too. \n\nCould any one tell me what exactly is included in each, tutorial, workshop and conference? Based on the color coding it seems like there are a ton of workshops and some tutorials, but then there are also sections marked invited talk and symposium. Are symposium and invited talks what is included in the \"conference\" event? I am not understanding what I will be missing out on if I just get the workshop and tutorial tickets. \n\nThis is my first rodeo. Not familiar with the format. ", 
            "subreddit": "MachineLearning", 
            "title": "[D] What is included in the various NIPS Events?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71otgj/d_what_is_included_in_the_various_nips_events/"
        }, 
        {
            "author": "ndpian", 
            "created_utc": 1506055549.0, 
            "domain": "arxiv.org", 
            "id": "71ostm", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71ostm/r_optiongan_learning_joint_rewardpolicy_options/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning", 
            "url": "https://arxiv.org/abs/1709.06683"
        }, 
        {
            "author": "minglu", 
            "created_utc": 1506045200.0, 
            "domain": "github.com", 
            "id": "71nxh7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71nxh7/p_the_deep_supervised_hashing_for_image_retrieval/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] The Deep Supervised Hashing for Image Retrieval on CIFAR10/MNIST/Fashion-MNIST", 
            "url": "https://github.com/mingloo/DeepSupervisedHashing"
        }, 
        {
            "author": "galapag0", 
            "created_utc": 1506042427.0, 
            "domain": "arxiv.org", 
            "id": "71nobr", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71nobr/r_classsplitting_generative_adversarial_networks/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Class-Splitting Generative Adversarial Networks improve state-of-the-art Inception scores using CIFAR-10 and STL-10", 
            "url": "https://arxiv.org/abs/1709.07359"
        }, 
        {
            "author": "chavid90", 
            "created_utc": 1506029329.0, 
            "domain": "self.MachineLearning", 
            "id": "71mdy4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71mdy4/p_visualising_decision_features_for_facial/", 
            "score": 4, 
            "selftext": "Applied the method discussed for localising objects in an image using class activation maps (Zhou et al. 2016) to fer2013 facial expression recognition dataset. Found some pleasant to the eye facial feature visualisations used in the decision. \n\nSome fun ones are:\n\n- Angry uses eyebrows as a strong feature\n\n- In Happy we see in some examples where highlighted cheeks are used as a feature \n\n- Surprised is obsessed with wide open eyes and space between the nose and upper lip\n\ngithub for code and example outputs\nhttps://github.com/chavdim/facial_expressions_cam\n", 
            "subreddit": "MachineLearning", 
            "title": "[P] Visualising decision features for facial expressions using class activation maps", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71mdy4/p_visualising_decision_features_for_facial/"
        }, 
        {
            "author": "feather_of_maat", 
            "created_utc": 1506020537.0, 
            "domain": "medium.com", 
            "id": "71le4t", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71le4t/r_matterport3d_research_dataset_a_massive_set_of/", 
            "score": 103, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Matterport3D Research Dataset: a massive set of labeled 3D scans of buildings, and what it enables", 
            "url": "https://medium.com/@llebttam_45762/announcing-the-matterport3d-research-dataset-815cae932939"
        }, 
        {
            "author": "psr10000", 
            "created_utc": 1506018400.0, 
            "domain": "rajpurkar.github.io", 
            "id": "71l53g", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71l53g/d_want_to_visualize_what_your_convnet_is_learning/", 
            "score": 54, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Want to visualize what your ConvNet is learning? Blogpost summarizing a couple of approaches!", 
            "url": "https://rajpurkar.github.io/mlx/visualizing-cnns/"
        }, 
        {
            "author": "nested_dreams", 
            "created_utc": 1506015283.0, 
            "domain": "cloudplatform.googleblog.com", 
            "id": "71krqm", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 35, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71krqm/n_google_adding_nvidia_p100_gpus_to_cloud/", 
            "score": 155, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Google Adding NVIDIA P100 GPUs to Cloud Offerings", 
            "url": "https://cloudplatform.googleblog.com/2017/09/introducing-faster-GPUs-for-Google-Compute-Engine.html"
        }, 
        {
            "author": "i_know_about_things", 
            "created_utc": 1506014642.0, 
            "domain": "arxiv.org", 
            "id": "71koxg", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71koxg/r_170906560_deep_reinforcement_learning_that/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] [1709.06560] Deep Reinforcement Learning that Matters", 
            "url": "https://arxiv.org/abs/1709.06560"
        }, 
        {
            "author": "fixedrl", 
            "created_utc": 1506012227.0, 
            "domain": "self.MachineLearning", 
            "id": "71keoe", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71keoe/d_difficulty_comparison_of_cartpole_swing_up_vs/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Difficulty comparison of CartPole Swing up vs Gym Pendulum ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71keoe/d_difficulty_comparison_of_cartpole_swing_up_vs/"
        }, 
        {
            "author": "warpri81", 
            "created_utc": 1506010078.0, 
            "domain": "momixa.com", 
            "id": "71k5k8", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71k5k8/p_momixa_applying_word_embedding_neural_networks/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Momixa - Applying word embedding neural networks to playlist generation", 
            "url": "https://momixa.com"
        }, 
        {
            "author": "clockworkmischief", 
            "created_utc": 1506008844.0, 
            "domain": "ihes.fr", 
            "id": "71k0dt", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71k0dt/d_memorandum_ergo/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Memorandum Ergo", 
            "url": "http://www.ihes.fr/~gromov/PDF/ergo-cut-copyOct29.pdf"
        }, 
        {
            "author": "flotothemoon", 
            "created_utc": 1506004356.0, 
            "domain": "self.MachineLearning", 
            "id": "71ji5v", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 27, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71ji5v/p_sigma_creating_a_machine_learning_framework/", 
            "score": 122, 
            "selftext": "**TLDR**: Asked this subreddit for advice in deciding on ML topic for high school thesis 2 years ago (see [original thread]( https://www.reddit.com/r/MachineLearning/comments/3jn5xf/question_what_interesting_machine_learning/)), ended up writing a machine learning framework from (almost) scratch in C#/F#. It can\u2019t do as much as all the others, isn\u2019t as fast or as pretty, but we still think it\u2019s kind of cool. Here it is: our [github repo]( https://github.com/ThinkingTransistor/Sigma) and a short [UI demo](https://gfycat.com/HospitableCriminalAlpaca).\n# Results\n\nUpfront the current feature set of our framework [Sigma](https://github.com/ThinkingTransistor/Sigma), to give you an idea of what the next few paragraphs are about:\n\n* Input, Output, Dense, Dropout, Recurrent, SoftmaxCE / SquaredDiff cost layers\n* Gradient descent, Momentum, Adadelta, Adagrad optimisers\n* Hooks for storing / restoring checkpoints, timekeeping, stopping (or doing other things) on certain criteria, computing and reporting runtime metrics\n* Easy addition of new layers with functional automatic differentiation\n* Linear and non-linear networks with arbitrarily connected constructs\n* Distributed multi- and single- CPU and GPU (CUDA) backends \n* Native graphical interface where parameters can be interacted with and monitored in real-time\n\n&nbsp;\n\n---\n\n&nbsp;\n# 1. Introduction\n\nThis is the story of us writing a machine learning framework for our high school thesis, of what we learned and how we went about writing one from scratch. The story starts about 3 years ago: we saw a video of [MarI/O](https://www.youtube.com/watch?v=qv6UVOQ0F44), a Super Mario AI that could learn to play Super Mario levels. We thought that was about the coolest thing of all time and wanted to do something at least kind of similar for our high school thesis. \n\n&nbsp;\n## 1.1 The Original Plan\n\nFast forward, over 2 years ago we asked for help in deciding what kind of machine learning project we could feasibly do for our senior year high school thesis (see [original thread]( https://www.reddit.com/r/MachineLearning/comments/3jn5xf/question_what_interesting_machine_learning/)). Quite ambitiously, we proposed a time investment of about 1000 hours total (as in 500 hours each over the course of 8 months) \u2013 for what project, we didn\u2019t know yet. In that thread, we were generously met with a lot of help, advice ranged from reproducing existing papers to implementing specific things to getting to understand the material and then seeing what peaked our interest. After some consideration, we figured we would implement something along the lines of DeepMinds arcade game AI and then make it more general, figuring that would be easy for some reason. When planning our project in more detail we however quickly realised that \n\n1. we had no idea what we were doing and \n2. it would be a shame to do all that work from scratch and have it be so arbitrarily specific. \n\n&nbsp;\n## 1.2 Pivoting\n\nBefore doing anything very productive, we had to properly study machine learning. We figured this might take a while and allocated that part of our time to writing the theoretical part of our thesis, which conveniently overlapped. But because around that time we had to hand in an official target definition for our thesis, we set the most generic \u201cgoals\u201d we could get away with. For reference, for a concerning number of months our project was officially named \u201cSoftware framework for diverse machine learning tasks\u201d with an even longer and even less specific subtitle. During further study and first attempts to draft the actual target definition for our project, our plans gradually shifted from a machine learning framework for playing specific types of games pivoted to an \u201cany kind of visual input\u201d learning framework and then finally to an \u201canything\u201d machine learning framework \u2013 because why not, it seemed like an interesting challenge and we were curious to see how far we would get.\n\n&nbsp;\n\n---\n&nbsp;\n# 2. Research and Planning\n\nAlright, so we've decided to write a machine learning framework. How does one create a machine learning framework? It takes many weeks to get reasonably proficient in just using a given framework, and that with proper guides, video tutorials and forums to ask for help. Creating a machine learning framework is a whole other story, with no 12 step guidelines to follow. For a considerable amount of time, we were at a loss at what we actually needed to implement \u2013 constantly encountering new and conflicting terms, definitions and not-so-obvious-\u201cbut the actual conclusion is obvious\u201d-articles. After a little over a month we slowly got a very basic grasp of how this whole machine learning thing worked \u2013 something with functions that are approximated at certain points in steps typically using differentiation to make some metric go down \u2013 still magic, but a bit less so (until we read about CNNs, LSTMs and then GANs, each of which confused the heck out of us for some time). \n\n&nbsp;\n## 2.1 Sketching our Framework\n\nAs soon as we knew a bit about the art of machine learning we got more serious about the writing a new framework part. Because there are no guides for that, we resorted to reading the source code of established frameworks \u2013 all to us relevant parts, many times, until it made some sense. In the meantime, we had decided to use C# as our primary language \u2013 mostly because we were already very familiar with it and didn\u2019t want to also have to learn a new language, but officially also because there were no other proper neural network frameworks for .NET. Alongside reading the source code of machine learning libraries (mainly Deeplearning4J, Brainstorm and Tensorflow) we sketched out how we wanted our own framework to be used. After some time, we felt like there was some unnecessary confusion in getting to know machine learning frameworks as an outsider and we set out to design our API to avoid that. Note that because our design makes sense to us doesn\u2019t mean that it makes more sense than the existing ones to other people, nor do we recommend everyone wishing to use machine learning to write their own framework, just to spare their own sanity.  \n\nOur na\u00efve ideas on how a machine learning should look like was clearly inspired by our C#/Java based programming experience, as is evident from the code example we drafted a few weeks in:\n\n    Sigma sigma = Sigma.Create(\"minsttest\");\n    GUIMonitor gui = (GUIMonitor) sigma.AddMonitor(new GUIMonitor(\"Sigma GUI Demo\"));\n    gui.AddTabs({\"Overview\", \"Data\", \"Tests\"});\n    \n    sigma.Prepare();\n    \n    DataSetSource inputSource = new MultiDataSetSource(new FileSource(\"mnist.inputs\"), new CompressedFileSource(new FileSource(\"mnist.inputs.tar.gz\"), new URLSource(\"http://....url...../mnist.inputs.targ.gz\")));\n    DataSetSource targetSource = new MultiDataSetSource(new FileSource(\"mnist.targets\"), new CompressedFileSource(new FileSource(\"mnist.targets.tar.gz\"), new URLSource(\"http://....url...../mnist.targets.targ.gz\" [, output: \"otherthandefault\"]) [, compression: new TarGZUnpacker(), output: \"mnist.inputs\" , forceUpdate: false]));\n    DataSet data = new DataSet(new ImageRecordReader(inputSource, {28, 28}).Extractor({ALL} => {inputs: {Extractor.BatchSize, 1, 28, 28}}).Preprocess(Normalisor()), new StringRecordReader(targetSource).Extractor({0} => {targets: {Extractor.BatchSize, 1}} [, blockSize: auto/all/1024^3]);\n    \n    Network network = new Network(\"mynetwork\");\n    network.Architecture = Input(inputShape: {28, 28}) + 2 * FullyConnected(size: 1024) + SoftmaxCE() + Loss();\n    \n    Trainer trainer = sigma.CreateTrainer(\"mytrainer\");\n    trainer.SetNetwork(network);\n    trainer.SetInitialiser(new GaussianInitialiser(mean: 0.0, standardDeviation: 0.05));\n    trainer.SetTrainingDataIterator(MinibatchIterator(batchSize: 50, data[\"inputs\"], data[\"targets\"]);\n    trainer.SetOptimiser(new SGDOptimiser(learningRate: 0.01);\n    trainer.AddActiveHook(EarlyStopper(patience: 3));\n    trainer.AddActiveHook(StopAfterEpoch(epoch: 2000));\n    \n    gui.AccentColor[\"trainer1\"] = Colors.DeepOrange;\n    gui.tabs[\"overview\"].AddSubWindow(new LineChartWindow(name: \"Error\", sources: {\"*<Trainer>.Training.Error\"}) [, x: 1, y: 0, width: 2, height: 1]);\n    gui.tabs[\"overview\"].AddSubWindow(new LineChartWindow(name: \"Accuracy\", sources: {\"*<Trainer>.Training.Accuracy\"}));\n    \n    sigma.Run();\n    \nAnd skipping ahead a bit, it should be noted that the final framework is extremely similar to what we envisioned here: merely changing around a few syntax things and names, the above example from about a year ago can be used 1:1 in our current framework. The jury is still out on whether that\u2019s a sign of really good or really bad design. Also note the python-style kwargs notation for layer constructor arguments, which was soon discarded in favour of something that actually compiles in C#. But back to the timeline.\n\n&nbsp;\n## 2.2 The Sigma Architecture\n\nAfter defining the code examples and sketching out the rough parts we felt a machine learning framework needed, we arrived at this general architecture for \u201cSigma.Core\u201d, divided into core components (which translate almost 1:1 to namespace in our project):\n\n* **Util**: Mostly boring, well, utility stuff, but also registries, a key part of our architecture. Because we wanted to be able to inspect and visualise everything we needed a global way to access things by identifier \u2013 a registry. Our registry is essentially a dictionary with a string key which may contain more registries. Nested registries can be resolved using registry resolvers in dot notation with some fancy wildcards and tags in angel brackets (e.g. \u201cnetwork.layers.*<fullyconnected>.weights\u201d). \n* **Data**:\tDatasets, the records that make them up in various formats, the pipeline to load, extract, prepare and cache them from disk, web, or wherever they come from and make them available as \u201cblocks\u201d. These blocks are parts of an extracted dataset, consist of many individual records, and are used to avoid loading all of a potentially very large dataset into memory at once. Also, data iterators, which slices larger blocks from datasets into pieces that are then fed to the model. \n* **Architecture**: Abstract definitions for machine learning models, consisting of layer \u201cconstructs\u201d, which are lightweight placeholder layers defining what a layer will look like before its fully instantiated. These layers may be in any order and connected with however many other layers they would like. \n* **Layers**: Unfortunately named since we started out with just neural networks, but these are the individual layers of our machine learning networks \u2013 they store meta-parameters (e.g. size) and actual trainable parameters (e.g. the actual weights).\n* **Math**:  Everything that has directly to do with math and low-level computations. All the automatic differentiation logic (which is very much required for doing proper machine learning) and everything that modifies our data is processed here in various backends (e.g. distributed CPU / GPU). To support calculating derivatives with respect to anything we opted for an approach with symbolic objects \u2013 essentially an object for a number or an array where the actual data was hidden (it can be fetched, but only via copies). These symbolic objects are passed around through a handler which does the actual data modification. This abstraction proved to be useful when implementing CUDA support where, due to the asynchronous execution of the CUDA stream, the raw data could not exposed to the user anyway, at least not without major performance hits (host-device synchronisation is very slow). \n* **Training**: The largest component with many subcomponents, all revolving around the actual training process. A training process is defined in a \u201ctrainer\u201d, which specifies the following: \n  * **Initialisers**, that define how a models parameters are initialised, which can be configured with registry identifiers. For example, trainer.addInitialiser(\u201clayers.*.biases\u201d, new GaussianInitialiser(0.1, 0.0)); would initialise all parameters named \u201cbiases\u201d with a Gaussian distribution of 0.1 (mean 0). \n  * **Modifiers**, that would modify parameters at runtime, for example to clip weights to a certain range. \n  * **Optimisers**, that define how a model learns (e.g. gradient descent). Because we mainly considered neural networks we only implemented gradient based optimisers, but the interface theoretically supports any kind of optimisation. \n  * **Hooks**, that \u201chook\u201d into the training process at certain time steps and can do whatever you want (e.g. update visualisations, store / restore checkpoints, compute and log metrics, do something (e.g. stop) when some criteria are satisfied). \n  * **Operators**, that delegate work to workers which execute it with a certain backend computation handler according to some parameters. Notable is our differentiation of \u201cglobal\u201d and \u201clocal\u201d processing, where global is the most recent global state. This global state is fetched by local workers that then do the actual work, publish their results to the operator which merges it back into the global scope. A global timestep event is only ejected when all local workers have submitted their work for that timestep, enabling more fine control in distributed learning, at least in theory. \n* **Sigma**: The root namespace that can create Sigma environments and trainers. An environment may contain multiple trainers, which are all run and, if specified, visualised simultaneously (which was supposed to be helpful in hyperparameter search). \n* **Monitors**: Technically outside of the core project, but still a component. These monitors can be attached to a Sigma environment and can then, well, monitor almost everything about the trainers of that environment using the aforementioned registry entries. Behaviour can be injected using commands, a special form of hooks that are only invoked once. This way monitors can be used almost independently of the core Sigma project and can be pretty much anything, like a graphical interface or a live, locally hosted website.\n\n&nbsp;\n\n---\n&nbsp;\n# 3. Implementation\n\nAnd that\u2019s what we implemented, step by step. We started out with me mainly working on Sigma.Core and my partner on our visualisation interface, working to a common interface for months until we could finally combine our individual parts and have it miraculously work in a live graphical interface. The specifics of implementation were very interesting and quite challenging to us, but most of the particulars are probably rather dull to read \u2013 after all, most of the time things didn\u2019t work and when we fixed something, we moved on to the next something that didn\u2019t.  \n\n&nbsp;\n## 3.1 Low-level Data and Mathematical Processing\n\nThe very first thing we did was getting the data \u201cETL\u201d (extract transform load) pipeline up and running, mainly fetching data from a variety of sources, loading them into a dataset and extracting them as blocks. I then focused on the mathematical processing part \u2013 everything that had to do with using math and calculating derivatives in our framework. I based our functional automatic differentiation, aptly named \u201cSigmaDiff\u201d, on an F# library for autodiff named \u201cDiffSharp\u201d, which I modified heavily to support n-dimensional arrays, improve performance significantly, fix a few bugs, support multiple simultaneous non-global backends, variable data types, and some more stuff that I\u2019m forgetting. The specific details of getting that to work aren\u2019t very interesting \u2013 a lot of glue code, refactoring and late-night bug-chasing because the backpropagation didn\u2019t work as it should with some specific combination of operations. One memorable bug was that when remapping backend operations to my own OpenBLAS-based backend I forgot that matrix transposition did more than just change its shape \u2013 a mistake that cost me weeks in debugging efforts down the line, because things just didn\u2019t work properly with large layers or more than 1 record per minibatch (duh). \n\n&nbsp;\n## 3.2 Performance Optimisations\n\nCompared to all the backend work, the \u201cmiddleware\u201d of layers and optimisers was rather trivial to implement, as there are hundreds of tutorials and papers on how to create certain layers and optimisers, where I only had to map them to our own solution. Really, that part should have taken a few weeks at most, but took that much longer because we only then discovered dozens of bugs and stability issues. Skipping over a lot of uninteresting details here, it should be noted that at this point performance of the framework was quite bad. I\u2019m talking 300ms/iteration of 100 MNIST records with just a few dense layers on a high-end computer bad. This bad performance not only slowed training but also actual development down by quite a lot, hiding a few critical bugs and never letting us test the entire framework in a real-world use case within a reasonable time. You might wonder why we didn\u2019t just fix the performance from the get-go, but we wanted to make the actual training work first so we would have something to show for our thesis. In hindsight not the ideal choice, but it still worked out quite well and otherwise we wouldn\u2019t have been able to demonstrate our project adequately in time for the final presentation. \n\n&nbsp;\n### 3.2.1 A Self-Adjusting Buffer\n\nIt took many months before we finally got around to addressing the performance issues, but there was no single fix in sight, rather a collection of hundreds of small to medium sized improvements. A major issue was the way our SigmaDiff math processor handled operations: for every operation, a copy was created for the resulting data. That added up. The copying was necessary because backwards differentiation requires all intermediate values, so we couldn\u2019t just not copy things. We couldn\u2019t even create all required buffers in a static way ahead of time because there was (and still is) no way to traverse the operations that will be executed \u2013 the computation graph is constructed anew every time, and we can\u2019t completely rely on them to remain constant. To introduce a reliable way of buffering anyway, we introduced the concept of sessions: A session was meant to be a set of operations that would be repeated many times. Iterations, essentially. When a session is started we would start storing all created arrays in our own store and when an array of the same dimensions was requested in the next session we could return the one from last session, all without allocating any new memory. If more memory was required than last time, we could still allocate it, if less was used, we could discard it for the next session, rendering this neatly self-adjusting. To not overwrite data that was created within a session but was needed for the next one (e.g. parameters) we added an explicit \u201climbo\u201d buffer, which was basically just a flag that could be set at runtime for a certain array that marked it as \u201cdo not reuse\u201d. \n\n&nbsp;\n### 3.2.2 SIMD and Avoiding Intermediate Allocation\n\nOther significant performance improvements were adding SIMD instructions (which enables processing of typically 8 values at the same time for CPU-bound arithmetic operations) wherever possible and reducing other memory allocation to a minimum by adding some in-place operations wherever intermediate values weren't strictly needed. For example, copying results when accumulating gradients during backpropoagation on nodes with multiple operands is unnecessary because the intermediate values aren't used. By analysing profilers to death, I eventually got the iteration time for my MNIST sample down to an acceptable 18ms in release configuration (speedup of about 17x). Incidentally, the core was now so fast that our visualiser sometimes crashed because it couldn\u2019t keep up with all the incoming data. \n\n## 3.3 Monitoring with Sigma\n\n&nbsp;\n### 3.3.1 The monitoring System\n\nWhen developing Sigma, we not only focused on the \u201cmathematical\u201d backend but also implemented a feature rich monitoring system which allows any application to be built on top of Sigma (or better said Sigma.Core). Every parameter can be observed, every change hooked, every parameter managed. With this monitoring system, we built a monitor (i.e. application) that can be used to learn Sigma and machine learning in general. \n\n&nbsp;\n### 3.3.2 The WPF Monitor\n\nUsers should be able to not only use Sigma, but also **learn** with Sigma. To address this issue, we built a feature-rich application (with WPF) that allows users to interact with Sigma. plot learning graphs, manage parameters and control the AI like controlling a music player. This monitor, as every other component of Sigma, is fully customisable and extensible. All components were designed with reusability in mind, which allows users to build their own complex application on top of the default monitor. But why describe a graphical user interface? See it for yourself, here is the UI (and Sigma) in action. (Example builds of Sigma can be downloaded on GitHub).\n\n\n[Learn to learn at the press of a button](https://gfycat.com/HospitableCriminalAlpaca)\n\n[Direct interaction during the learning process](https://gfycat.com/TediousDiscreteArmedcrab)\n\n[Save, restore and share checkpoints](https://gfycat.com/LastElderlyBelugawhale)\n\n\n## 3.4 CUDA Support and Finishing Touches\n\n\nOnly 2 months ago we started finalising and polishing our framework: adding CUDA support, fixing many stability issues and rounding off a few rough spots that annoyed us. The CUDA support part was particularly tricky as I could only use CuBLAS, not CuDNN, because our backend doesn\u2019t, by design, understand individual layers but just raw computation graphs. A problematic side effect of the previously described session-logic was that there was no guarantee when buffers would be freed, as that was the job of the indeterministic GC. To not leak CUDA device memory I added my own bare-bones reference counter to the device memory allocator, which would be updated when buffers were created / finalised, which works surprisingly well. With CuBLAS, many custom optimised kernels and many nights of my time we achieved around 5ms/iteration for the same sample on a single GTX 1080, which we deemed acceptable for our envisioned use cases.\n\n\n&nbsp;\n\n---\n&nbsp;\n# 4. Conclusion\n\nApproximately 3000 combined hours, tens of thousands of lines of code and many long nights later we are proud to finally present something we deem reasonably usable for what it is: [Sigma](https://github.com/ThinkingTransistor/Sigma), a machine learning framework that might help you understand a little bit more about machine learning. As of now, we probably won\u2019t be adding many new features to Sigma, mainly because we\u2019re working on a new project related to it that\u2019s now taking up most of our available time. Even though it lacks a lot of default features (most importantly the host of default layer types other frameworks offer), we\u2019re quite happy with how far we got with our project and hope that it\u2019s an adequate update to our original question 2 years ago. We would be happy if some of you could check it out and give us some feedback. \n\n&nbsp;\n\n## 4.1 The Cost of Creating a Machine Learning Framework \n\nExcluding time, it\u2019s quite cheap. Honestly, with some solid prior programming experience (so that the low-level programming part doesn't become an issue), the whole thing isn't terribly difficult and is probably something most people could do, given enough time. A lot of time. Overall, we it took us approximately: \n\n1. Some 600 hours of research\n2. Some 2400 hours of development\n3. 2 tortured souls, preferably sold to the devil in exchange for less bugs\n\nWe have long since stopped properly counting, so take these numbers with a grain of salt, but they should be in the right ballpark. \n\n&nbsp;\n  \n## 4.2 Final remarks \nAll in all, an undertaking like this is extremely time intensive. It was very much overkill for a high school thesis from the get-go, and we knew that, but it just kept getting more and more elaborate, essentially taking up all of our available time and then some. It was definitely worth it though, for now we have a solid understanding how things work on a lower level and, most importantly, we can say we\u2019ve actually written a machine learning framework, which grants us additional bragging rights :)\n", 
            "subreddit": "MachineLearning", 
            "title": "[P] Sigma - Creating a machine learning framework from scratch (Update on high school thesis advice thread)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71ji5v/p_sigma_creating_a_machine_learning_framework/"
        }, 
        {
            "author": "rzepeda1", 
            "created_utc": 1505986768.0, 
            "domain": "self.MachineLearning", 
            "id": "71i16c", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71i16c/d_idea_about_helping_with_earthquake_damage/", 
            "score": 4, 
            "selftext": "Hi guys im new here and know very little about Machine Learning, im a civil engineer actually and with what happen in Mexico some colleagues post on different social media platform so people can send pictures of the damage on their homes and they can give an opinion of the level of damage\n\nthat basically depends on the direction and type of crack, thickness and lenghth of the crack.( in LATAM the construction material is CMU for homes, but it can be an input depending on the case, along with a few of other factors)\n\nso far a lot of people send pictures and some of them have been really busy volunteering on that.\n\ndo you think is something machine learning can help with ? how would it work ? and how could i get start with that ? \n\nThanks for the comments! \n\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Idea about Helping with Earthquake Damage assessment", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71i16c/d_idea_about_helping_with_earthquake_damage/"
        }, 
        {
            "author": "voiruloo", 
            "created_utc": 1505986572.0, 
            "domain": "github.com", 
            "id": "71i0q9", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71i0q9/p_binary_latent_representations_for_efficient/", 
            "score": 21, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Binary Latent Representations for Efficient Ranking", 
            "url": "https://github.com/maciejkula/binge"
        }, 
        {
            "author": "yik_yak_paddy_wack", 
            "created_utc": 1505970096.0, 
            "domain": "self.MachineLearning", 
            "id": "71gyee", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71gyee/d_pytorch_tesnorboard_crayon_vs_tensorboardpytorch/", 
            "score": 11, 
            "selftext": "I was wondering what people's opinions are on the two ([tensorboard-pytorch][1] and [crayon][2])? \n[1]: https://github.com/lanpa/tensorboard-pytorch\n[2]: https://github.com/torrvision/crayon", 
            "subreddit": "MachineLearning", 
            "title": "[D] Pytorch + Tesnorboard: crayon vs tensorboard-pytorch", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71gyee/d_pytorch_tesnorboard_crayon_vs_tensorboardpytorch/"
        }, 
        {
            "author": "evc123", 
            "created_utc": 1505963349.0, 
            "domain": "youtube.com", 
            "id": "71geuw", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": {
                "oembed": {
                    "author_name": "O'Reilly", 
                    "author_url": "https://www.youtube.com/user/OreillyMedia", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/NQK4ZY_gwKI?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/NQK4ZY_gwKI/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "AI is the new electricity. - Andrew Ng (Coursera)", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 107, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71geuw/d_andrew_ng_ranks_how_valuable_current_ml_methods/", 
            "score": 285, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Andrew Ng ranks how valuable current ML methods are", 
            "url": "https://www.youtube.com/watch?v=NQK4ZY_gwKI"
        }, 
        {
            "author": "alexbotev", 
            "created_utc": 1505962082.0, 
            "domain": "self.MachineLearning", 
            "id": "71gauw", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 17, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71gauw/d_scipy_lbfgsb_significantly_worse_than_minfunc/", 
            "score": 7, 
            "selftext": "So I recently got quite significantly worse results from `scipy.optimize.fmin_l_bfgs_b` compared to Matlab's minFunc. I've made a small example [here](https://gist.github.com/botev/c3196335d0afcbb46220ca3f021ca448) where this is a very very tiny network. The scipy code results in a value of `0.139` while the Matlab code easily found `0.00035`. \n\nThe question is does anyone have a good explanation for the large discrepancy?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Scipy LBFGS-B significantly worse than minFunc", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71gauw/d_scipy_lbfgsb_significantly_worse_than_minfunc/"
        }, 
        {
            "author": "Pfohlol", 
            "created_utc": 1505959125.0, 
            "domain": "arxiv.org", 
            "id": "71g0yo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71g0yo/r_170602633_realvalued_medical_time_series/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] [1706.02633] Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs", 
            "url": "https://arxiv.org/abs/1706.02633"
        }, 
        {
            "author": "_alphamaximus_", 
            "created_utc": 1505954946.0, 
            "domain": "alphagomovie.com", 
            "id": "71fmmt", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71fmmt/d_alphago_movie/", 
            "score": 22, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] AlphaGo Movie", 
            "url": "https://www.alphagomovie.com/"
        }, 
        {
            "author": "divye_kapoor", 
            "created_utc": 1505954854.0, 
            "domain": "engineering.linkedin.com", 
            "id": "71fmbi", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71fmbi/n_case_study_top_comments_on_linkedin/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Case Study: Top Comments on LinkedIn (Infrastructure, ML Architecture)", 
            "url": "https://engineering.linkedin.com/blog/2017/09/serving-top-comments-in-professional-social-networks"
        }, 
        {
            "author": "ndpian", 
            "created_utc": 1505954567.0, 
            "domain": "arxiv.org", 
            "id": "71flav", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71flav/r_deep_reinforcement_learning_that_matters/", 
            "score": 27, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] \"Deep Reinforcement Learning that Matters\"", 
            "url": "https://arxiv.org/pdf/1709.06560.pdf"
        }, 
        {
            "author": "inejc", 
            "created_utc": 1505932860.0, 
            "domain": "self.MachineLearning", 
            "id": "71d6sq", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71d6sq/p_a_pytorch_implementation_of_paragraph_vectors/", 
            "score": 7, 
            "selftext": "I'm implementing a library for training paragraph vector models as proposed by Q. V. Le et al. (Distributed Representations of Sentences and Documents).\n\nThe code is available on GitHub:\nhttps://github.com/inejc/paragraph-vectors\n\nI would appreciate any kind of feedback. Contributions in any form are also more than welcome (I have already opened some issues regarding future work).", 
            "subreddit": "MachineLearning", 
            "title": "[P] A PyTorch implementation of Paragraph Vectors (doc2vec)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71d6sq/p_a_pytorch_implementation_of_paragraph_vectors/"
        }, 
        {
            "author": "adagrad", 
            "created_utc": 1505927305.0, 
            "domain": "self.MachineLearning", 
            "id": "71cini", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71cini/d_how_do_you_organize_your_research/", 
            "score": 14, 
            "selftext": "As a new PhD student, I'm curious to hear from more experienced graduate students and industry researchers about best practices for organizing machine learning experiments, literature review, code and data organization, etc.", 
            "subreddit": "MachineLearning", 
            "title": "[D] How do you organize your research?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71cini/d_how_do_you_organize_your_research/"
        }, 
        {
            "author": "zionsrogue", 
            "created_utc": 1505919772.0, 
            "domain": "pyimagesearch.com", 
            "id": "71bnd9", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 51, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71bnd9/my_preconfigured_deep_learning_python_amazon_aws/", 
            "score": 219, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "My pre-configured Deep Learning + Python Amazon AWS AMI is publicly available for your use.", 
            "url": "http://www.pyimagesearch.com/2017/09/20/pre-configured-amazon-aws-deep-learning-ami-with-python/"
        }, 
        {
            "author": "fixedrl", 
            "created_utc": 1505913648.0, 
            "domain": "self.MachineLearning", 
            "id": "71b0h5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71b0h5/d_will_doubleblind_review_of_nips_causes_some/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Will double-blind review of NIPS causes some papers months later on ArXiv ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/71b0h5/d_will_doubleblind_review_of_nips_causes_some/"
        }, 
        {
            "author": "gtam_ds", 
            "created_utc": 1505849374.0, 
            "domain": "engineering.pivotal.io", 
            "id": "715gfr", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/715gfr/r_interpreting_decision_trees_and_random_forests/", 
            "score": 19, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Interpreting Decision Trees and Random Forests", 
            "url": "http://engineering.pivotal.io/post/interpreting-decision-trees-and-random-forests/"
        }, 
        {
            "author": "harvey_slash", 
            "created_utc": 1505846488.0, 
            "domain": "medium.com", 
            "id": "7153rp", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7153rp/d_dynamically_expandable_neural_networks_for/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Dynamically Expandable Neural Networks for compact, adaptive life long learning", 
            "url": "https://medium.com/@harshsayshi/dynamically-expandable-neural-networks-ce75ff2b69cf"
        }, 
        {
            "author": "crouching_dragon_420", 
            "created_utc": 1505844953.0, 
            "domain": "self.MachineLearning", 
            "id": "714x3k", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 28, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/714x3k/d_16gb_memory_gpu_for_rent/", 
            "score": 14, 
            "selftext": "Hi everyone,\nWe are doing some research using a novel convolution operation that is extremely memory hungry. We ran out of luck with our 1080 TIs and unfortunately, our budget also. Therefore, we are looking to rent GPU with 16 GB memory **per GPU** (like the P5000), preferably some with decent FP16 FLOPS (like the P40). If you know how to get some, please share it with us. Thank you.", 
            "subreddit": "MachineLearning", 
            "title": "[D] 16GB memory GPU for rent?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/714x3k/d_16gb_memory_gpu_for_rent/"
        }, 
        {
            "author": "evc123", 
            "created_utc": 1505839863.0, 
            "domain": "blogs.unity3d.com", 
            "id": "714aqh", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 24, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/714aqh/n_introducing_unity_machine_learning_agents/", 
            "score": 242, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Introducing: Unity Machine Learning Agents <-- interface to hook up ML Agents to Unity Game Engine", 
            "url": "https://blogs.unity3d.com/2017/09/19/introducing-unity-machine-learning-agents/"
        }, 
        {
            "author": "fng185", 
            "created_utc": 1505824297.0, 
            "domain": "eurekalert.org", 
            "id": "712n8u", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/712n8u/r_kernelpredicting_convolutional_networks_for/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings, SIGGRAPH 2017", 
            "url": "https://www.eurekalert.org/pub_releases/2017-06/dr-drp063017.php"
        }, 
        {
            "author": "flrngel", 
            "created_utc": 1505822472.0, 
            "domain": "github.com", 
            "id": "712hc9", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/712hc9/p_tensorflow_implementation_of_facebook_tagspace/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Tensorflow implementation of Facebook TagSpace", 
            "url": "https://github.com/flrngel/TagSpace-tensorflow"
        }, 
        {
            "author": "skariel", 
            "created_utc": 1505821300.0, 
            "domain": "arxiv.org", 
            "id": "712dv7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/712dv7/research_photorealistic_simulator_for_autonomous/", 
            "score": 15, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[Research] Photo-realistic simulator for autonomous vehicles", 
            "url": "https://arxiv.org/pdf/1708.05869.pdf"
        }, 
        {
            "author": "oximoroide", 
            "created_utc": 1505819898.0, 
            "domain": "self.MachineLearning", 
            "id": "7129uz", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 22, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7129uz/d_getting_a_spot_as_visiting_phd_student/", 
            "score": 18, 
            "selftext": "UPDATE: In case someone comes across this post in the future, here's my piece of advice. I finally got a good position, and I got it through my contacts. A friend of mine was studying at the destination so he put a word in with a professor there. Shortly before that, I also contacted a few Ph.D. students at good labs. Two out of three replied quite positively and almost instantly, and the third one might have done the same had I not told them that I had already found a position. My advice: don't contact professors. If your field is more or less active, I'm sure they get 10 e-mails a day from students wanting to work with them, so you cannot expect them to reply, even relatively unknown professors. If you don't know anyone who can recommend you at a good university, contact Ph.D. students. But don't spam. Only contact those you really want to work with, or whose work aligns well with yours. TL;DR: the comments here are actually very good advice. Use your network and/or contact Ph.D. students.\n--------\nOriginal post:\n\nI am currently doing a PhD and I'd like to spend a semester abroad. I already have a few publications, one at a top venue, a few others at good ones. I do not require funding from the receiving institution and I am willing to work hard.\n\nI have contacted quite a few professors already but my e-mails go mostly ignored. I have only got 2 responses out of 16 e-mails, both negative (\"apply through the established procedure\"; \"I got no more room\"). Some e-mails I sent weeks ago. I'm facing a deadline to get funding for this at my home institution so I'm getting a little (very) anxious. I try to keep e-mails short and to the point. The first ones were more lengthy, but the main bits were clear, I think.\n\nAny advice to get a position like this? Know of any destinations that might be receptive to this kind of thing? I get that good professors are busy and get a lot of requests, but I'd say this is a bargain. I'll basically work for you for free! Worst-case scenario: I'll hang out around your lab for a few months.\n\nEDIT: I'm at UPM (http://www.upm.es/). \nPublications: https://sites.google.com/site/ordozb/publications\nI've actually studied many different topics in ML aside from my publications. I'd mostly like to work on deep learning but I'd be happy to work on almost anything ML-related.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Getting a spot as visiting PhD student", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7129uz/d_getting_a_spot_as_visiting_phd_student/"
        }, 
        {
            "author": "percivalbreccia", 
            "created_utc": 1505808153.0, 
            "domain": "math.stackexchange.com", 
            "id": "711isi", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/711isi/discussion_why_use_weighted_sampling_instead_of/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[discussion] why use 'weighted sampling' instead of 'deterministic selection'", 
            "url": "https://math.stackexchange.com/questions/2431398/why-use-weighted-sampling-instead-of-deterministic-selection"
        }, 
        {
            "author": "themathstudent", 
            "created_utc": 1505804664.0, 
            "domain": "medium.com", 
            "id": "711ata", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/711ata/p_fake_news_classifier/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Fake News Classifier", 
            "url": "https://medium.com/@sachin.abeywardana/fake-news-classifier-e061b339ad6c"
        }, 
        {
            "author": "gicht", 
            "created_utc": 1505800606.0, 
            "domain": "github.com", 
            "id": "7110va", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7110va/p_kerasrl_needs_your_help_transitioning/", 
            "score": 40, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Keras-RL needs your help: transitioning maintenance to the community", 
            "url": "https://github.com/matthiasplappert/keras-rl/issues/132"
        }, 
        {
            "author": "_alphamaximus_", 
            "created_utc": 1505791668.0, 
            "domain": "medium.com", 
            "id": "710b9e", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/710b9e/d_the_ten_fallacies_of_data_science_towards_data/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] The Ten Fallacies of Data Science - Towards Data Science - Medium", 
            "url": "https://medium.com/@brennash/the-ten-fallacies-of-data-science-9b2af78a1862"
        }, 
        {
            "author": "jer_pint", 
            "created_utc": 1505788775.0, 
            "domain": "self.MachineLearning", 
            "id": "7101tb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 27, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7101tb/discussion_solving_a_rubiks_cube_using_a_simple/", 
            "score": 25, 
            "selftext": "Plenty of efficient algorithms exist to solve a rubik's cube. I was curious to find out if a neural net could learn how to solve a cube in the most \"efficient\" way, by solving the cube in less than 20 moves, i.e [god's number](http://www.cube20.org/).\n\nThis is a very naive solution, to start as a proof of concept. I used a 2 layer neural net: 1 convnet layer and 1 feedforward layer. The input is the state of the cube to be solved. The output is the next predicted move until solved. For the training set, I generated games at random during training for games of 10 moves or less from solved with the corresponding solutions as label. At each step of the solution, I made the network make a guess for its next move and used SGD for training. I trained this over many epochs until the loss was relatively steady.\n\nSurprisingly, the network works decently well for any position less than 6 moves away from solved. I found that by shuffling up to 6 moves, sometimes more, it is able to correctly solve it. It doesn't always work and can definitely use improvement, but its a good place to start.\n\n[Here's a video of it](https://youtu.be/QLUca-x2ZVo) in action. The source code is available [here](https://github.com/jerpint/rubiks_cube_convnet), just follow the instructions to shuffle the cube yourself and solve it with the network.\n\nThere is plenty of exploring to do! Bigger networks might be one solution, fancier networks would likely be more appropriate. I thought of reinforcement learning, but decided to use the simpler supervised-learning approach to begin.\n\nDISCLAIMER: The search-space for a properly mixed cube is HUGE (something like 4e19 iirc). The original thought was that a ConvNet might be able to solve such a space. I do think that a more sophisticated approach would be necessary to solve from more than 10 moves.  Maybe an idea would be to 'introduce' the cube to known algorithms and let it learn to optimise based on those. Also a reward system reinforcement approach could be a good idea.", 
            "subreddit": "MachineLearning", 
            "title": "[Discussion] Solving a Rubik's Cube using a Simple ConvNet", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7101tb/discussion_solving_a_rubiks_cube_using_a_simple/"
        }, 
        {
            "author": "A_WILD_STATISTICIAN", 
            "created_utc": 1505782004.0, 
            "domain": "medium.com", 
            "id": "70zebz", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70zebz/p_a_system_for_intelligent_pothole_detection/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] A system for Intelligent Pothole Detection using smartphone sensor data", 
            "url": "https://medium.com/@percepsense/intelligent-pothole-detection-879ef635dd38"
        }, 
        {
            "author": "dundermifflined", 
            "created_utc": 1505781880.0, 
            "domain": "self.MachineLearning", 
            "id": "70zdv5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70zdv5/d_sample_size_for_a_classification_problem_in/", 
            "score": 0, 
            "selftext": "I'm interested in learning about the standard practices or methods that are commonly used by researchers to justify the sample size for a classification problem in machine learning with clinical significance. For regression analysis, there are some well established methods such as ANOVA, power analysis, inverse power analysis, precision analysis, etc using which one can determine the sample size. However, for machine learning, I haven't come across such standard methods to justify the size of the sample size? If you are aware of such methods or practices that are commonly used in clinical research, kindly post them as comments below. Thanks.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Sample size for a classification problem in clinical research", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70zdv5/d_sample_size_for_a_classification_problem_in/"
        }, 
        {
            "author": "dexter4121", 
            "created_utc": 1505772598.0, 
            "domain": "self.MachineLearning", 
            "id": "70yg2i", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70yg2i/d_ai_composes_music/", 
            "score": 0, 
            "selftext": "https://futurism.com/a-new-ai-can-write-music-as-well-as-a-human-composer/\n\nHow would have they approached this problem?\nWhat features they would have used?", 
            "subreddit": "MachineLearning", 
            "title": "[D] AI composes music", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70yg2i/d_ai_composes_music/"
        }, 
        {
            "author": "alexmlamb", 
            "created_utc": 1505765774.0, 
            "domain": "youtube.com", 
            "id": "70xoba", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": {
                "oembed": {
                    "author_name": "The Nutty Netter (Alex Lamb)", 
                    "author_url": "https://www.youtube.com/channel/UC6YglsfUYu0iwySl_zyEcYA", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/buUF5F8UCH8?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/buUF5F8UCH8/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Learning to Act by Predicting the Future (Alex Lamb and Sherjil Ozair)", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70xoba/d_learning_to_act_by_predicting_the_future_alex/", 
            "score": 17, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Learning to Act by Predicting the Future (Alex Lamb and Sherjil Ozair)", 
            "url": "https://www.youtube.com/watch?v=buUF5F8UCH8"
        }, 
        {
            "author": "dimGG89", 
            "created_utc": 1505763176.0, 
            "domain": "self.MachineLearning", 
            "id": "70xdiu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70xdiu/d_discovering_differences_in_datasets/", 
            "score": 1, 
            "selftext": "Hello all,\n\nI am quite new to machine learning so please excuse any mistakes from my part. I have two datasets containing KPIs regarding the performance of two athletes. Each dataset contains 40 variables both qualitative and quantitative with around 900 records of data. The variables are not independent in general. \n\nMy goal is to find meaningful differences between the datasets in order to explain why an athlete performs better than the other and in which KPIs or combination of KPIs he does so. So far I was able to do so by visually comparing graphs which is a time consuming procedure. I would like to automate and possibly improve this procedure by using machine learning algorithms. My initial thought was to use some kind of a clustering algorithm. \n\nI would be grateful if someone could propose to me a methodology to follow or point me in the correct direction.\n\nThank you in advance!", 
            "subreddit": "MachineLearning", 
            "title": "[D] Discovering differences in datasets", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70xdiu/d_discovering_differences_in_datasets/"
        }, 
        {
            "author": "edunov", 
            "created_utc": 1505759616.0, 
            "domain": "github.com", 
            "id": "70wyq4", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70wyq4/p_pytorch_implementation_of_facebooks_seq2seq/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Pytorch implementation of Facebook's Seq2Seq", 
            "url": "https://github.com/facebookresearch/fairseq-py"
        }, 
        {
            "author": "lfotofilter", 
            "created_utc": 1505756569.0, 
            "domain": "nlml.github.io", 
            "id": "70wlnm", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70wlnm/p_want_to_understand_tsne_better_heres_a/", 
            "score": 52, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Want to understand t-SNE better? Here's a step-by-step guide to the math, with numpy implementation.", 
            "url": "https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/"
        }, 
        {
            "author": "gwern", 
            "created_utc": 1505750054.0, 
            "domain": "arxiv.org", 
            "id": "70vwou", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70vwou/r_learning_functional_causal_models_with/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] \"Learning Functional Causal Models with Generative Neural Networks\", Goudet et al 2017", 
            "url": "https://arxiv.org/abs/1709.05321"
        }, 
        {
            "author": "j_lyf", 
            "created_utc": 1505749546.0, 
            "domain": "twitter.com", 
            "id": "70vuj5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 382, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70vuj5/d_twitter_thread_on_andrew_ngs_transparent/", 
            "score": 825, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Twitter thread on Andrew Ng's transparent exploitation of young engineers in startup bubble", 
            "url": "https://twitter.com/betaorbust/status/908890982136942592"
        }, 
        {
            "author": "navoshta", 
            "created_utc": 1505746191.0, 
            "domain": "kernels.io", 
            "id": "70vhan", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70vhan/p_new_version_of_our_jupyter_notebook_client_for/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] New version of our Jupyter Notebook client for iPad is available for beta testing now \u2014 bringing code completion, fixes and more. (x-post from /r/IPython)", 
            "url": "https://kernels.io/kernels-v-1-0-3-beta/"
        }, 
        {
            "author": "tdionis", 
            "created_utc": 1505737973.0, 
            "domain": "hackernoon.com", 
            "id": "70uoqw", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70uoqw/p_how_we_hacked_gta_v_for_carvana_kaggle_challenge/", 
            "score": 41, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] How we Hacked GTA V for Carvana Kaggle Challenge", 
            "url": "https://hackernoon.com/hacking-gta-v-for-carvana-kaggle-challenge-6d0b7fb4c781"
        }, 
        {
            "author": "ramitchellnz", 
            "created_utc": 1505729902.0, 
            "domain": "devblogs.nvidia.com", 
            "id": "70u2py", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70u2py/r_gradient_boosting_using_gpus_with_xgboost/", 
            "score": 100, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Gradient boosting using GPUs with XGBoost", 
            "url": "https://devblogs.nvidia.com/parallelforall/gradient-boosting-decision-trees-xgboost-cuda/"
        }, 
        {
            "author": "Simoncarbo", 
            "created_utc": 1505728341.0, 
            "domain": "self.MachineLearning", 
            "id": "70tz1n", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 34, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70tz1n/discussion_what_are_the_problems_of_the/", 
            "score": 44, 
            "selftext": "Two days ago, an article quoting Hinton who was saying that backprop is not necessarily the way to go for AI, generated lots of very cool discussion on this sub-reddit ([here](https://www.reddit.com/r/MachineLearning/comments/70e4ex/n_hinton_says_we_should_scrap_back_propagation/?st=j7pzc8n6&sh=0c5dd149)).\n\nThe discussion mainly went in the direction of asking what are alternatives to backprop. In this discussion I would like us to answer the question: what are the problems of backprop? Indeed, the article didn't really answer this question, only refering shortly to the difficulties of unsupervised learning.\n\nHere's my initial input:  \nProblems that seem to be intrinsic to backprop :  \n\n* Continuous learning problems (backprop forgets previous knowledge)\n* Modularity (backprop uses all available connections automatically) (_edit: not very clear, could be removed_)\n* Robustness to specific noise (adversarial examples)\n\nProblems that currently pose lots of difficulties and we're not sure are possible with backprop:  \n\n* Learning reasoning/planning\n* Unsupervised learning\n\nDo you agree with these? What other problems have you noticed during your work?", 
            "subreddit": "MachineLearning", 
            "title": "[Discussion] What are the problems of the backpropagation algorithm?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70tz1n/discussion_what_are_the_problems_of_the/"
        }, 
        {
            "author": "ug96", 
            "created_utc": 1505722531.0, 
            "domain": "github.com", 
            "id": "70tmmo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70tmmo/r_cnn_fixations_an_unravelling_approach_to/", 
            "score": 19, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] CNN fixations: An unravelling approach to identify discriminative image regions", 
            "url": "https://github.com/val-iisc/cnn-fixations"
        }, 
        {
            "author": "mofoss", 
            "created_utc": 1505720200.0, 
            "domain": "jungle-ml.com", 
            "id": "70thr8", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70thr8/p_counting_word_frequencies_over_data_scienceml/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Counting Word Frequencies over Data Science/ML Job Postings on Indeed.com", 
            "url": "http://www.jungle-ml.com/2017/09/17/data-science-job-qualifications-via-web-scraping-indeed-com/"
        }, 
        {
            "author": "finallyifoundvalidUN", 
            "created_utc": 1505701032.0, 
            "domain": "arxiv.org", 
            "id": "70s1ry", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70s1ry/rimagenet_training_in_24_minutes/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R]ImageNet Training in 24 Minutes", 
            "url": "https://arxiv.org/abs/1709.05011"
        }, 
        {
            "author": "phobrain", 
            "created_utc": 1505691332.0, 
            "domain": "self.MachineLearning", 
            "id": "70r4zc", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70r4zc/d_schizophrenic_translation_of_images/", 
            "score": 6, 
            "selftext": "How hard would it be to train a net to output a photo of, say, a fish when presented with a bird? Training pairs might include complex scenes as well as simple objects:\n\n    http://phobrain.com/pr/home/view.html\n\nHow many 1080 Ti's would you apply to that problem?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Schizophrenic translation of images", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70r4zc/d_schizophrenic_translation_of_images/"
        }, 
        {
            "author": "undefdev", 
            "created_utc": 1505689828.0, 
            "domain": "self.MachineLearning", 
            "id": "70qzvz", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 36, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70qzvz/dmeta_more_moderation/", 
            "score": 63, 
            "selftext": "I noticed some changes in this subreddit during the last few months that I didn't like and wanted to know how most people here feel about this.\n\n*****\n\n### Disregard for the rules\n\nIt appears like there are many people posting here that haven't even read the rules - this is of course particularly obvious when looking at posts without appropriate Tag (this rule was in part made to verify that people read the rules).\n\nI think this is also a reason why the quality of posts in general declines... Bad posts encourage other people to make bad posts as well, and people who spend their time downvoting and reporting posts grow tired of this subreddit and go elsewhere, so I'd advocate more (at the very least automatic) moderation.\n\nThe same bot that applies the tag could just delete the post if there isn't any, the author could then post again *after* having read the rules - *if the post still seems appropriate*.\n\n\n### \"What should I do with my life?\" posts\n\nOf course we all understand that machine learning is super exciting and the more people throw their brains at it the better, but we shouldn't have to read everyone's life story and try to give personalized advice on how to proceed to become a \"machine learning person\".\n\nWe can't speak on behalf of universities or companies and we can't assess whether machine learning would be a good career path for someone we don't know.\n\nIf a question like this is not answered in the FAQ people should ask themselves whether it should be answered in the FAQ - if not, I believe the post would probably be useless to most readers of this subreddit and should maybe not be posted in the first place.\n\n\n### WAYR\n\nI personally think WAYR was pretty great and I don't understand why it's not stickied every week automatically.\n\n\n*****\n\nIf there are any other things you would like to bring up, please do - this is just what I could think of off the top of my head.\n", 
            "subreddit": "MachineLearning", 
            "title": "[D][Meta] More Moderation?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70qzvz/dmeta_more_moderation/"
        }, 
        {
            "author": "Phylliida", 
            "created_utc": 1505673945.0, 
            "domain": "self.MachineLearning", 
            "id": "70pd0i", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70pd0i/training_rnns_as_fast_as_cnns_doesnt_do_well/", 
            "score": 14, 
            "selftext": "Seeing [this](https://arxiv.org/abs/1709.02755) paper I decided to try them out. I used [char-rnn-pytorch](https://github.com/spro/char-rnn.pytorch) and simply added a third option to use the [sru](https://github.com/taolei87/sru). Because they have the same interface it was really easy to do.\n\nPerhaps I am misunderstanding them, but I didn't get good results at all. Perhaps I need to use different parameters? I tried larger sizes and more layers but it didn't seem to help. My accuracy went up a bit at the start and then stopped increasing, while for LSTMs and GRUs it kept going up quite a bit more (and had better generation results when the training was done). It also ran slightly slower than LSTMs and GRUs for a small network (2 layers of size 50 on GPU), though it may have ran faster for larger networks I don't remember but honestly I didn't care that much because it didn't work well.\n\nSo am I misunderstanding something? I can write up my results specifically but this is fairly repeatable if you want to try yourself and this point should give you the jist of what I experienced. I was hoping for faster LSTMs or GRUs so that was a little disappointing tbh.", 
            "subreddit": "MachineLearning", 
            "title": "Training RNNs as Fast as CNNs doesn't do well", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70pd0i/training_rnns_as_fast_as_cnns_doesnt_do_well/"
        }, 
        {
            "author": "JustinQueeber", 
            "created_utc": 1505667138.0, 
            "domain": "self.MachineLearning", 
            "id": "70on5w", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70on5w/d_how_to_correctly_batch_my_temporal_data_to_feed/", 
            "score": 6, 
            "selftext": "I have gained experience with building NNs and more specifically LSTM RNNs in TensorFlow on a very simple level. I am now trying to build far more advanced LSTMs, but am having trouble figuring out how to correctly batch the specific data I am using.\n\nMy dataset consists of temporal financial data. Each row of the dataset contains an ID number, a timestamp, ~100 features as real numbers and the labelled output value as a real number too. I am attempting to pass in all of these features (~100 real number inputs per time step) for each ID in order of their timestamps as a sequence into my LSTM and hence make a prediction of the labelled value at each timestamp. The biggest issue I am having is that each ID doesn't exist across all timestamps of the data set.\n\nThere are ~1800 timestamps and ~1400 IDs. My initial thought was to batch the data as a sliding window, similar to the diagram shown the in \"Variables and placeholders\" section of [this blog](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767).\n\n[Here is an example of the structure of my dataset and the first 5 batches I would create from it.](https://imgur.com/a/vvVCE)\n\nThe blue box shows each batch, the red indicates that this ID does not exist at this timestep (no data), and the green indicates that it does.\n\nIn this diagram, I am taking batches of 5 IDs over sequences of 3 timestamps. These 5 sequences would be processed through my NN and the total loss over the 5 sequences would then be calculated and thus the optimization applied. Of course, in reality I would actually be taking more IDs and longer sequences in each batch.\n\nClearly, at the earlier and later timestamps, there is a lot of missing data. How can I best work around this? Can I just pad the input sequences with 0s and set the labels as 0 too, or would this influence the backpropagation and optimization of my NN?\n\nIf padding with 0s is not the correct way around this, what is a better way to batch my data and feed it into the NN?", 
            "subreddit": "MachineLearning", 
            "title": "[D] How to correctly batch my temporal data to feed it into an LSTM?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70on5w/d_how_to_correctly_batch_my_temporal_data_to_feed/"
        }, 
        {
            "author": "vubjof", 
            "created_utc": 1505650983.0, 
            "domain": "self.MachineLearning", 
            "id": "70n6vi", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 88, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70n6vi/can_i_do_a_phd_in_machine_learning_with_an_md/", 
            "score": 15, 
            "selftext": "what is your opinion? \nobv i'd have to do alot of work on my math background", 
            "subreddit": "MachineLearning", 
            "title": "Can i do a Phd in machine learning with an MD degree?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70n6vi/can_i_do_a_phd_in_machine_learning_with_an_md/"
        }, 
        {
            "author": "KetoReddit", 
            "created_utc": 1505609920.0, 
            "domain": "self.MachineLearning", 
            "id": "70kmfn", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70kmfn/crossing_over_two_genomes_with_neat/", 
            "score": 3, 
            "selftext": "Hello all!\n\nI've had an ongoing project for a few months now, and I'm getting rather close.  The issue that I currently have is that I can't cross over two genomes without having explosive complexity from it.  I read in the MIT papers for NEAT (NeuroEvolution of Augmenting Topologies) that the genomes are to evolve incrementally, however, I'm having an incredibly difficult time with it.  I feel like part of it would be from giving the child the maximum amount of neurons between both parents, but in SethBling's other code ( https://repl.it/JHik/1 ), it seems to work just fine.  I directly replicated his code for crossing over into my own program but it seems to just over-complexify the population.\n\nIf there are any Python programmers out there, my code can be seen at the following link:\nhttps://repl.it/LCPn/17\n\nThe crossover function can be seen on line 365.  Currently, it is replaced by simply copying one parent's genome.  There's code commented out for my attempt at crossing genomes over.\n\nAlso, upon executing the code, it will display all of the genomes in this manner:\n\nfitness ~ len(links) ~ len(neurons)\n\n... followed by a diagram of all species (with some data) and all genomes.\n\nAny help at all is appreciated.  Thank you!", 
            "subreddit": "MachineLearning", 
            "title": "Crossing over two genomes with NEAT", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70kmfn/crossing_over_two_genomes_with_neat/"
        }, 
        {
            "author": "changoplatanero", 
            "created_utc": 1505594439.0, 
            "domain": "self.MachineLearning", 
            "id": "70j9dg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70j9dg/d_what_are_some_common_heuristics_used_in_rnn/", 
            "score": 24, 
            "selftext": "I have an RNN LM and I want to sample sentences from it. (In the model I'm working with there is nothing to attend to. It's just a plain LSTM.) I noticed that if I do a stochastic beam search I tend to get lots of repetition. For example, when I trained it on Yelp reviews it will produce sentences like \"This was my first time at this restaurant and the food was good but the service was bad and the food was good but the service was bad and the food was good.\" Or it will also want to finish a sentence with an endless sequence of exclamation marks. \n\nIt's easy enough to come up with some heuristics to prevent exploring these options in the beam. Forbidding it from using any trigram more than once seems to work pretty well. Now that I'm writing up a paper about my model though I need some justifiable heuristics instead of my ad-hoc ones. \n\nAre these problems that I'm observing well-known? Does anyone have a reference for some commonly applied beam search heuristic constraints?", 
            "subreddit": "MachineLearning", 
            "title": "[D] What are some common heuristics used in RNN beam decoding for text generation?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70j9dg/d_what_are_some_common_heuristics_used_in_rnn/"
        }, 
        {
            "author": "nivm321", 
            "created_utc": 1505594091.0, 
            "domain": "timdettmers.com", 
            "id": "70j88n", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 21, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70j88n/d_credit_assignment_in_deep_learning_tim_dettmers/", 
            "score": 37, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Credit Assignment in Deep Learning - Tim Dettmers", 
            "url": "http://timdettmers.com/2017/09/16/credit-assignment-deep-learning/"
        }, 
        {
            "author": "alexmlamb", 
            "created_utc": 1505590602.0, 
            "domain": "youtube.com", 
            "id": "70iwgv", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": {
                "oembed": {
                    "author_name": "The Nutty Netter (Alex Lamb)", 
                    "author_url": "https://www.youtube.com/channel/UC6YglsfUYu0iwySl_zyEcYA", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/ljdwwM5kIrw?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/ljdwwM5kIrw/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Is the US Falling Behind China in AI Research?", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 42, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70iwgv/d_is_the_us_falling_behind_china_in_ai_research/", 
            "score": 31, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Is the US Falling Behind China in AI Research?", 
            "url": "https://www.youtube.com/watch?v=ljdwwM5kIrw"
        }, 
        {
            "author": "jasmeetsb", 
            "created_utc": 1505586731.0, 
            "domain": "datasciencecentral.com", 
            "id": "70ij44", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70ij44/r_my_analysis_on_comparative_performance_of_deep/", 
            "score": 86, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] My analysis on comparative performance of Deep Learning Frameworks supported by Keras - TensorFlow Vs MXNet Vs CNTK Vs Theano", 
            "url": "http://www.datasciencecentral.com/profiles/blogs/search-for-the-fastest-deep-learning-framework-supported-by-keras"
        }, 
        {
            "author": "falmasri", 
            "created_utc": 1505585785.0, 
            "domain": "self.MachineLearning", 
            "id": "70ifla", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70ifla/why_andrew_nj_answered_to_very_general_and_newbie/", 
            "score": 0, 
            "selftext": "https://www.quora.com/session/Andrew-Ng/2", 
            "subreddit": "MachineLearning", 
            "title": "Why andrew nj answered to very general and newbie question only on quora ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70ifla/why_andrew_nj_answered_to_very_general_and_newbie/"
        }, 
        {
            "author": "TDaltonC", 
            "created_utc": 1505580444.0, 
            "domain": "self.MachineLearning", 
            "id": "70hx15", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70hx15/d_when_its_time_to_raise_money_for_your_ml/", 
            "score": 0, 
            "selftext": "There are a lot of great practical uses of ML/AI and a lot of great investors looking to fund them. If you're trying to get an ML startup off the ground, you should know how investors like talking about it. \n\nBelow is a blog post I wrote to summarize how I learned to talk about data while raising a $1M seed round for my startup. I'd love to get everyone's feedback on it!\n\n~~~~\n\n**\u201cData is like a Moat!\u201d and Other Bad Ways to Talk About Data and AI**\n\nIf you\u2018re founding, funding, or fueling a startup, you need to know how to talk about data.\nAlmost 50 years in to the Information Age and we still lack great ways to talk about data and its value. Newly arrived 3rd wave AI is hungry for data in a way that 1st and 2nd wave AI never were. A fresh influx of capital is fueling work and attention around AI again. But in some ways, these new companies and projects make it ever harder to talk about data.\nOur language about data is sprinting away from us. We look back on phrases like \u201chorseless carriage\u201d or \u201ctalking telegraph\u201d and wonder how people could have used such backwards language when a future tech arrived.\n\nBut we\u2019re doing the same thing today in how we talk about data. Here\u2019s how I\u2019ve heard a lot of investors, creators, and other founders talk about data . By understanding the history and limits of these analogies, you can be one step ahead of everyone in understanding the impact of data and AI on business.\n\nThere are 4 main analogies that people are using to communicate the valuable role that data plays in business; especially in AI-centric projects:\n\n* \u201cData network effects\u201d\n \n* \u201cData moats\u201d\n \n* \u201cSystems of Intelligence\u201d\n \n* \u201cAI Flywheels\u201d \u2190 my favorite!\n\nHere\u2019s how I understand each of these analogies with my team at Dopamine Labs.\n\n**Data Network Effect**\n\nA Data Network Effect is a property of a product that improves with the more data it has available, due to emergent relationships between segments of the data. It\u2019s an analogy to Metcalfe\u2019s Law as it played out in telephone networks and the later \u201cSocial Network Effects\u201d that explain the defensibility and the growth of businesses like Facebook. Like a social or telephone network, only some small fraction of the data in a data network is useful to any specific customer or task (there are \u2014 at most \u2014 only 500 people I care about on Facebook of my thousand-odd connections). In a product experiencing a data network effect, as the total amount of data available to a learning system grows, you can provide more value to any individual user even though they only interact personally with a different small sliver of the data \u2018network\u2019. Another nice thing about this analogy is that, like in a communications network, every member of the network generates value for the network simply by participating in it.\n\n*So if you\u2019re designing an AI-powered product, how might you store, collect, analyze, and act upon your data such that your models and product appreciate with more data?*\n\n**Data Moat**\n\nA Data Moat describes a competitive advantage a business holds because of its proprietary data set. It\u2019s a modern extension of traditional \u201cmoats\u201d of business, such as vendor lock-in, branding, trade secrets, efficiencies of scale, and regulatory capture. Like their literal counterpart, the metaphorical data moat protects an earned market position from potential followers. Warren Buffett described the importance of moats in his investment theses: \u201cIn business I look for economic castles protected by unbreachable \u2018moats\u2019.\u201d Like these other defensibility moats, a strong data moat inhibits competitors from stealing marketshare.\nTo build a strong data moat, your data set must be, amongst other things, large, unable to be synthesized or statistically replicated from first principles, and difficult to acquire due to access barriers. (For example, Dopamine\u2019s Persuasion AI benefits from our many relationships with many different customers. A CV-based Cancer-Detection AI benefits from exclusivity and patient data privacy laws. An ultrasonic oilfield surveying AI benefits from land rights and cost barriers. A fraud detecting AI benefits from a regulatory environment that keeps transactions private.)\n\nAs a counterexample, a CV-based scene parsing system, which can train using freely and widely available natural image data, might struggle to build a data moat. Since there\u2019s no protection to that data source, anyone can train a model on it. That means no one model could be expected to drastically outperform another based on data alone.\n\n*So if you\u2019re designing an AI-powered product, what sort of proprietary data set might you incorporate into your core value proposition as to create a defensible data moat?*\n\n**System of Intelligence (SoI)**\n\nSoI is an analogy that Jerry Chen at Greylock Partners makes to the traditional Systems of Record (SoR) that businesses use, brought into a more contemporary age. SoRs capture value by placing themselves between the customer and their data. For example, they\u2019re the SaaS that a business might use for managing data about its customers, employees, IT infrastructure, and finances. It\u2019s the Salesforces, Workdays, Oracles, and Atlasssians. If a customer can\u2019t access their data without continuing to pay for your software/service, you\u2019re in a very strong position.\n\nAnalogously, Systems of Intelligence capture value by putting themselves between their customer and the value generated by their data.\n\nThis analogy is a good description of a business model, and that makes it useful for telling data driven businesses apart from a data driven toys.\n\n*If you\u2019re building an AI-powered tool and your customer doesn\u2019t know what to do with their data without your System of Intelligence (that has also been trained on lots of other people\u2019s data \u2014 see Data Network Effect), that\u2019s a wicked strong position to be in.*\n\n**Data/AI Flywheel**\n\nA Data/AI Flywheel is an analogy we like from Bradford Cross at DCVC. It\u2019s akin to Jim Collins\u2019 business value flywheel. Collins\u2019 analogy is one is my favorites because it explains how businesses grow and become defensible. It explains how, in a well-aligned business, value generated in one part of the business fuels the growth of value in another part of the business.\n\n[How data creates value in a data flywheel.](https://cdn-images-1.medium.com/max/2000/1*KdoG8uVkPPvi4-3xnQYDMw.png)\n\nCross\u2019s AI-specific analogy of the Data Flywheel explores how the role that humans have previously played in creating value in business processes is being replaced by AIs.\n\nThis analogy also provides a way to think about even very young AI-powered companies. You can evaluate them based on the existence or strength of each linkage in the Flywheel: does this better solution to a customer need expand data collection? Does the increase in data from a new customer improve the core model? Does a better model deliver better value?\n\n*If you\u2019re designing an AI-powered product, how might your data and business processes cooperate to create an AI-Flywheel?*\n\nI wish I could tell you that there\u2019s a right way to talk about data, but there isn\u2019t. We use analogies to talk about things we don\u2019t understand, and we don\u2019t understand how data and AI will impact business yet.\n\nEventually, we\u2019ll get to stop making these analogies. we will have a organic diction (lexicon) of data . . . but by then, the next generation of great tech companies will be established.\nFor people living and working at the leading edge, bad analogies are just like water.\n\n\nwww.useDopamine.com", 
            "subreddit": "MachineLearning", 
            "title": "[D] When It's Time to Raise Money for Your ML StartUp, Here's How You Should Talk About It", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70hx15/d_when_its_time_to_raise_money_for_your_ml/"
        }, 
        {
            "author": "galapag0", 
            "created_utc": 1505568812.0, 
            "domain": "blog.acolyer.org", 
            "id": "70gw5j", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70gw5j/r_adversarial_examples_for_evaluating_reading/", 
            "score": 11, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Adversarial examples for evaluating reading comprehension systems", 
            "url": "https://blog.acolyer.org/2017/09/13/adversarial-examples-for-evaluating-reading-comprehension-systems/"
        }, 
        {
            "author": "ObserverAtNight", 
            "created_utc": 1505565204.0, 
            "domain": "self.MachineLearning", 
            "id": "70gn3c", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70gn3c/d_need_help_verifying_my_results_for_a/", 
            "score": 1, 
            "selftext": "Hi,\n\nI work on a project where I want to classify sensor data that was recorded. I have multiple time series where each sample consists of >60 dimensions, one for each sensor.\n\nI don't use more preprocessing than performing a z-transformation for each sensor and cutting non relevant intervals. For classifying, I train each sample as an individual data point (I know this is probably not the best way when you have a time-series, but I'm still learning). \n\nFor classification I used kNN, SVM and MLP. They all gave me accuracy results of 60-70%. I used 5 fold cross-validation and averaged the results. Now after some research, I found random forest and though I would give it a try. Again used 5 fold cv and I got results of 100% accuracy, seldom 98%. I still cant believe that this is true, so I thought it might be the data. Because I used every sample as individual point, data points are really near to their next neighbors (sensor frequency is 150Hz). That's why a random chosen training and test set yield accuracy of 90-100% for all models. So I used cross-validation to be sure that there a blocks of data unseen by the model. Now I think there might be also such a problem for the random forest method. I know people say it might be the best classifier for structured data, but still I can't believe it scores 100% almost all the time. \n\nThis is where I need help: How can I design my test and train set to verify this result? What should I test to see it really learns to classify the input? ", 
            "subreddit": "MachineLearning", 
            "title": "[D] Need help verifying my results for a classification task", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70gn3c/d_need_help_verifying_my_results_for_a/"
        }, 
        {
            "author": "Kiuhnm", 
            "created_utc": 1505556555.0, 
            "domain": "self.MachineLearning", 
            "id": "70g4l3", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70g4l3/d_memoryaugmented_rnns_with_timedependent_access/", 
            "score": 6, 
            "selftext": "I read that memory-augmented RNNs usually rely on *soft* attention to access the memory. This is good because everything is differentiable but also bad because the computation cost grows with the amount of available memory.\n\nSince these nets don't just read from but also write to the memory, why don't you use a virtual view of the memory which depends on the time step? For instance, you could define 8 (partially overlapping, why not?) views of the memory and cycle through them in a round-robin fashion.\n\nMaybe you could associate a (learned?) id vector with each view and send that vector to the net so that it knows which view is active.\n\nDo you think the net would learn to work in such a setting and make good use of all the available memory?\n\n---\n\nedit:\n\nWhat I'm suggesting is still soft attention. The net doesn't control the views but should learn to exploit them (like a moving platform in a game) and the net uses soft attention to read and write within the views.\n\nThere's no hard action. That's the whole point of my suggestion.", 
            "subreddit": "MachineLearning", 
            "title": "[D] memory-augmented RNNs with time-dependent access patterns?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70g4l3/d_memoryaugmented_rnns_with_timedependent_access/"
        }, 
        {
            "author": "piykat", 
            "created_utc": 1505542730.0, 
            "domain": "self.MachineLearning", 
            "id": "70fewg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 21, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70fewg/how_to_go_about_writing_a_research_paper_on/", 
            "score": 14, 
            "selftext": "Fellow Redditors,\n\nI have been working in the ML field for about an year now. My job requires me to read a lot of research papers and implementing them with the standard ML Stack. \n\nI am much more interested in ML Research and working for companies like DeepMind or FAIR. Seeing that a lot of you have published good quality papers, I would just like to know how you guys come up with good research ideas, as most of the ideas I generate have already been implemented.\n", 
            "subreddit": "MachineLearning", 
            "title": "How to go about writing a research paper on Machine Learning?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70fewg/how_to_go_about_writing_a_research_paper_on/"
        }, 
        {
            "author": "quaternion", 
            "created_utc": 1505541798.0, 
            "domain": "self.MachineLearning", 
            "id": "70fcxm", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70fcxm/q_infinite_mixture_models_for_infogan/", 
            "score": 2, 
            "selftext": "I need to discover the mixture of classes and continuous styles in my dataset, but they are entirely unknown - both their number and relative mixture. The impressive results from InfoGAN in disentangling these latent sources seemed promising, but I note that the authors' example on MNIST actually provided a categorical input with a known dimensionality, rather than a categorical input whose dimensionality could be learned. Is it possible and advisable to try extending InfoGAN so that it could learn the number of classes as well as the number of styles? Perhaps someone has already done this?", 
            "subreddit": "MachineLearning", 
            "title": "[Q] Infinite mixture models for InfoGAN?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70fcxm/q_infinite_mixture_models_for_infogan/"
        }, 
        {
            "author": "justnikos", 
            "created_utc": 1505535937.0, 
            "domain": "docs.microsoft.com", 
            "id": "70ez6e", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70ez6e/cntk_22_c_r_bindings_multigpu_keras_2x_faster/", 
            "score": 72, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "CNTK 2.2: C# & R bindings, multigpu Keras, 2x faster seq2seq, and more.", 
            "url": "https://docs.microsoft.com/en-us/cognitive-toolkit/ReleaseNotes/CNTK_2_2_Release_Notes"
        }, 
        {
            "author": "jasoncbenn", 
            "created_utc": 1505531039.0, 
            "domain": "github.com", 
            "id": "70eml0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70eml0/p_built_a_scraper_for_nips_2017_paper_abstracts/", 
            "score": 17, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Built a scraper for NIPS 2017 paper abstracts, here's the database dump", 
            "url": "https://github.com/JasonBenn/nips-scraper/"
        }, 
        {
            "author": "Wonnk13", 
            "created_utc": 1505524552.0, 
            "domain": "axios.com", 
            "id": "70e4ex", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 132, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70e4ex/n_hinton_says_we_should_scrap_back_propagation/", 
            "score": 252, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Hinton says we should scrap back propagation and invent new methods", 
            "url": "https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html"
        }, 
        {
            "author": "TrickyDTrump", 
            "created_utc": 1505503936.0, 
            "domain": "venturebeat.com", 
            "id": "70c5zd", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 21, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70c5zd/n_google_launches_tensorboard_api_to_enhance/", 
            "score": 292, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Google launches TensorBoard API to enhance machine learning visualizations", 
            "url": "https://venturebeat.com/2017/09/11/google-launches-tensorboard-api-to-enhance-machine-learning-visualizations/"
        }, 
        {
            "author": "i_am_klmn", 
            "created_utc": 1505493398.0, 
            "domain": "self.MachineLearning", 
            "id": "70b07q", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70b07q/d_in_gan_evaluation_how_are_inception_scores/", 
            "score": 3, 
            "selftext": "Datasets like CIFAR-10 and STL-10  have different number of classes(10) than ImageNet. Is the Inception Net fine-tuned with these datasets before calculating the score? If so, shouldn't a common fine-tuned net be used by everyone for evaluation? ", 
            "subreddit": "MachineLearning", 
            "title": "[D] In GAN evaluation, how are inception scores calculated for non-ImageNet datasets ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70b07q/d_in_gan_evaluation_how_are_inception_scores/"
        }, 
        {
            "author": "Reiinakano", 
            "created_utc": 1505491092.0, 
            "domain": "arxiv.org", 
            "id": "70ar01", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70ar01/r_an_empirical_study_of_ai_population_dynamics/", 
            "score": 15, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] An Empirical Study of AI Population Dynamics with Million-agent Reinforcement Learning", 
            "url": "https://arxiv.org/abs/1709.04511"
        }, 
        {
            "author": "giuseppe_bonaccorso", 
            "created_utc": 1505486445.0, 
            "domain": "bonaccorso.eu", 
            "id": "70a95q", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70a95q/quickprop_an_almost_forgotten_neural_training/", 
            "score": 44, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Quickprop: an almost forgotten neural training algorithm", 
            "url": "https://www.bonaccorso.eu/2017/09/15/quickprop-an-almost-forgotten-neural-training-algorithm/"
        }, 
        {
            "author": "zthoutt", 
            "created_utc": 1505485226.0, 
            "domain": "blogs.nvidia.com", 
            "id": "70a4l5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70a4l5/d_episode_of_ai_podcast_with_the_creator_of_the/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Episode of AI Podcast with the creator of the Game of Thrones AI (Ep. 38)", 
            "url": "https://blogs.nvidia.com/ai-podcast/"
        }, 
        {
            "author": "downtownslim", 
            "created_utc": 1505483171.0, 
            "domain": "sites.google.com", 
            "id": "709x1d", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/709x1d/r_bair_oneshot_visual_imitation/", 
            "score": 18, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] BAIR: One-Shot Visual Imitation", 
            "url": "https://sites.google.com/view/one-shot-imitation"
        }, 
        {
            "author": "Deep_Fried_Learning", 
            "created_utc": 1505481788.0, 
            "domain": "arxiv.org", 
            "id": "709snm", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/709snm/r_170101833_oriented_response_networks/", 
            "score": 24, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] [1701.01833] Oriented Response Networks", 
            "url": "https://arxiv.org/abs/1701.01833"
        }, 
        {
            "author": "nianhao", 
            "created_utc": 1505478649.0, 
            "domain": "medium.com", 
            "id": "709ip2", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/709ip2/r_analysis_of_algorithms_from_davis2017_object/", 
            "score": 29, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Analysis of algorithms from DAVIS-2017 Object Segmentation Challenge", 
            "url": "https://medium.com/mlreview/a-meta-analysis-of-davis-2017-video-object-segmentation-challenge-c438790b3b56"
        }, 
        {
            "author": "taki0112", 
            "created_utc": 1505469870.0, 
            "domain": "github.com", 
            "id": "708vt7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/708vt7/p_squeezeandexcitation_networks_tensorflow/", 
            "score": 23, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Squeeze-and-Excitation Networks Tensorflow Implementation (ILSVRC 2017 winner)", 
            "url": "https://github.com/taki0112/SENet-Tensorflow"
        }, 
        {
            "author": "bendie_benderson", 
            "created_utc": 1505466180.0, 
            "domain": "self.MachineLearning", 
            "id": "708ntk", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/708ntk/conditional_gan_learns_to_an_extent_and_then/", 
            "score": 6, 
            "selftext": "Hi.\n\nI am trying to train a basic cgan for MNIST based on labels (https://arxiv.org/abs/1411.1784) in keras. I am concatenating the noise and condition at the input for the generator. For the discriminator I concatenate the condition after applying 3 consecutive convolutions to the input image (so that the image structure can be interpreted without destroying it by adding conditions right away). The condition is then also fed to a dense layer before adding it to the sigmoid output. The network seems to learn upto 200 iterations where it generates more or less number-like structures but then later starts to just destroy what it has learnt and generates big white blobs covering the whole image. (number pixes are white and background black). What could possibly be wrong? below are my G and D networks. I am using SGD optimizer with lr 0.0005, momentum 0.9 for both G and D. Any ideas?\n\n    concatenate=Concatenate(axis=1)\n#Generator architecture\n    g_input = Input(shape=[100])\n    c_input = Input(shape=[10])\n    inp=concatenate([g_input,c_input])\n    nch=200\n    H = Dense(nch*int(img_height/4)*int(img_width    /4),kernel_initializer='glorot_uniform',bias_initializer='zeros',activation='relu')(inp)\n    H = BatchNormalization(axis=1,momentum=0.5,epsilon=0.001)(H)\n    H = Reshape((int(img_height/4),int(img_width/4),nch))(H)\n    H = UpSampling2D(size=(2,2),data_format='channels_last')(H)\n    H = Conv2D(filters=int(nch/2),kernel_size=(3,3),strides=    (1,1),padding='same',data_format='channels_last',\n    kernel_initializer='glorot_uniform',bias_initializer='zeros')(H)\n    H = LeakyReLU(0.01)((H))\n    H = UpSampling2D(size=(2,2),data_format='channels_last')(H)\n    H = BatchNormalization(axis=1,momentum=0.9,epsilon=0.001)(H)\n    H = Conv2D(filters=50,kernel_size=(3,3),strides=    (1,1),padding='same',data_format='channels_last',\n           kernel_initializer='glorot_uniform',bias_initializer='zeros')(H)\n    H=  LeakyReLU(0.01)((H))\n    g_output = Conv2D(filters=1,kernel_size=(1,1),strides=    (1,1),padding='same',data_format='channels_last',\n           activation = 'sigmoid',kernel_initializer='glorot_uniform',bias_initializer='zeros')(H)\n\n    generator = Model([g_input,c_input],g_output)\n    generator.compile(optimizer=opt_g,loss='binary_crossentropy',metrics=['accuracy'])\n    generator.summary() \n\n#discriminator architecture\n    d_input = Input(shape=(28,28,1))\n    c_input = Input(shape=[10])\n    nch=256\n    H = Conv2D(filters=nch,kernel_size=(3,3),strides=(1,1),data_format='channels_last')    (d_input)\n    H = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same',data_format='channels_last')(H)\n    H = LeakyReLU(0.01)((H))\n    H = Dropout(0.5)(H)\n    H = Conv2D(filters=int(nch/2),kernel_size=(3,3),strides=    (1,1),data_format='channels_last')(H)\n    H = MaxPooling2D(pool_size=(2,2),strides=    (2,2),padding='same',data_format='channels_last')(H)\n    H = LeakyReLU(0.01)((H))\n    H = Dropout(0.5)(H)\n    H = Flatten()(H)\n    Inp=concatenate([H,c_input])\n    H = Dense(nch)(Inp)\n    H = LeakyReLU(0.01)((H))\n    H = Dropout(0.5)(H)\n    d_output = Dense(2,activation='sigmoid')(H)\n    discriminator = Model(inputs=[d_input,c_input],outputs=[d_output])\n    discriminator.compile(optimizer=opt_d,loss='binary_crossentropy',metrics=['accuracy'])\n    discriminator.summary() \n\n", 
            "subreddit": "MachineLearning", 
            "title": "Conditional GAN learns to an extent and then destroys what it has learnt", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/708ntk/conditional_gan_learns_to_an_extent_and_then/"
        }, 
        {
            "author": "Kaixhin", 
            "created_utc": 1505460358.0, 
            "domain": "fastcompany.com", 
            "id": "708c7y", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/708c7y/n_facebooks_new_lab_bolsters_montreals_bragging/", 
            "score": 96, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Facebook's New Lab Bolsters Montreal's Bragging Rights As An AI Hub", 
            "url": "https://www.fastcompany.com/40465968/facebooks-new-lab-bolsters-montreals-bragging-rights-as-an-ai-hub"
        }, 
        {
            "author": "wencc", 
            "created_utc": 1505450866.0, 
            "domain": "self.MachineLearning", 
            "id": "707ptn", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/707ptn/dwhats_the_most_efficient_way_to_add_a_class_to_a/", 
            "score": 3, 
            "selftext": "Say I have a global classifier trained from ImageNet Challenge that has 1000 classes. What is the best way to add a new class to the classifier? Any paper on this?", 
            "subreddit": "MachineLearning", 
            "title": "[D]What's the most efficient way to add a class to a global classifier?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/707ptn/dwhats_the_most_efficient_way_to_add_a_class_to_a/"
        }, 
        {
            "author": "KrustyKrab111", 
            "created_utc": 1505444952.0, 
            "domain": "self.MachineLearning", 
            "id": "7078y0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 28, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7078y0/machine_learning_undergraduate_intern_positions/", 
            "score": 14, 
            "selftext": "Hi guys,\n\nBeen trying really hard to get a position at any company for intern positions for machine learning as an undergraduate but companies just don't seem to care. It seems like every intern position in machine learning is limited to people with masters or PhD positions. Is there any way for an undergraduate to really bag a position for ML?\n\nThanks\n\nP.S if you're hiring machine learning undergraduates, I'd be more than happy to send my resume over!", 
            "subreddit": "MachineLearning", 
            "title": "Machine Learning Undergraduate Intern positions?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/7078y0/machine_learning_undergraduate_intern_positions/"
        }, 
        {
            "author": "baylearn", 
            "created_utc": 1505439809.0, 
            "domain": "nips2017creativity.github.io", 
            "id": "706sz6", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/706sz6/n_nips_workshop_on_machine_learning_for/", 
            "score": 79, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] NIPS Workshop on Machine Learning for Creativity and Design", 
            "url": "https://nips2017creativity.github.io"
        }, 
        {
            "author": "Chipdoc", 
            "created_utc": 1505434877.0, 
            "domain": "lanl.gov", 
            "id": "706cjf", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/706cjf/machinelearning_earthquake_prediction_in_lab/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Machine-learning earthquake prediction in lab shows promise (Los Alamos National Lab)", 
            "url": "http://www.lanl.gov/discover/news-release-archive/2017/August/0830-machine-learning-earthquake-prediction.php"
        }, 
        {
            "author": "ov3rsight", 
            "created_utc": 1505434432.0, 
            "domain": "self.MachineLearning", 
            "id": "706az7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/706az7/d_ideas_for_a_class_project_given_my_background/", 
            "score": 0, 
            "selftext": "We have to have an idea for our semester project submitted soon. However, we haven't really covered any content yet. All we've looked at is linear regression so I don't know what a reasonable project would be. Our project needs to be focus around applying the techniques we learn to (big) data to answer a question rather than implementing existing algorithms or creating new ones. \n\nMy background is math major + CS minor. Fields of application that I'm interested in are psychology, neuroscience. biology, medicine and health - though I'm open to looking at other areas. Any suggestions for projects or for places to look?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Ideas for a class project given my background?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/706az7/d_ideas_for_a_class_project_given_my_background/"
        }, 
        {
            "author": "beef__", 
            "created_utc": 1505426907.0, 
            "domain": "self.MachineLearning", 
            "id": "705kpt", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/705kpt/d_what_the_hell_is_up_with_all_of_these_trump/", 
            "score": 8, 
            "selftext": "I've seen like, 4 individual \"generate trump tweets!\" projects on this sub in the past ~week", 
            "subreddit": "MachineLearning", 
            "title": "[D] What the hell is up with all of these \"Trump Tweets\" projects?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/705kpt/d_what_the_hell_is_up_with_all_of_these_trump/"
        }, 
        {
            "author": "mikeyanderson", 
            "created_utc": 1505415842.0, 
            "domain": "demos.algorithmia.com", 
            "id": "704e0j", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/704e0j/p_chaining_algorithms_for_social_image_recommender/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[p] Chaining algorithms for social image recommender", 
            "url": "http://demos.algorithmia.com/social-image-rec/"
        }, 
        {
            "author": "tsorn", 
            "created_utc": 1505411272.0, 
            "domain": "self.MachineLearning", 
            "id": "703w4c", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/703w4c/d_depth_preserving_convolutions/", 
            "score": 4, 
            "selftext": "I have a problem non-image domain where spatial information is important, however with a much larger depth, say 50 (vs. 3 for RGB images). Have any of tried convolutions where some depth is preserved? I'm having trouble visualizing an architecture in my head. Paper recommendations also appreciated.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Depth preserving convolutions", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/703w4c/d_depth_preserving_convolutions/"
        }, 
        {
            "author": "qeVut7tguCpxKqqMPtWU", 
            "created_utc": 1505410375.0, 
            "domain": "self.MachineLearning", 
            "id": "703sje", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/703sje/p_commercial_3d_morphable_face_models/", 
            "score": 5, 
            "selftext": "I'm working for a small start up and trying to follow some of the results in the literature that reconstruct 3D models from videos or images but it seems all of them use some datasets (mostly the Basel Face Model or facewarehouse) that are not accessible for commercial usage. Did anyone else encounter this issue? is there an affordable (or free) model available? ", 
            "subreddit": "MachineLearning", 
            "title": "[P] Commercial 3D Morphable Face Models", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/703sje/p_commercial_3d_morphable_face_models/"
        }, 
        {
            "author": "0b01", 
            "created_utc": 1505399013.0, 
            "domain": "rickyhan.com", 
            "id": "702jcy", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/702jcy/p_gradient_trader_part_1_the_surprising/", 
            "score": 58, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Gradient Trader Part 1: The Surprising Usefulness of Autoencoders", 
            "url": "http://rickyhan.com/jekyll/update/2017/09/14/autoencoders.html"
        }, 
        {
            "author": "ledilb", 
            "created_utc": 1505398036.0, 
            "domain": "blog.sicara.com", 
            "id": "702fpe", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/702fpe/d_keras_tutorial_content_based_image_retrieval/", 
            "score": 28, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Keras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder", 
            "url": "https://blog.sicara.com/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511"
        }, 
        {
            "author": "deshrajdry", 
            "created_utc": 1505395584.0, 
            "domain": "github.com", 
            "id": "7027au", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7027au/evalai_an_open_source_alternative_of_kaggle/", 
            "score": 129, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "EvalAI: An open source alternative of Kaggle", 
            "url": "https://github.com/Cloud-CV/EvalAI"
        }, 
        {
            "author": "LeeviLux", 
            "created_utc": 1505393418.0, 
            "domain": "self.MachineLearning", 
            "id": "70202n", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70202n/imagenet_choice_of_classes/", 
            "score": 6, 
            "selftext": "ive just used tensorflow image recognition for the first time on some of my holiday pictures and noticed that the 1000 imagenet classes\n\nhttp://image-net.org/challenges/LSVRC/2014/browse-synsets\n\ndo not contain anything to do with people: there is no class for face, person, man, woman, hand, foot. just curious if anyone has a link that explains the choice of categories. my guess is that they want to avoid some form of abuse.", 
            "subreddit": "MachineLearning", 
            "title": "imagenet choice of classes", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/70202n/imagenet_choice_of_classes/"
        }, 
        {
            "author": "trowway1239", 
            "created_utc": 1505387458.0, 
            "domain": "self.MachineLearning", 
            "id": "701j70", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 159, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/701j70/those_working_in_machine_learningdata_science_in/", 
            "score": 147, 
            "selftext": "It would be great if we have more machine learning developers/ data scientists in [this survey](https://docs.google.com/forms/d/e/1FAIpQLScTh14SV6qbMvGrGz5-XQz0aGp04j5M4P_4ciaSOXsTBfzvGA/viewform) pulished in [HN](https://news.ycombinator.com/item?id=15088840) :)", 
            "subreddit": "MachineLearning", 
            "title": "Those working in Machine Learning/Data Science in Europe, what are your salaries?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/701j70/those_working_in_machine_learningdata_science_in/"
        }, 
        {
            "author": "villasv", 
            "created_utc": 1505385146.0, 
            "domain": "learndatasci.com", 
            "id": "701dqo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/701dqo/data_science_interview_questions/", 
            "score": 87, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Data Science Interview Questions", 
            "url": "http://www.learndatasci.com/data-science-interview-questions"
        }, 
        {
            "author": "BammyBums", 
            "created_utc": 1505380440.0, 
            "domain": "bam4d.github.io", 
            "id": "7013el", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/7013el/playing_connect4_in_kdimensions/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Playing connect-4 in K-dimensions", 
            "url": "https://bam4d.github.io/?a=1#/post/k-dimensional-connect-n--part-2-deep-learning/3"
        }, 
        {
            "author": "NaughtyCranberry", 
            "created_utc": 1505378501.0, 
            "domain": "self.MachineLearning", 
            "id": "700zj8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/700zj8/d_is_there_an_imagenet_equivalent_for_audio/", 
            "score": 23, 
            "selftext": "I want to benchmark different approaches to audio classification and I was wondering is there is a standard dataset to train and test on?\n\nEdit: Thank you for all your responses, much appreciated.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Is there an ImageNet equivalent for audio signal classification?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/700zj8/d_is_there_an_imagenet_equivalent_for_audio/"
        }, 
        {
            "author": "rakzah", 
            "created_utc": 1505377068.0, 
            "domain": "self.MachineLearning", 
            "id": "700wpi", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/700wpi/overfitting_neural_network_yielding_better_test/", 
            "score": 0, 
            "selftext": "Hi,\n\nI'm training a pretty simple regression neural network with ~160 features and 200.000 training examples (of which I'm using 20.000 for testing). In order to find the best performing network, I'm training it with many different randomly selected hyper parameters.\n\nWhile deep and wide architectures obviously overfit extremely, they also yield slighty better test performance than other architectures. For example, a network with 4 hidden layers with 800, 960, 1150 and 1380 units respectively results in a mean squared error of 3.28 while an architecture with only 2 hidden layers and 300/210 units results in a MSE of 3.32.\n\nHow can this be explained? I don't feel like using an overfitted network is sensible, however it does show better test performance than less overfitted networks. I already tried adding dropout layers and using L2 regularization and while both reduce overfitting, both the training and test loss are higher.\n", 
            "subreddit": "MachineLearning", 
            "title": "Overfitting Neural Network yielding better test set performance", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/700wpi/overfitting_neural_network_yielding_better_test/"
        }, 
        {
            "author": "ajbouh", 
            "created_utc": 1505362570.0, 
            "domain": "github.com", 
            "id": "6zzz6j", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zzz6j/p_use_any_tensorflow_model_in_a_single_line_of/", 
            "score": 68, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Use any TensorFlow model in a single line of code", 
            "url": "https://github.com/ajbouh/tfi"
        }, 
        {
            "author": "pmigdal", 
            "created_utc": 1505345777.0, 
            "domain": "github.com", 
            "id": "6zyhuh", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zyhuh/p_zygmuntzgoodbooks10k_ten_thousand_books_six/", 
            "score": 17, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] zygmuntz/goodbooks-10k - Ten thousand books, six million ratings", 
            "url": "https://github.com/zygmuntz/goodbooks-10k"
        }, 
        {
            "author": "psoulos", 
            "created_utc": 1505341084.0, 
            "domain": "self.MachineLearning", 
            "id": "6zy10r", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zy10r/d_is_there_research_on_visualizing_layers_of_a/", 
            "score": 6, 
            "selftext": "I can find a lot of references on ways to visualize discriminative networks and understand what is happening at each layer in the network. Is there any corresponding research on understanding generative layers? For instance, what do the layers in a VAE decoder or the generator of a VAE do?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Is there research on visualizing layers of a generative model?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zy10r/d_is_there_research_on_visualizing_layers_of_a/"
        }, 
        {
            "author": "scottauds", 
            "created_utc": 1505339686.0, 
            "domain": "displayr.com", 
            "id": "6zxvrk", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zxvrk/p_dimensionality_reduction_using_tsne/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Dimensionality Reduction Using t-SNE", 
            "url": "https://www.displayr.com/using-t-sne-to-visualize-data-before-prediction/"
        }, 
        {
            "author": "lioru", 
            "created_utc": 1505336065.0, 
            "domain": "github.com", 
            "id": "6zxhua", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zxhua/p_updated_ssd_code_to_support_pascal_gpus/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Updated SSD code to support Pascal GPUs", 
            "url": "https://github.com/ghostcow/caffe"
        }, 
        {
            "author": "vanHavel", 
            "created_utc": 1505332709.0, 
            "domain": "vanhavel.github.io", 
            "id": "6zx44m", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zx44m/p_recognizing_game_genres_from_screenshots_using/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Recognizing Game Genres From Screenshots using CNNs", 
            "url": "https://vanhavel.github.io/2017/09/12/cnn-games.html"
        }, 
        {
            "author": "LovaszExtension", 
            "created_utc": 1505329268.0, 
            "domain": "nips.cc", 
            "id": "6zwqc3", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zwqc3/n_view_nips_accepted_papers_by_subject/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] View NIPS accepted papers by Subject", 
            "url": "https://nips.cc/Conferences/2017/Schedule?bySubject"
        }, 
        {
            "author": "zhamisen", 
            "created_utc": 1505325989.0, 
            "domain": "eurekalert.org", 
            "id": "6zwco7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zwco7/n_ai_uses_less_than_two_minutes_of_videogame/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] AI uses less than two minutes of videogame footage to recreate game engine", 
            "url": "https://www.eurekalert.org/pub_releases/2017-09/giot-aul091117.php"
        }, 
        {
            "author": "Inori", 
            "created_utc": 1505319417.0, 
            "domain": "self.MachineLearning", 
            "id": "6zvlm2", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 35, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zvlm2/d_openai_closing_down_gym_toolkit_website/", 
            "score": 160, 
            "selftext": "If you go to https://gym.openai.com/ now it just redirects you to their github repo. There was no official statement about it, the site just went down out of the blue. Only source about it actually closing down right now is Greg Brockman's reply on twitter.\n\nhttps://twitter.com/gdb/status/907855318591438848\n\n> Yep, the Github repo has been the focus of the project for the past year. The Gym site looks cool but hasn't been maintained.\n\n\nIt is really sad news for me personally. The companion website was very useful, both as a benchmark and source of information on practical algo implementations and tricks via writeups. Greg mentioned lack of maintenance, but it I think it was good enough for what it offered.\n\nMaybe if community lets openAI know website will be missed they'll reconsider?", 
            "subreddit": "MachineLearning", 
            "title": "[D] openAI closing down gym toolkit website", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zvlm2/d_openai_closing_down_gym_toolkit_website/"
        }, 
        {
            "author": "Eriklindernoren", 
            "created_utc": 1505314794.0, 
            "domain": "github.com", 
            "id": "6zv30y", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zv30y/p_mlfromscratch_library_of_bare_bones_python/", 
            "score": 142, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] ML-From-Scratch: Library of bare bones Python implementations of Machine Learning models and algorithms", 
            "url": "https://github.com/eriklindernoren/ML-From-Scratch"
        }, 
        {
            "author": "Eternahl", 
            "created_utc": 1505312707.0, 
            "domain": "self.MachineLearning", 
            "id": "6zuv72", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zuv72/d_gans_generator_memorization/", 
            "score": 22, 
            "selftext": "Hi!\n\nWhile reading the paper on BEGAN (https://arxiv.org/pdf/1703.10717.pdf), a question came to my mind.\n\nHow can we be sure that such Generative Networks don't memorize some examples of the training, and then \"simply\" learn the mapping from the noise to N examples of the training?\n\n\nWhen the Generative Network complexity is high enough, N (numbers of training samples learned) can be large and thus give a misleading impression of diversity. On top of that, the *reconstruction error* (since examples are not perfectly memorized) of the generator adds to the effect/impression of \"generated from scratch\" examples. Is it something the active researchers community is taking care of?\n\n\nThe reason I am asking is that I was wondering if a very simple *nearest neighbors* approach would work? Where one would show the top-5 NN in the training set for each generated examples. It would somehow ensure that this doesn't occur and be beneficial for such papers.\n\n\nNow I see some reasons why this might not be done/does not appear:\n\n1. The metric of nearest neighbors is not so straightforward or tractable (because of the dimensionality). But it would be done if we found some meaningful metric, to ensure the generator doesn't enter in this \"mode\".\n\n2. This is done as sort of a sanity check but not documented in such papers because obvious.\n\n3. There is something in the architecture that I misunderstood, preventing such a mode to occur.\n\n4. Showing empirically the \"space continuity\" (as in 4.3 of the aforementioned paper) is a sufficient evidence that the model generalizes and does not memorize.\n\n\nThank you in advance for the clarifications, I hope this question won't be too naive for this sub.\n ", 
            "subreddit": "MachineLearning", 
            "title": "[D] GANs: Generator memorization", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zuv72/d_gans_generator_memorization/"
        }, 
        {
            "author": "Discordy", 
            "created_utc": 1505311872.0, 
            "domain": "medium.com", 
            "id": "6zurv8", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zurv8/a_metaanalysis_of_davis2017_video_object/", 
            "score": 30, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "A Meta-analysis of DAVIS-2017 Video Object Segmentation Challenge", 
            "url": "https://medium.com/@eddiesmo/a-meta-analysis-of-davis-2017-video-object-segmentation-challenge-c438790b3b56"
        }, 
        {
            "author": "MasterEpictetus", 
            "created_utc": 1505305155.0, 
            "domain": "self.MachineLearning", 
            "id": "6zu5nk", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zu5nk/d_what_is_the_state_of_the_art_on_automatic/", 
            "score": 2, 
            "selftext": "Can you guys provide me some references (papers, blog posts, etc.) in the topic?", 
            "subreddit": "MachineLearning", 
            "title": "[D] What is the state of the art on automatic speech recognition?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zu5nk/d_what_is_the_state_of_the_art_on_automatic/"
        }, 
        {
            "author": "nocortex", 
            "created_utc": 1505298029.0, 
            "domain": "vicarious.com", 
            "id": "6ztnqi", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ztnqi/r_schema_networks_zeroshot_transfer_with_a/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics", 
            "url": "https://www.vicarious.com/img/icml2017-schemas.pdf"
        }, 
        {
            "author": "tar_paulin", 
            "created_utc": 1505297591.0, 
            "domain": "self.MachineLearning", 
            "id": "6ztmse", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ztmse/examples_of_dense_coding_reinforcement_learning/", 
            "score": 1, 
            "selftext": "I have seen various examples of RL networks where the output layer consists of either a single node, and others that have a set of nodes but with a winner-takes-all (sparse) representation.\n\n-Are there examples of networks with a dense output representation? How is this mechanism achieved ?\n\nNote; My interest in this comes from the perspective of scalability. If we have 700 voluntary motor neurons with an on-off state, with a dense representation we require only 700 output nodes. With a sparse representation we presumably require 2^700 nodes.\n", 
            "subreddit": "MachineLearning", 
            "title": "Examples of dense coding Reinforcement Learning neural networks ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ztmse/examples_of_dense_coding_reinforcement_learning/"
        }, 
        {
            "author": "crouching_dragon_420", 
            "created_utc": 1505292914.0, 
            "domain": "github.com", 
            "id": "6ztctb", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ztctb/p_tensorflow_implementation_of_spectral/", 
            "score": 27, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Tensorflow implementation of \"Spectral Normalization for Generative Adversarial Networks\" (ICML 2017)", 
            "url": "https://github.com/minhnhat93/tf-SNDCGAN"
        }, 
        {
            "author": "wordbag", 
            "created_utc": 1505268592.0, 
            "domain": "self.MachineLearning", 
            "id": "6zropp", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zropp/d_bob_mankoff_former_cartoon_editor_of_the_new/", 
            "score": 13, 
            "selftext": "Does anyone know the story here? Bob Mankoff is pretty cool, but I'm pretty sure he's not a machine learning person.\n\nhttps://www.bobmankoff.com/bio/\nhttps://nips.cc/Conferences/2017/AcceptedPapersInitial\nhttps://arxiv.org/abs/1709.03570\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Bob Mankoff, former cartoon editor of the New Yorker, is an author on an accepted NIPS paper.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zropp/d_bob_mankoff_former_cartoon_editor_of_the_new/"
        }, 
        {
            "author": "scottauds", 
            "created_utc": 1505264891.0, 
            "domain": "displayr.com", 
            "id": "6zrcmr", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zrcmr/p_decision_tree_visualizations_using_sankey/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Decision Tree Visualizations using Sankey Diagrams", 
            "url": "https://www.displayr.com/sankey-diagram/"
        }, 
        {
            "author": "distant_gradient", 
            "created_utc": 1505261133.0, 
            "domain": "ku.cloud.panopto.eu", 
            "id": "6zr03x", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zr03x/n_session_videos_from_emnlp_2017/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Session Videos from EMNLP 2017", 
            "url": "https://ku.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx"
        }, 
        {
            "author": "monsta-hd", 
            "created_utc": 1505259839.0, 
            "domain": "github.com", 
            "id": "6zqw0l", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zqw0l/p_photo_organizer_app_using_cnns/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Photo organizer app using CNNs", 
            "url": "https://github.com/monsta-hd/photo-organizer"
        }, 
        {
            "author": "nishnik", 
            "created_utc": 1505256494.0, 
            "domain": "self.MachineLearning", 
            "id": "6zqkn5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zqkn5/p_first_pytorch_code_implemented/", 
            "score": 3, 
            "selftext": "I have participated in few Kaggle competitions, and hence have a little hold of numpy.\nI always thought Keras is the simplest DL library (for any type of implementation), glad that I proved myself wrong.\nComments welcome!\n\nPS: Now I think PyTorch is as beginner friendly as Keras", 
            "subreddit": "MachineLearning", 
            "title": "[P] First PyTorch Code - Implemented Deep-Semantic-Similarity-Model", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zqkn5/p_first_pytorch_code_implemented/"
        }, 
        {
            "author": "jessissocial", 
            "created_utc": 1505254128.0, 
            "domain": "dev.to", 
            "id": "6zqc3m", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zqc3m/p_sentiment_analysis_on_trumps_tweets_using_python/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Sentiment analysis on Trump's tweets using Python", 
            "url": "https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-"
        }, 
        {
            "author": "opengmlearn", 
            "created_utc": 1505251277.0, 
            "domain": "statnews.com", 
            "id": "6zq1l3", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 76, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zq1l3/n_ibm_pitched_watson_as_a_revolution_in_cancer/", 
            "score": 181, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] IBM pitched Watson as a revolution in cancer care. It's nowhere close", 
            "url": "https://www.statnews.com/2017/09/05/watson-ibm-cancer/"
        }, 
        {
            "author": "abeppu", 
            "created_utc": 1505236751.0, 
            "domain": "engineering.siftscience.com", 
            "id": "6zoetg", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zoetg/n_models_in_disguise_how_sift_science_ships/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Models in Disguise: How Sift Science Ships Non-Disruptive Model Changes", 
            "url": "https://engineering.siftscience.com/models-disguise-sift-science-ships-non-disruptive-model-changes/"
        }, 
        {
            "author": "monsta-hd", 
            "created_utc": 1505236125.0, 
            "domain": "github.com", 
            "id": "6zoch0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zoch0/p_implementation_of_gaussian_processes_classifier/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Implementation of Gaussian Processes Classifier, MLP, k-NN, PCA, RBM, LogReg from scratch in python and examples on MNIST", 
            "url": "https://github.com/monsta-hd/ml-mnist"
        }, 
        {
            "author": "mlconf", 
            "created_utc": 1505235433.0, 
            "domain": "youtu.be", 
            "id": "6zo9q9", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": {
                "oembed": {
                    "author_name": "MLconf", 
                    "author_url": "https://www.youtube.com/channel/UCjeM1xxYb_37bZfyparLS3Q", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/EVymqG9mdJg?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/EVymqG9mdJg/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Shalini De Mello, Machine Learning Inside the Car at The AI Conference 2017", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zo9q9/n_shalini_de_mello_machine_learning_inside_the_car/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Shalini De Mello, Machine Learning Inside the Car", 
            "url": "https://youtu.be/EVymqG9mdJg"
        }, 
        {
            "author": "pmigdal", 
            "created_utc": 1505233066.0, 
            "domain": "gist.github.com", 
            "id": "6zo0e8", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zo0e8/p_einstein_summation_convention_for_pytorch/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Einstein summation convention for PyTorch", 
            "url": "https://gist.github.com/rockt/15ee013889d65342088e9260a377dc8f"
        }, 
        {
            "author": "DontVoteForMe", 
            "created_utc": 1505230243.0, 
            "domain": "mfstrategies.com", 
            "id": "6znouz", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6znouz/p_using_ai_to_build_a_trustworthy_twitter_bot/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Using AI to build a trustworthy Twitter bot", 
            "url": "http://www.mfstrategies.com/blog/2017/9/11/we-need-to-talk-about-kevin"
        }, 
        {
            "author": "terrorlucid", 
            "created_utc": 1505229654.0, 
            "domain": "nips.cc", 
            "id": "6znmjh", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6znmjh/n_list_of_accepted_paper_nips_2017/", 
            "score": 48, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] List of accepted paper @NIPS 2017", 
            "url": "https://nips.cc/Conferences/2017/Schedule?showParentSession=8797"
        }, 
        {
            "author": "chaitu_005", 
            "created_utc": 1505229523.0, 
            "domain": "blog.statsbot.co", 
            "id": "6znm1j", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6znm1j/r_guide_on_building_your_own_neural/", 
            "score": 195, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Guide on building your own neural conversational agent", 
            "url": "https://blog.statsbot.co/chatbots-machine-learning-e83698b1a91e"
        }, 
        {
            "author": "CaHoop", 
            "created_utc": 1505224666.0, 
            "domain": "youtube.com", 
            "id": "6zn3m7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": {
                "oembed": {
                    "author_name": "Sam Coope", 
                    "author_url": "https://www.youtube.com/user/samoolboule", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/czalwzb5FHY?start=425&feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/czalwzb5FHY/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Deep Learning for Arbitrary Code Generation: Thesis Presentation", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zn3m7/p_deep_learning_for_arbitrary_code_generation/", 
            "score": 69, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Deep Learning for Arbitrary Code Generation: Thesis Presentation - Training recurrent VAEs to Generate Haskell Programs", 
            "url": "https://www.youtube.com/watch?v=czalwzb5FHY&t=425s"
        }, 
        {
            "author": "yvespeirsman", 
            "created_utc": 1505217045.0, 
            "domain": "nlp.town", 
            "id": "6zmg2i", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zmg2i/p_named_entity_recognition_and_the_road_to_deep/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Named Entity Recognition and the Road to Deep Learning", 
            "url": "http://nlp.town/blog/ner-and-the-road-to-deep-learning/"
        }, 
        {
            "author": "pmigdal", 
            "created_utc": 1505208412.0, 
            "domain": "blog.deepsense.ai", 
            "id": "6zlw0a", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zlw0a/p_human_log_loss_for_image_classification_and_a/", 
            "score": 21, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Human log loss for image classification (and a conditional entropy trick)", 
            "url": "https://blog.deepsense.ai/human-log-loss-for-image-classification/"
        }, 
        {
            "author": "huehener", 
            "created_utc": 1505186060.0, 
            "domain": "youtube.com", 
            "id": "6zkeh0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": {
                "oembed": {
                    "author_name": "Preserve Knowledge", 
                    "author_url": "https://www.youtube.com/user/Charleshche", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/zPqFbkdcvWQ?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/zPqFbkdcvWQ/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Meet Geoffrey Hinton, U of T's Godfather of Deep Learning", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zkeh0/d_meet_geoffrey_hinton_u_of_ts_godfather_of_deep/", 
            "score": 69, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Meet Geoffrey Hinton, U of T's Godfather of Deep Learning", 
            "url": "https://www.youtube.com/watch?v=zPqFbkdcvWQ"
        }, 
        {
            "author": "hardmaru", 
            "created_utc": 1505171658.0, 
            "domain": "learningai.io", 
            "id": "6zj2ya", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zj2ya/p_ai_gym_workout_proximal_policy_optimization/", 
            "score": 22, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] AI Gym Workout (Proximal Policy Optimization blog post and implementation)", 
            "url": "https://learningai.io/projects/2017/07/28/ai-gym-workout.html"
        }, 
        {
            "author": "Pieranha", 
            "created_utc": 1505170360.0, 
            "domain": "self.MachineLearning", 
            "id": "6ziyht", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ziyht/d_tokenizationword_segmentation_of_chinese_hindi/", 
            "score": 11, 
            "selftext": "One challenge of doing multi-lingual NLP is to properly preprocess many different kinds of languages. I'd like to properly convert raw Chinese/Mandarin, Arabic, Hindi, Japanese and Russian texts into their linguistically meaningful units (LMUs). For English that would be words and is fairly trivial. For other languages not so much. A few questions that I'd love to hear your thoughts on:\n\n1. Any software you can recommend for handling these languages?\n2. How would you adjust vocabulary sizes for the different languages?\n\nI'd prefer to stay at a level of linguistic complexity pr. unit similar to word-level for English rather than character-level if possible.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Tokenization/word segmentation of Chinese, Hindi etc.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ziyht/d_tokenizationword_segmentation_of_chinese_hindi/"
        }, 
        {
            "author": "sniklaus", 
            "created_utc": 1505160583.0, 
            "domain": "github.com", 
            "id": "6zhy7u", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 88, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zhy7u/r_video_frame_interpolation_via_adaptive/", 
            "score": 222, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Video Frame Interpolation via Adaptive Separable Convolution", 
            "url": "https://github.com/sniklaus/pytorch-sepconv"
        }, 
        {
            "author": "ajbouh", 
            "created_utc": 1505158592.0, 
            "domain": "self.MachineLearning", 
            "id": "6zhqfj", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zhqfj/d_how_do_you_manage_models_data_andor_logs_in/", 
            "score": 27, 
            "selftext": "Looking for rants about how current conventions, tools and workflows are failing people. Or workarounds that folks have come up with to get stuff done despite frustrations with the above.\n\nBackground: I'm working on an open source project to make it easier to manage models, data, and logs in TensorFlow... especially across multiple machines. I want to make sure we're focusing on the right real world situations.", 
            "subreddit": "MachineLearning", 
            "title": "[D] How do you manage models, data, and/or logs in TensorFlow?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zhqfj/d_how_do_you_manage_models_data_andor_logs_in/"
        }, 
        {
            "author": "theflofly", 
            "created_utc": 1505158019.0, 
            "domain": "self.MachineLearning", 
            "id": "6zho5z", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zho5z/d_what_is_the_gradient_of_a_reduction_operation/", 
            "score": 14, 
            "selftext": "Let's say that at some point we obtain a matrix A:\n[\n [2, 4],\n [6, 8]\n]\nAnd we do a sum on the axis 1, we will obtain a vector B:\n[\n [6],\n [14]\n]\nIf the gradient w.r.t B is:\n[\n [7],\n [2]\n]\nThen the gradient w.r.t A is:\n[\n [7, 7],\n [2, 2]\n]\nI don't understand why we replicate the gradient on the reduced dimension and why for instance we don't split the gradient like:\n[\n [3.5, 3.5],\n [1, 1]\n].\nIf someone as a mathematical explanation or a link to some resources it would be helpful.\n\nMy question comes from the following comment in the TensorFlow source code:\n\n// The partial derivative for any input along a \"reduced\" dimension\n\n\n// is just 1, so we only need replicate the output gradient on such a\n\n\n// dimension to its \"expanded\" shape.\n\n\n// Running example:\n\n\n// input is\n\n\n// [[a, b, c],\n\n\n//  [d, e, f]]\n\n\n// reduction_indices = [1]\n\n\n// Sum = [a + b + c, d + e + f]\n\n\n// if the gradient is [g1, g2]\n\n\n// We want the propagated gradient to be\n\n\n// [[g1, g1, g1],\n\n\n//  [g2, g2, g2]]", 
            "subreddit": "MachineLearning", 
            "title": "[D] What is the gradient of a reduction operation on a Matrix?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zho5z/d_what_is_the_gradient_of_a_reduction_operation/"
        }, 
        {
            "author": "luiscosio", 
            "created_utc": 1505155945.0, 
            "domain": "medium.com", 
            "id": "6zhfxy", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zhfxy/n_cruise_how_we_built_the_first_real_selfdriving/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Cruise: How we built the first real self-driving car (really)", 
            "url": "https://medium.com/kylevogt/how-we-built-the-first-real-self-driving-car-really-bd17b0dbda55"
        }, 
        {
            "author": "gizcard", 
            "created_utc": 1505152055.0, 
            "domain": "github.com", 
            "id": "6zh0ax", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zh0ax/p_multigpu_seq2seq_learning/", 
            "score": 15, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Multi-GPU seq2seq learning", 
            "url": "https://github.com/NVIDIA/OpenSeq2Seq"
        }, 
        {
            "author": "lorenmontez", 
            "created_utc": 1505147384.0, 
            "domain": "self.MachineLearning", 
            "id": "6zghw7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zghw7/d_how_to_effectively_train_attention_maps/", 
            "score": 5, 
            "selftext": "I am currently working on a medical AI project on lung-cancer detection. I am now trying to first use Kaggle Data Science Bowl open dataset for my model testing. I intend to use a fully unsupervised approach from input CT images to cancer label. However, the model usually learns nothing without underfitting to all negative samples.\n\nAfter that, I planned to supervise attention maps first based on nodule labels which hand build the ground-truth attention maps from normalized Gaussian distribution. But, it was still underfitting, which all the samples give the same solution.\n\nI have tried using different learning rate, loss function et cetera but none of them helped. I have heard that a good initialization is a key to the success. But since the CT images are three dimensional (stacked images), it simply cannot use any pre-trained model on that.\n\nThe proposed model is to use a 3D FPN (feature pyramid network) network to output a feature layer and an attention map which both with the same resolution from the input data (48x48x48x48, the last dimension represents different patch, cropped by standard step size) from the same last convolutional layer.\n\nI am wondering whether they are some more proper way to train a good attention maps? Or do I miss something important? Any advise will be appreciated.\n\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] How to effectively train attention maps?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zghw7/d_how_to_effectively_train_attention_maps/"
        }, 
        {
            "author": "Lofar788", 
            "created_utc": 1505147086.0, 
            "domain": "self.MachineLearning", 
            "id": "6zggp8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zggp8/d_any_advice_on_the_best_way_to_train_a_dueling/", 
            "score": 3, 
            "selftext": "So I built a DQN, and I'm farily confident it will work.  The problem is its going to take a couple of days to train, and I may have to restart my computer or something.  I'm going to save the models and then reload them.  When I reload them i'm wondering if it would be a good idea to also restart the exploration rate as well.  Will that help stop local maximums, or would it cause it to diverge, and ruin all the progress I made.  Any suggestion?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Any advice on the best way to train a dueling DQN?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zggp8/d_any_advice_on_the_best_way_to_train_a_dueling/"
        }, 
        {
            "author": "giuseppe_bonaccorso", 
            "created_utc": 1505135971.0, 
            "domain": "bonaccorso.eu", 
            "id": "6zfcbd", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zfcbd/artificial_intelligence_is_a_matter_of_language/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Artificial Intelligence is a matter of Language", 
            "url": "https://www.bonaccorso.eu/2017/09/11/artificial-intelligence-is-a-matter-of-language/"
        }, 
        {
            "author": "demonFudgePies", 
            "created_utc": 1505133788.0, 
            "domain": "self.MachineLearning", 
            "id": "6zf4y8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zf4y8/where_can_i_find_the_nips_2017_papers/", 
            "score": 11, 
            "selftext": "There was a post a couple of days ago saying that the decisions are out. However, I couldn't find any of the papers. Does anybody know when the list will be made public?", 
            "subreddit": "MachineLearning", 
            "title": "Where can I find the NIPS 2017 papers?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zf4y8/where_can_i_find_the_nips_2017_papers/"
        }, 
        {
            "author": "outbackdude", 
            "created_utc": 1505122876.0, 
            "domain": "self.MachineLearning", 
            "id": "6zed5j", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zed5j/d_anyone_want_to_try_some_decentralised/", 
            "score": 2, 
            "selftext": "I'm keen to get some machine learning servers running using SONM.io just to test out the tech.\n\nAre there any specific Docker applications I can spin up that the community would like to try out?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Anyone want to try some decentralised, distributed servers for ML?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zed5j/d_anyone_want_to_try_some_decentralised/"
        }, 
        {
            "author": "mofoss", 
            "created_utc": 1505115731.0, 
            "domain": "self.MachineLearning", 
            "id": "6zdym6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zdym6/d_ambiguity_decomposition/", 
            "score": 5, 
            "selftext": "So I ran across an old NIPS paper that talks about ambiguity in regards to ensemble models. \n\nhttps://papers.nips.cc/paper/1001-neural-network-ensembles-cross-validation-and-active-learning.pdf\n\nI also made a blog post on it since I found it so interesting, describing what it means and it's role in ensemble learning.\n\nhttp://www.jungle-ml.com/2017/09/10/ambiguity-bias-variance-decomposition-understanding-randomized-ensemble-methods/\n\nAround the end of my blog post, I state the following:\n\n\"So, essentially, we want to maximize the variance of the estimators about their ensemble in order to minimize the variance of our ensemble with regards to its fitting against data.\"\n\nAm I understanding it correctly? This would mean increasing the spread or rather making each estimator as different as possible around its total mean (increasing variance) results in an overall reduction in generalization error for the ensemble. I see the paradigm of bias-variance tradeoff in the sense if the estimators were all very similar, that increases 'bias'? \n\nWould this imply that for decreasing error, decision trees in a random forest should ideally disagree with each other as much as possible?\n\nAlso, has there been work done to maximize this 'disagreement' between the estimators in an ensemble. \n\nPerhaps generate estimators such that they are not only randomized (eg. bagging), but very different from one another as well.\n\n\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Ambiguity Decomposition", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zdym6/d_ambiguity_decomposition/"
        }, 
        {
            "author": "evc123", 
            "created_utc": 1505113788.0, 
            "domain": "arxiv.org", 
            "id": "6zduh2", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 43, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zduh2/r_170902755_training_rnns_as_fast_as_cnns/", 
            "score": 180, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] [1709.02755] Training RNNs as Fast as CNNs", 
            "url": "https://arxiv.org/abs/1709.02755"
        }, 
        {
            "author": "frozenca", 
            "created_utc": 1505113600.0, 
            "domain": "self.MachineLearning", 
            "id": "6zdu23", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zdu23/how_can_i_visually_discriminate_objects_with/", 
            "score": 3, 
            "selftext": "I'm currently classifying fruits and vegetables using CNNs. In most cases it works well but it makes lots of errors when classifying different fruits with similar visual features. For example, my confusion matrix indicates that the CNN confounds (purple) grapes with blueberries, (green) apples with (green) tomatoes, (red) apples with (red) tomatoes, and so on. It works not so good during external validation (mAP goes below 70%).\n\nThese are my works done so far:\n\n1. Collecting more, more and more DB with articulate images (already doing this)\n2. Discriminate classes little bit more: e.g.) Divide a single class 'apple' into 'green apple' and 'red apple' and reannotate whole image DB (I did this but it was not successful)\n\nHow can I enhance my classifier?\n\n", 
            "subreddit": "MachineLearning", 
            "title": "How can I visually discriminate objects with different classes but have similar visual features? (color, shape, ...)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zdu23/how_can_i_visually_discriminate_objects_with/"
        }, 
        {
            "author": "Lalouch77777123", 
            "created_utc": 1505112965.0, 
            "domain": "self.MachineLearning", 
            "id": "6zdsn9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 32, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zdsn9/what_exactly_is_overfitting_and_why_do_we_prefer/", 
            "score": 15, 
            "selftext": "I'm quite new to Machine Learning, and after reading about the bias-variance tradeoff and overfitting/underfitting, several questions raised in my mind:\n\n 1. If I have a model with 15% error on train set and 14% error on validation set, and another model with 5% error on train set and 13% error on validation set, I understood it is generally better to pick the first model (note that 13% vs 14% is small but not negligible). Why is that? If the validation set results are better, why would I care about the train set?\n \n 2.  I know of several methods to reduce overfitting\n* lower tree depth \n* high value in the regulation parameter\n* optimize parametrs on the validation set (and not the test set)\n* keep the train score just a little better than the test score (as in my first question) \n\n   do all these methods relate to the same type of \"overfitting\", in what sense optimizing parameters on the validation set and having a high regulation parameter are both referring to the same \"overfitting\" concept?\n\n 3. [in this  photo][1] (taken from [here][2])\nit is explained that for eta=0.1 (eta in this case is the learning rate in XGBOOST) the model is underfitting, and for eta=0.9 the model is overfitting. I can see how for eta=0.9 the model is overfitting (as the test error goes up) but for me it seems that also for eta=0.1 the model is overfitting - as the test error goes a little up while the train error goes down. What am I missing?\n\n  [1]: https://i.stack.imgur.com/Tie7c.png\n  [2]: https://stats.stackexchange.com/questions/168666/boosting-why-is-the-learning-rate-called-a-regularization-parameter \"here\"\n\n 5. Lets say I have 2 tree models. One has tree_depth=4 and 10% error, and the other has tree_depth=7 and 9.7% error (so clearly the first model has lower complexity than the latter, but the second model is a better by a small but not negligible amount). I read it is recommended to pick the first model over the latter, but why is that? Is it because the first model is probably less complex/overfitted? And if so, then why would I care if the complexity reduces error? Is it maybe because the first model would probably have less variance when introduced to unseen datasets?\n\nNote: answer to even a single question of those would be highly appreciated and I have no statistic or math background so would highly appreciate (when possible) simple language", 
            "subreddit": "MachineLearning", 
            "title": "What exactly is overfitting and why do we prefer models that aren't overfitted even when results are better?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zdsn9/what_exactly_is_overfitting_and_why_do_we_prefer/"
        }, 
        {
            "author": "NegatioNZor", 
            "created_utc": 1505110494.0, 
            "domain": "tech.finn.no", 
            "id": "6zdnam", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zdnam/p_deep_nlpbased_recommenders_at_finnno/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Deep NLP-Based Recommenders at Finn.no", 
            "url": "https://tech.finn.no/2017/09/08/NLP-based-recommenders-at-finn/"
        }, 
        {
            "author": "asobolev", 
            "created_utc": 1505104667.0, 
            "domain": "artem.sobolev.name", 
            "id": "6zd986", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zd986/r_backpropagation_in_stochastic_computation/", 
            "score": 36, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Backpropagation in Stochastic Computation Graphs: Continuous Case", 
            "url": "http://artem.sobolev.name/posts/2017-09-10-stochastic-computation-graphs-continuous-case.html"
        }, 
        {
            "author": "kdd12345", 
            "created_utc": 1505081899.0, 
            "domain": "self.MachineLearning", 
            "id": "6zbban", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6zbban/d_how_do_you_deal_with_bias_created_by_model/", 
            "score": 4, 
            "selftext": "I work in a social service agency, hence the one time user.  When you first create a model to better target treatment to clients, you probably have a relatively random assignment going in.  This allows you to look at clients who have received treatment and those that haven't and build models with features that are (at least somewhat) balanced across both groups.  How do you rebuild the model after some period of time has elapsed though?  All of your outcomes are now based on whether your model assigned someone to the treatment or non-treatment condition.  So how do you create an unbiased sample to rebuild?\n\nThe common answer would be to run a random assignment trial, but in our case there are concerns that with holding treatment from a group that you think should get it, would be highly unethical, even if it is the only way to update a model in the future.  Are there any other options?", 
            "subreddit": "MachineLearning", 
            "title": "[D] How do you deal with bias created by model assignment?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6zbban/d_how_do_you_deal_with_bias_created_by_model/"
        }, 
        {
            "author": "jassi1994", 
            "created_utc": 1505061019.0, 
            "domain": "jasdeep06.github.io", 
            "id": "6z97gp", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z97gp/punderstanding_lstm_in_tensorflowusing_mnist/", 
            "score": 58, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P]Understanding LSTM in Tensorflow(Using MNIST dataset)", 
            "url": "https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/"
        }, 
        {
            "author": "risig_sag", 
            "created_utc": 1505055606.0, 
            "domain": "medium.com", 
            "id": "6z8o7v", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 17, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z8o7v/r_experiments_with_new_center_loss_function_for/", 
            "score": 100, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Experiments with New \"Center Loss\" Function for Classification", 
            "url": "https://medium.com/mlreview/experiments-with-a-new-loss-term-added-to-the-standard-cross-entropy-85b080c42446"
        }, 
        {
            "author": "olympus999", 
            "created_utc": 1505054516.0, 
            "domain": "self.MachineLearning", 
            "id": "6z8kry", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z8kry/d_keras_12_now_support_multigpu_mxnet_backend/", 
            "score": 9, 
            "selftext": "Source:\nhttps://aws.amazon.com/blogs/ai/apache-mxnet-release-candidate-now-supports-apple-core-ml-and-keras-v1-2/\n\n\nThe first source I found about it is from less than month ago, so most people are not aware of this (like me until now). But Keras 1.2 now supports multi GPU with MXNet backend, which seem to be really good news!\n\n\nDownside is that it works only on Keras 1.2, but one can hope to see it implemented in Keras 2.0!\n\n\nQuestions that arise:\n\n1) Does multi GPU also works with consumer graphic cards like 1070/1080? \nI see no reason why it should not work with consumer cards, but as of now I am not completely sure it does support consumer cards. I think it is a valid question as industrial cards have NVLink which makes multi-GPU usage easier. The link above is running tests on Industrial Cards.\n\n2) Is MXNet as good as Tensorflow or Theano? I personally have no experience with MXNet.\n\nI do intend to answer these questions myself after I have done some more reading. But if someone already can give their inputs, it would be nice!", 
            "subreddit": "MachineLearning", 
            "title": "[D] Keras 1.2 now support multi-GPU (MXNet backend)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6z8kry/d_keras_12_now_support_multigpu_mxnet_backend/"
        }, 
        {
            "author": "notenoughram", 
            "created_utc": 1505030659.0, 
            "domain": "fast.ai", 
            "id": "6z74rv", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 38, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z74rv/why_fastai_switched_from_keras_tf_to_pytorch/", 
            "score": 113, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Why fast.ai switched from Keras + TF to PyTorch", 
            "url": "http://www.fast.ai/2017/09/08/introducing-pytorch-for-fastai/"
        }, 
        {
            "author": "andrew000123", 
            "created_utc": 1505027640.0, 
            "domain": "self.MachineLearning", 
            "id": "6z6z89", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z6z89/how_is_ensemble_of_different_models_better_than/", 
            "score": 2, 
            "selftext": "I know that bagging reduces error by reducing the variance in the results. \n\nWhy is Ensembling of different models better than a simple bagging? Does Ensembling of different models help to reduce bias as well as variance? Or just variance? Or just bias? And how does it result in that variance/bias reduction?", 
            "subreddit": "MachineLearning", 
            "title": "How is ensemble of different models better than bagging?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6z6z89/how_is_ensemble_of_different_models_better_than/"
        }, 
        {
            "author": "tfluxxin", 
            "created_utc": 1505018513.0, 
            "domain": "nips.cc", 
            "id": "6z6g41", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z6g41/news_nips_workshops_are_up_on_the_schedule/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[News] NIPS workshops are up on the schedule", 
            "url": "https://nips.cc/Conferences/2017/Schedule"
        }, 
        {
            "author": "thestrongai", 
            "created_utc": 1505017541.0, 
            "domain": "thestrongai.com", 
            "id": "6z6dqb", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z6dqb/d_starting_a_newsletter_on_agi_and_ml_which/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Starting a newsletter on AGI and ML. Which papers (ICML/NIPS/etc) should we write tutorials about?", 
            "url": "http://thestrongai.com/"
        }, 
        {
            "author": "breandan", 
            "created_utc": 1505003219.0, 
            "domain": "arxiv.org", 
            "id": "6z5ad4", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z5ad4/r_milabot_a_deep_reinforcement_learning_chatbot/", 
            "score": 15, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] MILABOT: A Deep Reinforcement Learning Chatbot", 
            "url": "https://arxiv.org/pdf/1709.02349.pdf"
        }, 
        {
            "author": "jeffatgoogle", 
            "created_utc": 1505000459.0, 
            "domain": "self.MachineLearning", 
            "id": "6z51xb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 535, 
            "num_crossposts": 1, 
            "permalink": "/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/", 
            "score": 962, 
            "selftext": "We had so much fun at our [2016 AMA](https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/) that we\u2019re back again!\n\nWe are a group of research scientists and engineers that work on the Google Brain team. You can learn more about us and our work at [g.co/brain](http://g.co/brain), including a [list of our publications](https://research.google.com/pubs/BrainTeam.html), our [blog posts](https://research.googleblog.com/search/label/Google%20Brain), our [team's mission and culture](https://research.google.com/teams/brain/about.html), some of our particular areas of research, and can read about the experiences of our first cohort of [Google Brain Residents](http://g.co/brainresidency) who \u201cgraduated\u201d in June of 2017.\n\nYou can also learn more about the TensorFlow system that our group open-sourced at [tensorflow.org](http://tensorflow.org) in November, 2015.  In less than two years since its open-source release, TensorFlow has attracted a vibrant community of developers, machine learning researchers and practitioners from all across the globe.\n\nWe\u2019re excited to talk to you about our work, including topics like creating machines that [learn how to learn](https://research.google.com/pubs/pub45826.html), enabling people to [explore deep learning right in their browsers](https://research.googleblog.com/2017/08/harness-power-of-machine-learning-in.html), Google's custom machine learning TPU chips  and systems ([TPUv1](https://arxiv.org/abs/1704.04760) and [TPUv2](http://g.co/tpu)), use of machine learning for [robotics](http://g.co/brain/robotics) and [healthcare](http://g.co/brain/healthcare), our papers accepted to [ICLR 2017](https://research.googleblog.com/2017/04/research-at-google-and-iclr-2017.html), [ICML 2017](https://research.googleblog.com/2017/08/google-at-icml-2017.html) and NIPS 2017 (public list to be posted soon), and anything else you all want to discuss.\n\nWe're posting this a few days early to collect your questions here, and we\u2019ll be online for much of the day on September 13, 2017, starting at around 9 AM PDT to answer your questions.\n\nEdit: 9:05 AM PDT: A number of us have gathered across many locations including Mountain View, Montreal, Toronto, Cambridge (MA), and San Francisco.  Let's get this going!\n\nEdit 2: 1:49 PM PDT: We've mostly finished our large group question answering session.  Thanks for the great questions, everyone!  A few of us might continue to answer a few more questions throughout the day.\n\nWe are:\n\n* [Jeff](http://research.google.com/people/jeff) [Dean](https://scholar.google.com/citations?user=NMS69lQAAAAJ) (/u/jeffatgoogle)\n* [George](https://scholar.google.com/citations?user=ghbWy-0AAAAJ&hl=en) [Dahl](https://research.google.com/pubs/104884.html) (/u/gdahl)\n* [Samy Bengio](http://research.google.com/pubs/bengio.html) (/u/samybengio)\n* [Prajit Ramachandran](https://scholar.google.com/citations?user=ktKXDuMAAAAJ&hl=en) (/u/prajit)\n* [Alexandre Passos](https://scholar.google.com/citations?user=P3ER6nYAAAAJ&hl=en) (/u/alextp)\n* [Nicolas Le Roux](https://scholar.google.com/citations?user=LmKtwk8AAAAJ&hl=en) (/u/Nicolas_LeRoux)\n* [Sally Jesmonth](https://www.linkedin.com/in/sally-jesmonth-853b9624/) (/u/sallyjesm)\n* [Irwan Bello] (https://scholar.google.com/citations?user=mY6p8gcAAAAJ&hl=en) /u/irwan_brain)\n* [Danny Tarlow](https://scholar.google.com/citations?hl=en&user=oavgGaMAAAAJ&view_op=list_works&sortby=pubdate) (/u/dtarlow)\n* [Jasmine Hsu](https://scholar.google.com/citations?hl=en&user=WcXt6YQAAAAJ) (/u/hellojas)\n* [Vincent Vanhoucke](http://vincent.vanhoucke.com) (/u/vincentvanhoucke)\n* [Dumitru Erhan](https://scholar.google.com/citations?user=wfGiqXEAAAAJ&hl=en&oi=ao) (/u/doomie)\n* [Jascha Sohl-Dickstein](https://research.google.com/pubs/JaschaSohldickstein.html) (/u/jaschasd)\n* [Pi-Chuan Chang](https://scholar.google.com/citations?user=8_8omVoAAAAJ&hl=en) (/u/pichuan)\n* [Nick Frosst](https://scholar.google.ca/citations?user=1yVnaTgAAAAJ&hl=en) (/u/nick_frosst)\n* [Colin Raffel](https://scholar.google.com/citations?user=I66ZBYwAAAAJ&hl=en&oi=ao) (/u/craffel)\n* [Sara Hooker](https://www.linkedin.com/in/sararosehooker/) (/u/sara_brain)\n* [Greg Corrado](https://scholar.google.com/citations?user=HBtozdUAAAAJ&hl=en) (/u/gcorrado)\n* [Fernanda Vi\u00e9gas](http://hint.fm/) (/u/fernanda_viegas)\n* [Martin Wattenberg](http://hint.fm/) (/u/martin_wattenberg)\n* [Rajat Monga](https://research.google.com/pubs/RajatMonga.html) (/u/rajatmonga)\n* [Katherine Chou] (https://www.linkedin.com/in/katherinechou) (/u/katherinechou)\n* [Douglas Eck] (https://research.google.com/pubs/author39086.html) (/u/douglaseck)\n* [Jonathan Hseu] (https://www.linkedin.com/in/jonathan-hseu-38088521/) (/u/jhseu)\n* [David Dohan] (https://www.linkedin.com/in/ddohan) (/u/ddohan)\n* \u2026 and maybe others: we\u2019ll update if others become involved.", 
            "subreddit": "MachineLearning", 
            "title": "We are the Google Brain team. We'd love to answer your questions (again)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/"
        }, 
        {
            "author": "orange_robot338", 
            "created_utc": 1504996428.0, 
            "domain": "queirozf.com", 
            "id": "6z4pb1", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z4pb1/d_evaluation_metrics_for_classification_problems/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Evaluation Metrics for Classification Problems: Quick Examples + References", 
            "url": "http://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references"
        }, 
        {
            "author": "giuseppe_bonaccorso", 
            "created_utc": 1504984207.0, 
            "domain": "bonaccorso.eu", 
            "id": "6z3kmw", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z3kmw/an_annotated_path_to_start_with_machine_learning/", 
            "score": 34, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "An annotated path to start with Machine Learning", 
            "url": "https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/"
        }, 
        {
            "author": "zsdh123", 
            "created_utc": 1504975499.0, 
            "domain": "cs231n.stanford.edu", 
            "id": "6z2pyv", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z2pyv/r_stanford_deep_learning_framework_comparsion/", 
            "score": 20, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Stanford Deep Learning Framework Comparsion", 
            "url": "http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture8.pdf"
        }, 
        {
            "author": "zsdh123", 
            "created_utc": 1504975257.0, 
            "domain": "github.com", 
            "id": "6z2p5o", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z2p5o/p_reinfore_learning_tool_box_contains_trpo_a3c/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Reinfore learning tool box, contains trpo, a3c algorithm for continous action space", 
            "url": "https://github.com/jjkke88/RL_toolbox"
        }, 
        {
            "author": "kevinty", 
            "created_utc": 1504974966.0, 
            "domain": "github.com", 
            "id": "6z2o4c", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z2o4c/p_vae_with_vgg_loss_for_celeba/", 
            "score": 21, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] VAE with VGG loss for celebA", 
            "url": "https://github.com/yzwxx/vae-celebA"
        }, 
        {
            "author": "superaromatic", 
            "created_utc": 1504970928.0, 
            "domain": "self.MachineLearning", 
            "id": "6z2aac", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z2aac/d_roundup_of_relevant_freenode_irc_channels/", 
            "score": 2, 
            "selftext": "Here is a quick roundup of various [Freenode](https://freenode.net/) IRC technical chat channels associated with machine learning:\n\n* [##machinelearning](https://riot.im/app/#/room/#freenode_##machinelearning:matrix.org) (370 members)\n* [##statistics](https://riot.im/app/#/room/#freenode_##statistics:matrix.org) (160 members)\n* [#tensorflow](https://riot.im/app/#/room/#freenode_#tensorflow:matrix.org) (80 members) (mostly inactive)\n* [#scikit-learn](https://riot.im/app/#/room/#freenode_#scikit-learn:matrix.org) (50 members)\n* [##nlp](https://riot.im/app/#/room/#freenode_##nlp:matrix.org) (40 members)\n* [#keras](https://riot.im/app/#/room/#freenode_#keras:matrix.org) (20 members) (mostly inactive)\n* [##it-group](https://riot.im/app/#/room/#freenode_##it-group:matrix.org) (10 members) (intersection of information theory and learning)\n\nDid I miss any? The indicated approximate number of members is current as of the time of this post. Note that using https://riot.im/ will maintain a persistent connection which is great.\n\nDisclaimer: These channels are not associated with this subreddit.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Roundup of relevant Freenode IRC channels", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6z2aac/d_roundup_of_relevant_freenode_irc_channels/"
        }, 
        {
            "author": "crazymonezyy", 
            "created_utc": 1504956845.0, 
            "domain": "self.MachineLearning", 
            "id": "6z18ew", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 35, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z18ew/d_has_deep_learning_been_reduced_to_a_joke/", 
            "score": 0, 
            "selftext": "Was killing some time on Medium, reading the sheer number of articles on neural networks and deep learning written by \"self taught javascript/python/deep learning engineers\" with little to no knowledge of the complex math running under the hood and realized that with how Keras and CS231n have simplified stuff deep learning has become the new \"full stack javascript\". Is there even a point to actually getting an education in this area anymore or is one better off doing their research in other areas of computer science?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Has deep learning been reduced to a joke?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6z18ew/d_has_deep_learning_been_reduced_to_a_joke/"
        }, 
        {
            "author": "Karyo_Ten", 
            "created_utc": 1504952339.0, 
            "domain": "reddit.com", 
            "id": "6z0zlo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z0zlo/p_pytorch_and_pytorch_tricks_for_kaggle/", 
            "score": 88, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] PyTorch and Pytorch tricks for Kaggle competitions (Amazon satellite image comp)", 
            "url": "https://www.reddit.com/r/pytorch/comments/6z0yeo/pytorch_and_pytorch_tricks_for_kaggle/"
        }, 
        {
            "author": "suckmydb", 
            "created_utc": 1504949811.0, 
            "domain": "clclp.me", 
            "id": "6z0us0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z0us0/data_science_and_machine_learning_courses/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Data science and machine learning courses", 
            "url": "https://clclp.me/4Stf"
        }, 
        {
            "author": "inboble", 
            "created_utc": 1504949102.0, 
            "domain": "signifiedorigins.wordpress.com", 
            "id": "6z0thy", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z0thy/p_adaptive_template_model_of_intelligence/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Adaptive Template Model of Intelligence", 
            "url": "https://signifiedorigins.wordpress.com/2017/09/08/a-template-driven-approach-to-high-level-pattern-recognition/"
        }, 
        {
            "author": "orange_robot338", 
            "created_utc": 1504936242.0, 
            "domain": "queirozf.com", 
            "id": "6z057f", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6z057f/d_winning_solutions_overview_kaggle_instacart/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Winning Solutions Overview: Kaggle Instacart Competition", 
            "url": "http://queirozf.com/entries/winning-solutions-overview-kaggle-instacart-competition"
        }, 
        {
            "author": "Inori", 
            "created_utc": 1504933849.0, 
            "domain": "github.com", 
            "id": "6yzzyu", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yzzyu/p_tensorflow_agents_efficient_batched/", 
            "score": 69, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow", 
            "url": "https://github.com/tensorflow/agents"
        }, 
        {
            "author": "ajtioch", 
            "created_utc": 1504931944.0, 
            "domain": "github.com", 
            "id": "6yzvi7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yzvi7/r_sketchparse_multitask_deep_network_for_freehand/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] SketchParse: Multi-task deep network for freehand sketch segmentation (x-post - r/computervision)", 
            "url": "https://github.com/val-iisc/sketch-parse"
        }, 
        {
            "author": "gizcard", 
            "created_utc": 1504924646.0, 
            "domain": "github.com", 
            "id": "6yzcjd", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yzcjd/research_using_very_deep_autoencoders_for/", 
            "score": 33, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[Research] Using very deep autoencoders for collaborative filtering", 
            "url": "https://github.com/NVIDIA/DeepRecommender"
        }, 
        {
            "author": "anonDogeLover", 
            "created_utc": 1504913010.0, 
            "domain": "self.MachineLearning", 
            "id": "6yye4n", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yye4n/d_the_scope_of_memory_in_machine_learning/", 
            "score": 4, 
            "selftext": "I'm wondering the scope of how \"memory\" is used in machine learning. From what I've ever seen, it seems like it's only mentioned as a way of keeping information around for a while during a task where a new stream of input may need access to that information. If you are familiar with memory research in psychology and cognitive science, this would seem like a very narrow scope (that's not a criticism of the work or scope, just an observation). I can see why attention is not studied \"directly\" in machine learning, as attention seems meaningless without reference to a task in can be in service of. However, for memory, psychologists sometimes ask what problem it solves in general. For example, what if you need to remember as much as possible. Remembering pixels is too much to store. How do you determine what should be remembered? We remember lots of things that it doesn't seem like we need. Further, how does memory for an image interact with the problem of categorization? Are there papers on a more traditional view of memory in machine learning / AI that I've missed?", 
            "subreddit": "MachineLearning", 
            "title": "[D] The scope of \"memory\" in machine learning", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yye4n/d_the_scope_of_memory_in_machine_learning/"
        }, 
        {
            "author": "mkocaoglu", 
            "created_utc": 1504910930.0, 
            "domain": "github.com", 
            "id": "6yy76b", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yy76b/p_causalgan_learning_causal_implicit_generative/", 
            "score": 45, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training", 
            "url": "https://github.com/mkocaoglu/CausalGAN"
        }, 
        {
            "author": "ICanChangeTheWorld", 
            "created_utc": 1504902898.0, 
            "domain": "self.MachineLearning", 
            "id": "6yxeqo", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yxeqo/d_would_a_convolutional_neural_network_trained/", 
            "score": 3, 
            "selftext": "If we created some kind of hash function which could consistently scramble an image by changing pixel values and (potentially) moving pixels around, would a CNN still be able to learn a basic classification task?\n\nI understand that the combinations of structures are important to a CNN, but if it was training from scratch and the \"scrambling\" was consistant, is there anything stopping the network from learning these new structures just as well?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Would a convolutional neural network trained from scratch work as well on scrambled images for classification?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yxeqo/d_would_a_convolutional_neural_network_trained/"
        }, 
        {
            "author": "newperson77777777", 
            "created_utc": 1504899577.0, 
            "domain": "self.MachineLearning", 
            "id": "6yx2k8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yx2k8/d_looking_to_create_an_ensemble_of_cnns_for/", 
            "score": 2, 
            "selftext": "Hi, I'm looking to create an ensemble of CNNs for segmentation. Do you know how many CNNs I should have in my ensemble? Would I be tuning this or is there a general amount that I should have? I noticed that papers tend to have between 10-30 CNNs. How about 100? Or 1000? Are the limitations here just resource constraints? Just from reading online it seems there's a drop-off after a certain point, which makes sense because if you assume using a single classifier may vary from the mean from a certain amount, increasing the number of samples would more closely approximate the mean up until you reach a good sample size, if you're doing model averaging. Is there any resource for the number of classifiers used for ensemble methods, especially for segmentation? Also, clearly there are other types of ensemble methods as well - is there any systematic study on using different types of ensemble methods with CNNs (bagging, adaboost, etc.) and which are advantageous, especially for segmentation?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Looking to create an ensemble of CNNs for segmentation", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yx2k8/d_looking_to_create_an_ensemble_of_cnns_for/"
        }, 
        {
            "author": "alxndrkalinin", 
            "created_utc": 1504893446.0, 
            "domain": "allennlp.org", 
            "id": "6ywf92", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ywf92/p_allennlp_an_opensource_nlp_research_library/", 
            "score": 24, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] AllenNLP: An open-source NLP research library, built on PyTorch", 
            "url": "http://allennlp.org/"
        }, 
        {
            "author": "khanrc", 
            "created_utc": 1504887242.0, 
            "domain": "github.com", 
            "id": "6yvree", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 20, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yvree/p_gans_comparison_without_cherrypicking/", 
            "score": 127, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] GANs comparison without cherry-picking", 
            "url": "https://github.com/khanrc/tf.gans-comparison"
        }, 
        {
            "author": "fixedrl", 
            "created_utc": 1504885580.0, 
            "domain": "self.MachineLearning", 
            "id": "6yvkx9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yvkx9/d_how_to_set_same_dropout_mask_for_different_data/", 
            "score": 2, 
            "selftext": "Suppose there are 2 data batches, each with 32 data points. First forward pass, one random mask is generated. In second pass, another different random mask generated. The goal here is to share this two random masks.\n\nThe key challenge is that there is no explicit random mask tensor in PyTorch. ", 
            "subreddit": "MachineLearning", 
            "title": "[D] How to set same Dropout mask for different data batches in PyTorch ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yvkx9/d_how_to_set_same_dropout_mask_for_different_data/"
        }, 
        {
            "author": "cchenai", 
            "created_utc": 1504882512.0, 
            "domain": "self.MachineLearning", 
            "id": "6yv9em", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yv9em/d_starting_to_an_interview_series_who_do_you_want/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Starting to an interview series. Who do you want to see interviewed in the field? Academic or business", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yv9em/d_starting_to_an_interview_series_who_do_you_want/"
        }, 
        {
            "author": "raghavgoyal14", 
            "created_utc": 1504882069.0, 
            "domain": "medium.com", 
            "id": "6yv7sz", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yv7sz/n_gulpio_an_opensource_io_framework_for_faster/", 
            "score": 14, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] GulpIO: An open-source IO framework for faster data loading and better GPU utilization - targeting videos", 
            "url": "https://medium.com/twentybn/introducing-gulpio-f97b07f1da58"
        }, 
        {
            "author": "HigherTopoi", 
            "created_utc": 1504869326.0, 
            "domain": "self.MachineLearning", 
            "id": "6yu4nv", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yu4nv/d_reward_function_search_and_intrinsic_motivation/", 
            "score": 6, 
            "selftext": "It is a non-trivial task to hand-design a relevant reward function in the real world. Inverse RL solves this task only conditionally, as the agent needs another agent (e.g. human) that performs the task sub-optimally. What has to be done without assuming the presence of a sub-optimal agent is that we make the agent to learn a mapping from (s,a,s') to a reward r (scalar/vector). Here, we assume deterministic mapping and no delayed reward for simplicity. This mapping is learnt by the agent through its experiences via SL, semi-SL, or UL just as in model-based RL, or even via RL. \n\nInitially, the agent is equipped with a simple, man-made reward function. For example, humans are hard-coded with stimuli such as pain and hunger, which can be considered as factors of the initial reward function. Having learnt the above mapping, adults can associate increase in his bank deposit with a reward. Thus, an agent tries to widen its definition of reward from basic stimuli to abstract concepts such as promotion. I think it is unlikely that humans always perform IRL-like mechanism to learn any reward. I believe that sometimes humans use a method similar to the one I mentioned to learn reward without the presence of sub-optimal agent. \n\nIn fact, the reward function search was done by an evolutionary algorithm in Singh et. al. (2010). I suppose this kind of approach is related to intrinsic motivation and hierarchical RL, but I found it to be somehow different. Is there any recent paper discussing the approach I suggested?\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Reward function search and intrinsic motivation", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yu4nv/d_reward_function_search_and_intrinsic_motivation/"
        }, 
        {
            "author": "YoYoYL", 
            "created_utc": 1504859218.0, 
            "domain": "self.MachineLearning", 
            "id": "6ytiod", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 26, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ytiod/d_i_have_a_job_interview_about_ml_data_science/", 
            "score": 92, 
            "selftext": "What should I focus on?\nThe job consists of working with data, understanding basic ML terminology & doing some work of my own with the company's data sets.\n\nI'm a programmer.\n\nWill appreciate any help! I have about 10 hours of free studying time.", 
            "subreddit": "MachineLearning", 
            "title": "[D] I have a job interview about ML & Data Science next week and I haven't done any related ML work since school (4 years ago). What is the best way to refresh my practical & theory in one week? (Book? Crash Course?)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ytiod/d_i_have_a_job_interview_about_ml_data_science/"
        }, 
        {
            "author": "Weenkus", 
            "created_utc": 1504849204.0, 
            "domain": "datawhatnow.com", 
            "id": "6ysvpy", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ysvpy/p_pseudolabeling_a_simple_semisupervised_learning/", 
            "score": 23, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Pseudo-labeling a simple semi-supervised learning method", 
            "url": "https://datawhatnow.com/pseudo-labeling-semi-supervised-learning/"
        }, 
        {
            "author": "mehdi_san", 
            "created_utc": 1504837303.0, 
            "domain": "eng.uber.com", 
            "id": "6yrxzt", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yrxzt/n_engineering_uncertainty_estimation_in_neural/", 
            "score": 22, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber", 
            "url": "https://eng.uber.com/neural-networks-uncertainty-estimation/"
        }, 
        {
            "author": "memedesu", 
            "created_utc": 1504831325.0, 
            "domain": "arxiv.org", 
            "id": "6yref0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yref0/r_synthetic_medical_images_from_dual_generative/", 
            "score": 20, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Synthetic Medical Images from Dual Generative Adversarial Networks", 
            "url": "https://arxiv.org/abs/1709.01872"
        }, 
        {
            "author": "gau_mar", 
            "created_utc": 1504819037.0, 
            "domain": "gmarti.gitlab.io", 
            "id": "6yq7ab", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yq7ab/p_not_deep_learning_but_a_nice_trick_to_sort/", 
            "score": 78, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Not Deep Learning, but a nice trick to sort distance matrices", 
            "url": "https://gmarti.gitlab.io/ml/2017/09/07/how-to-sort-distance-matrix.html"
        }, 
        {
            "author": "not_the_irrelevant", 
            "created_utc": 1504810242.0, 
            "domain": "self.MachineLearning", 
            "id": "6yp8th", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yp8th/p_twitter_spam_dataset/", 
            "score": 1, 
            "selftext": "I am starting work on a project, i want a classified set of spam tweets. I am not getting any results when searching. does anyone have a lead on where i can get such a dataset?\nIf i cannot get the dataset, i will have to manually obtain and classify them but i don't know how to specifically get spam tweets using tweepy.", 
            "subreddit": "MachineLearning", 
            "title": "[P] Twitter spam dataset.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yp8th/p_twitter_spam_dataset/"
        }, 
        {
            "author": "bobchennan", 
            "created_utc": 1504806081.0, 
            "domain": "microsoft.com", 
            "id": "6yosis", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 28, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yosis/n_microsoft_and_facebook_create_open_ecosystem/", 
            "score": 146, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Microsoft and Facebook create open ecosystem for AI model interoperability", 
            "url": "https://www.microsoft.com/en-us/cognitive-toolkit/blog/2017/09/microsoft-facebook-create-open-ecosystem-ai-model-interoperability/"
        }, 
        {
            "author": "pmigdal", 
            "created_utc": 1504794859.0, 
            "domain": "psyarxiv.com", 
            "id": "6ynks5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 99, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ynks5/r_deep_neural_networks_are_more_accurate_than/", 
            "score": 71, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Deep neural networks are more accurate than humans at detecting sexual orientation from facial images", 
            "url": "https://psyarxiv.com/hv28a/"
        }, 
        {
            "author": "xingdongrobotics", 
            "created_utc": 1504794790.0, 
            "domain": "self.MachineLearning", 
            "id": "6ynki3", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ynki3/d_modelbased_rl_via_neural_networkbased/", 
            "score": 10, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Model-based RL via Neural Network-based approaches ? instead of GP, iLQG or MPC", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ynki3/d_modelbased_rl_via_neural_networkbased/"
        }, 
        {
            "author": "Maraat", 
            "created_utc": 1504794723.0, 
            "domain": "blog.statsbot.co", 
            "id": "6ynk8s", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 17, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ynk8s/r_forecasting_future_currency_exchange_rates_with/", 
            "score": 37, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Forecasting future currency exchange rates with recurrent neural networks", 
            "url": "https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f"
        }, 
        {
            "author": "ofirpress", 
            "created_utc": 1504785919.0, 
            "domain": "ofir.io", 
            "id": "6ymrta", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ymrta/neural_language_modeling_from_scratch/", 
            "score": 117, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Neural Language Modeling From Scratch", 
            "url": "http://ofir.io/Neural-Language-Modeling-From-Scratch/"
        }, 
        {
            "author": "teapowder", 
            "created_utc": 1504770675.0, 
            "domain": "arxiv.org", 
            "id": "6ylt0h", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ylt0h/170807524_supervised_speech_separation_based_on/", 
            "score": 31, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[1708.07524] Supervised Speech Separation Based on Deep Learning: An Overview", 
            "url": "https://arxiv.org/abs/1708.07524"
        }, 
        {
            "author": "quirky_jjs", 
            "created_utc": 1504732318.0, 
            "domain": "predictscript.com", 
            "id": "6yikag", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yikag/p_predictscript_fully_hosted_ml_for_websites/", 
            "score": 34, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] PredictScript: Fully hosted ML for websites", 
            "url": "https://predictscript.com"
        }, 
        {
            "author": "AutoModerator", 
            "created_utc": 1504713329.0, 
            "domain": "self.MachineLearning", 
            "id": "6ygh99", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 55, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ygh99/simple_questions_thread_september_06_2017/", 
            "score": 26, 
            "selftext": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!\n", 
            "subreddit": "MachineLearning", 
            "title": "Simple Questions Thread September 06, 2017", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ygh99/simple_questions_thread_september_06_2017/"
        }, 
        {
            "author": "viggyr96", 
            "created_utc": 1504711602.0, 
            "domain": "self.MachineLearning", 
            "id": "6ygap9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ygap9/deeplearningai_notes_ppt_or_pdf/", 
            "score": 4, 
            "selftext": "Is the material available for the first two courses of the specialization? It was available for the machine learning course though. It would be really helpful for the reference", 
            "subreddit": "MachineLearning", 
            "title": "Deeplearning.ai notes (Ppt or Pdf)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ygap9/deeplearningai_notes_ppt_or_pdf/"
        }, 
        {
            "author": "m_ke", 
            "created_utc": 1504709548.0, 
            "domain": "arxiv.org", 
            "id": "6yg3ak", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yg3ak/r170901507_squeezeandexcitation_networks_imagenet/", 
            "score": 101, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R][1709.01507] Squeeze-and-Excitation Networks (ImageNet Winner)", 
            "url": "https://arxiv.org/abs/1709.01507"
        }, 
        {
            "author": "zdwiel", 
            "created_utc": 1504709388.0, 
            "domain": "self.MachineLearning", 
            "id": "6yg2pn", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yg2pn/d_reinforcement_learning_algorithm_selection_as_a/", 
            "score": 5, 
            "selftext": "Has anyone seen any papers discussing this topic? I remember seeing one at one point, but I can no longer find it. If I remember correctly, the gist of the paper was that they used multiple reinforcement learning algorithms training simultaneously and a meta learner which decided which agent to use at any given time step.", 
            "subreddit": "MachineLearning", 
            "title": "[D] reinforcement learning algorithm selection as a multi-armed bandit problem", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yg2pn/d_reinforcement_learning_algorithm_selection_as_a/"
        }, 
        {
            "author": "adamw1pl", 
            "created_utc": 1504707610.0, 
            "domain": "softwaremill.com", 
            "id": "6yfwh4", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yfwh4/traffic_analysis_with_opencv_part_1/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "Traffic analysis with OpenCV: Part 1", 
            "url": "https://softwaremill.com/traffic-analysis-opencv-intro/"
        }, 
        {
            "author": "sjrj0707", 
            "created_utc": 1504706653.0, 
            "domain": "self.MachineLearning", 
            "id": "6yft89", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yft89/moving_from_a_software_engineering_into_machine/", 
            "score": 9, 
            "selftext": "I'm a Software Engineer with a Masters Degree in Computer Science and Electronics. I have lately taken an interest in machine learning and have begun teaching myself the basics. Is this possible with self study to land a machine learning role? Maybe if I enter competitions to demonstrate my competency? Or will I have to look into doing a PHD/machine learning degree?", 
            "subreddit": "MachineLearning", 
            "title": "Moving from a Software Engineering into Machine Learning", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yft89/moving_from_a_software_engineering_into_machine/"
        }, 
        {
            "author": "amathlog", 
            "created_utc": 1504705067.0, 
            "domain": "self.MachineLearning", 
            "id": "6yfnys", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yfnys/d_exploring_policy_in_qprop/", 
            "score": 9, 
            "selftext": "Hi everyone,\nI've recently read the paper for the Q-Prop by S.Gu (https://arxiv.org/abs/1611.02247) and I've a question about the Critic update.\nFor a quick summary, the idea here is to combine Stochastic Policy Gradient (like in TRPO) and Deterministic Policy Gradient (like in DDPG) to update our policy. \nIn the paper, the algorithm do not use a policy exploration like in DDPG (an off-policy), it is only on policy with the stochastic policy doing the exploration. But the critic is updated like in the DDPG paper, with a exploration policy noted \u03b2, which is defined no where in their experiments.\nTherefore, I'd like to understand what do we need to choose as \u03b2. Should we use the on-policy (stochastic policy)? Or a normal noise applied on the deterministic policy? Or else?\n\nThanks in advance!", 
            "subreddit": "MachineLearning", 
            "title": "[D] Exploring policy in Q-Prop", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yfnys/d_exploring_policy_in_qprop/"
        }, 
        {
            "author": "sgaseretto", 
            "created_utc": 1504703797.0, 
            "domain": "self.MachineLearning", 
            "id": "6yfjze", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yfjze/has_anyone_downloaded_the_deeplearningai/", 
            "score": 12, 
            "selftext": "And if so, can you share them? Preferably incompleted", 
            "subreddit": "MachineLearning", 
            "title": "Has anyone downloaded the deeplearning.ai notebooks from the coursera specialization?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6yfjze/has_anyone_downloaded_the_deeplearningai/"
        }, 
        {
            "author": "bfelbo", 
            "created_utc": 1504695246.0, 
            "domain": "github.com", 
            "id": "6yew9s", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yew9s/r_code_and_pretrained_model_available_for_emotion/", 
            "score": 18, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Code and pretrained model available for emotion analysis (sarcasm, sentiment, insults etc.)", 
            "url": "https://github.com/bfelbo/DeepMoji"
        }, 
        {
            "author": "mttd", 
            "created_utc": 1504690706.0, 
            "domain": "arxiv.org", 
            "id": "6yem6o", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 32, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yem6o/r_deep_learning_technical_introduction/", 
            "score": 119, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Deep learning: Technical introduction", 
            "url": "https://arxiv.org/abs/1709.01412"
        }, 
        {
            "author": "zsdh123", 
            "created_utc": 1504690565.0, 
            "domain": "arxiv.org", 
            "id": "6yelvi", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yelvi/r_semantic_image_synthesis/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Semantic Image Synthesis", 
            "url": "https://arxiv.org/abs/1707.06873"
        }, 
        {
            "author": "voiruloo", 
            "created_utc": 1504684367.0, 
            "domain": "github.com", 
            "id": "6ye90l", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ye90l/p_bloom_embedding_layers_recommendations_with/", 
            "score": 30, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Bloom embedding layers: recommendations with compressed latent representations", 
            "url": "https://github.com/maciejkula/spotlight/tree/master/examples/bloom_embeddings"
        }, 
        {
            "author": "smart_neuron", 
            "created_utc": 1504683663.0, 
            "domain": "eng.uber.com", 
            "id": "6ye7of", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 1, 
            "permalink": "/r/MachineLearning/comments/6ye7of/n_meet_michelangelo_ubers_machine_learning/", 
            "score": 51, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Meet Michelangelo: Uber's Machine Learning Platform", 
            "url": "https://eng.uber.com/michelangelo/"
        }, 
        {
            "author": "downtownslim", 
            "created_utc": 1504656718.0, 
            "domain": "bair.berkeley.edu", 
            "id": "6yc2mr", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yc2mr/r_learning_a_multiview_stereo_machine/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Learning a Multi-View Stereo Machine", 
            "url": "http://bair.berkeley.edu/blog/2017/09/05/unified-3d/"
        }, 
        {
            "author": "zsdh123", 
            "created_utc": 1504655307.0, 
            "domain": "github.com", 
            "id": "6ybxva", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ybxva/p_chatbot_in_200_lines_of_code_tensorflow/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Chatbot in 200 lines of code [TensorFlow]", 
            "url": "https://github.com/zsdonghao/seq2seq-chatbot"
        }, 
        {
            "author": "virivim", 
            "created_utc": 1504650148.0, 
            "domain": "self.MachineLearning", 
            "id": "6ybg9v", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ybg9v/d_help_with_understanding_the_purpose_of_stacked/", 
            "score": 6, 
            "selftext": "I think I'm close to understanding the point of Stacked Autoencoders, but I need a little bit more of a nudge.\n\nHere's what I (questionably) know:\n\n1. Functionally, Autoencoders try to closely recreate their input. For example, training an Autoencoder on MNIST (28x28=784) will produce a new image (28x28=784) that looks at least somewhat similar.\n\n2. I can pass *my own personal handwriting of 0* into the Autoencoder. From there, I could care about one of two possibilites (3 or 4).\n\n3. First possibility: I pass *my* handwriting 0 through the Encoder as a 'Dimension Reduction' tool. I can use the reduced image (5x5=25) as input to a classifier to convert my handwriting digit into Int32.Zero. \n\n4. Second possibility: I pass *my* handwriting 0 through the Encoder, and when it gets Decoded, it returns the MNIST sample that *my* handwriting is most similar to.\n\n5. Referring to this link: https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks\nWhat is the +1 Neuron?\n\nAre these applications correct? Are there any applications for stacked Autoencoders that I'm missing? ", 
            "subreddit": "MachineLearning", 
            "title": "[D] Help with understanding the 'purpose' of Stacked Autoencoders", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ybg9v/d_help_with_understanding_the_purpose_of_stacked/"
        }, 
        {
            "author": "whydomyjointshurt", 
            "created_utc": 1504649294.0, 
            "domain": "self.MachineLearning", 
            "id": "6ybd8a", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ybd8a/d_suggestions_on_feeding_very_different_2d_sensor/", 
            "score": 5, 
            "selftext": "I'm running a regression model based on change over time in various 2D sensor inputs.  Each sensor captures a 100x100 image and there are 5 sensors.  5 sensors x 2 time steps gives me 10 \"channels\".\n\nI know there are more sophisticated ways of handling the temporal dimension, but to start with, I've naively stacked the 10 channels and am feeding them into a single, very vanilla CNN (3 blocks of CNN layers feeding into a FC layer).\n\nI have two questions:\n\n1.  Does it matter if for each sensor I stack the \"before\" and \"after\" values vs., say, the \"before\" and the \"diff\" (i.e., \"after\" - \"before\")?  Would seem that the CNN can arrive at one from the other quite easily through the weights, so it shouldn't matter.  (I don't see a major difference in my few experiments so far.)\n\n2.  There is a spatial relationship between the sensors inputs, but they are measuring very different things.  So, it's interesting when features from two diff. sensors are in the same region of the 100x100 space, but the sensor data is \"shaped\" very differently.  (For instance, RGB channels of animal pictures look very similar when you inspect them individually.  But here, one sensor's input may be large, rounded blobs whereas another's may be fine stripes.)  Is the naive stacking into one CNN a decent fit for this type of input?  Will the CNN filters figure out mostly what to extract?  Or do I need to be looking at something fancier like 3D cross-channel convolution?\n\nThanks.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Suggestions on feeding very different 2D sensor inputs into a CNN", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ybd8a/d_suggestions_on_feeding_very_different_2d_sensor/"
        }, 
        {
            "author": "pkmital", 
            "created_utc": 1504644211.0, 
            "domain": "github.com", 
            "id": "6yauct", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6yauct/p_creative_applications_of_deep_learning_with/", 
            "score": 104, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Creative Applications of Deep Learning with TensorFlow Python package", 
            "url": "http://github.com/pkmital/pycadl"
        }, 
        {
            "author": "spline_reticulator", 
            "created_utc": 1504638502.0, 
            "domain": "self.MachineLearning", 
            "id": "6ya759", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6ya759/d_in_the_searching_with_policy_and_value_networks/", 
            "score": 6, 
            "selftext": "In the definition of u(s, a) they seem to compute moves using the supervised learning network p_sigma. Why don't they used the reinforcement learning network p_rho?", 
            "subreddit": "MachineLearning", 
            "title": "[D] In the Searching with policy and value networks section of the AlphaGo paper why do they compute moves using the supervised learning network instead of the reinforcement learning network?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6ya759/d_in_the_searching_with_policy_and_value_networks/"
        }, 
        {
            "author": "fixedrl", 
            "created_utc": 1504617578.0, 
            "domain": "self.MachineLearning", 
            "id": "6y7wxu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y7wxu/d_in_rl_given_optimal_qfunction_and_transition/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] In RL, given optimal Q-function and transition probabilities, reward can be reversed uniquely. How about given reward and optimal Q-function, can transition probabilities to be uniquely determined ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y7wxu/d_in_rl_given_optimal_qfunction_and_transition/"
        }, 
        {
            "author": "fixedrl", 
            "created_utc": 1504617276.0, 
            "domain": "self.MachineLearning", 
            "id": "6y7vx7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y7vx7/d_is_it_reasonable_to_maximize_the_upper_bound_of/", 
            "score": 1, 
            "selftext": "Often we talk about maximizing the lower bound in order to force the true function to be maximized. But I am encountering a trick in a paper, they maximize the upper bound of the log-likelihood function. ", 
            "subreddit": "MachineLearning", 
            "title": "[D] Is it reasonable to maximize the upper bound of the log-likelihood ? Will the log-likelihood guaranteed to be maximized ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y7vx7/d_is_it_reasonable_to_maximize_the_upper_bound_of/"
        }, 
        {
            "author": "Spotlight0xff", 
            "created_utc": 1504604774.0, 
            "domain": "ytaigman.github.io", 
            "id": "6y6y06", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y6y06/r_facebook_research_voice_synthesis_for_inthewild/", 
            "score": 20, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Facebook Research: Voice Synthesis for in-the-Wild Speakers via a Phonological Loop", 
            "url": "https://ytaigman.github.io/loop/site/"
        }, 
        {
            "author": "LeanderKu", 
            "created_utc": 1504603482.0, 
            "domain": "self.MachineLearning", 
            "id": "6y6v65", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y6v65/d_evaluation_of_the_reconstruction_error_for/", 
            "score": 1, 
            "selftext": "I was researching the usage of Reconstruction Error (of Autoencoders) for Outlier-Detection but haven't found any blog post or paper discussing the downsides of the algorithm. What are problems encountered when using the Reconstruction Error for Outlier-Detection? I am especially interested in how the Reconstruction Error behaves in comparison to the distance of some data-point to the inlier-distribution in practice (does it continuously increase? Or is there a sharp increase observable?)", 
            "subreddit": "MachineLearning", 
            "title": "[D] Evaluation of the reconstruction Error for Outlier detection", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y6v65/d_evaluation_of_the_reconstruction_error_for/"
        }, 
        {
            "author": "AreYouEvenMoist", 
            "created_utc": 1504597535.0, 
            "domain": "kdd.org", 
            "id": "6y6iem", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y6iem/r_similarity_forests/", 
            "score": 34, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Similarity Forests", 
            "url": "http://www.kdd.org/kdd2017/papers/view/similarity-forests"
        }, 
        {
            "author": "bnmasd123", 
            "created_utc": 1504592447.0, 
            "domain": "self.MachineLearning", 
            "id": "6y66ga", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y66ga/d_automatic_sentence_generation_using_gan_or/", 
            "score": 5, 
            "selftext": "How is the field evolving? What is the recent results on generating sentences with the same meaning given an sentence?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Automatic sentence generation? (using GAN, or anything...)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y66ga/d_automatic_sentence_generation_using_gan_or/"
        }, 
        {
            "author": "taki0112", 
            "created_utc": 1504586905.0, 
            "domain": "self.MachineLearning", 
            "id": "6y5s4t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y5s4t/p_simple_implementation_of_resnext_densenet_in/", 
            "score": 1, 
            "selftext": "https://github.com/taki0112/Densenet-Tensorflow\n\nhttps://github.com/taki0112/ResNeXt-Tensorflow\n\ndataset = cifar10, mnist\n\nIf any errors in the code, please let me know.\n", 
            "subreddit": "MachineLearning", 
            "title": "[P] Simple Implementation of \"ResNeXt\", \"DenseNet\" in Tensorflow", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y5s4t/p_simple_implementation_of_resnext_densenet_in/"
        }, 
        {
            "author": "_alphamaximus_", 
            "created_utc": 1504584083.0, 
            "domain": "xkcd.com", 
            "id": "6y5jo8", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 51, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y5jo8/d_xkcd_ensemble_model/", 
            "score": 407, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] xkcd: Ensemble Model", 
            "url": "https://www.xkcd.com/1885/"
        }, 
        {
            "author": "alexmlamb", 
            "created_utc": 1504583486.0, 
            "domain": "sites.google.com", 
            "id": "6y5hu3", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y5hu3/n_icml_reproducibility_in_machine_learning/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] ICML Reproducibility in Machine Learning Workshop (Photos, Slides, Summary of Panel)", 
            "url": "https://sites.google.com/view/icml-reproducibility-workshop/home"
        }, 
        {
            "author": "radarsat1", 
            "created_utc": 1504578995.0, 
            "domain": "self.MachineLearning", 
            "id": "6y535z", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y535z/p_sounderfeit_using_an_adversarial_autoencoder_to/", 
            "score": 13, 
            "selftext": "Hey guys, today at the [SBCM](http://compmus.ime.usp.br/sbcm/2017/), I presented some recent work on using adversarial conditional autoencoders to copy the data->parameter and parameter->data relationships in a physical modeling sound synyhesizer.\n\nThe basic idea was to build a dataset of individual cycles of the waveforms produced under all combinations of synth parameters of a bowed string model (e.g. violin, cello..) and learn the conditional and/or latent distribution of the codes that can be used to reproduce the same sound, as well as to estimate the parameters of new input.  Adversarial regularization was used to promote a well-behaved (uniform amd decorrelated) latent space.  The idea was to encourage the latent variables to take on interesting aspects of the variance (e.g. physical dynamics) not represented fully by the known (static) parameters, and ensure as much as possible that they follow a uniform distribution to make for useful \"knobs\".\n\nI discuss some problems encountered in sampling the dynamic latent space of the synthesizer and how I (more or less) overcame these by expanding the dataset in some ways and limiting it in others. The result is a real-time audio synth based on a neural network decoder and wavetable overlap-add synthesis.\n\nCode available here: https://gitlab.com/sinclairs/sounderfeit\nPaper available [here](https://gitlab.com/sinclairs/sounderfeit/blob/87749cb85541559a05fe31d59a221bbf99542914/doc/Sinclair2017_sounderfeit.pdf)\n\nEdit: Watch a demonstration video [available here](https://youtu.be/y1wKMhJdeUw)", 
            "subreddit": "MachineLearning", 
            "title": "[P] sounderfeit: using an adversarial autoencoder to \"clone\" a sound synthesizer, presented today at the Brazilian Symposium on Computer Music", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y535z/p_sounderfeit_using_an_adversarial_autoencoder_to/"
        }, 
        {
            "author": "taki0112", 
            "created_utc": 1504578149.0, 
            "domain": "self.MachineLearning", 
            "id": "6y50bo", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y50bo/d_summary_of_vector_similarity_better_than_cosine/", 
            "score": 7, 
            "selftext": "https://github.com/taki0112/Vector_Similarity\n\nI summarized it,\nbut... do you really think ts-ss is better than cosine?\n\nWhen I tested the similarity between words, I had a good effect. \n\nHave you ever used it somewhere else?\n\nthank you\n\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Summary of Vector Similarity (better than cosine...)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y50bo/d_summary_of_vector_similarity_better_than_cosine/"
        }, 
        {
            "author": "brotherrain", 
            "created_utc": 1504575868.0, 
            "domain": "github.com", 
            "id": "6y4sv1", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y4sv1/an_open_source_vietnamese_natural_language/", 
            "score": 9, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "An open source Vietnamese natural language processing project", 
            "url": "https://github.com/magizbox/underthesea"
        }, 
        {
            "author": "kimoz1010", 
            "created_utc": 1504566570.0, 
            "domain": "self.MachineLearning", 
            "id": "6y3xln", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y3xln/d_why_is_a_multinomial_naive_bias_multinomial/", 
            "score": 1, 
            "selftext": "I am trying to understand and absorb what a multinomial naive bayes is in the context of text classification. I am having a hard time understanding why the algorithm assumes a multinomial underlying conditional distribution. Specifically, in the context of text classification, what is the multinomial experiment? What are the number of trials and what is the nature of each trial? In each trial, what are the different outcomes?\n\nEdit: sorry for the type in the title *naive bayes", 
            "subreddit": "MachineLearning", 
            "title": "[D] Why is a multinomial naive bias \"multinomial\"?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y3xln/d_why_is_a_multinomial_naive_bias_multinomial/"
        }, 
        {
            "author": "nadbor_", 
            "created_utc": 1504560577.0, 
            "domain": "nadbordrozd.github.io", 
            "id": "6y3bz3", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y3bz3/p_benchmarking_neural_architectures_for_text/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] benchmarking neural architectures for text classification", 
            "url": "http://nadbordrozd.github.io/blog/2017/08/12/looking-for-the-text-top-model/"
        }, 
        {
            "author": "anonDogeLover", 
            "created_utc": 1504554529.0, 
            "domain": "self.MachineLearning", 
            "id": "6y2ooj", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 69, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y2ooj/d_nips_2017_decisions_are_out/", 
            "score": 66, 
            "selftext": "Post your work or discuss the decisions here", 
            "subreddit": "MachineLearning", 
            "title": "[D] NIPS 2017 decisions are out!", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y2ooj/d_nips_2017_decisions_are_out/"
        }, 
        {
            "author": "jharris-yazabi", 
            "created_utc": 1504553517.0, 
            "domain": "medium.com", 
            "id": "6y2kqp", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y2kqp/n_getting_industry_experience_a_parttime_remote/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Getting industry experience: a part-time, remote apprenticeship in ML for STEM students (grad and undergrad)", 
            "url": "https://medium.com/@jeremie_sharpestminds/levelling-up-how-to-move-beyond-ai-nanodegrees-and-land-an-actual-job-in-machine-learning-6c5009d4e60"
        }, 
        {
            "author": "redditor39", 
            "created_utc": 1504541603.0, 
            "domain": "summerofhpc.prace-ri.eu", 
            "id": "6y19hx", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y19hx/p_learning_to_track_objects_in_4d_data/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Learning to track objects in 4D data", 
            "url": "https://summerofhpc.prace-ri.eu/learning-to-track-objects-in-4d-data/"
        }, 
        {
            "author": "Kaixhin", 
            "created_utc": 1504541199.0, 
            "domain": "robot-learning.org", 
            "id": "6y17xa", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y17xa/r_accepted_papers_corl2017/", 
            "score": 21, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Accepted Papers - CoRL2017", 
            "url": "http://www.robot-learning.org/accepted-papers"
        }, 
        {
            "author": "Ru5bae8K", 
            "created_utc": 1504534157.0, 
            "domain": "self.MachineLearning", 
            "id": "6y0inj", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6y0inj/d_mcts_with_learned_options_on_common_benchmarks/", 
            "score": 1, 
            "selftext": "Is anyone aware of papers showing Monte Carlo Tree Search-based planning using learned options (or a similar form of temporal abstraction) on any recent benchmark (ALE, Gym, Roboschool, etc.)?", 
            "subreddit": "MachineLearning", 
            "title": "[D] MCTS with learned options on common benchmarks", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6y0inj/d_mcts_with_learned_options_on_common_benchmarks/"
        }, 
        {
            "author": "Delthc", 
            "created_utc": 1504526272.0, 
            "domain": "self.MachineLearning", 
            "id": "6xzw8c", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xzw8c/d_on_the_combination_of_recent_reinforcement/", 
            "score": 5, 
            "selftext": "Hello,\n\nI wonder if someone has tried to combine some of the recent RL research results that DeepMind and OpenAI published.\nThey seem to be easily implemented, combineable, and sound like a good direction for a general, strong baseline.\n\n1. PPO, a sample efficient actor-critic algorithm ( https://blog.openai.com/openai-baselines-ppo/ )\n2. Parameter Noise, to improve exploration of the agent ( https://blog.openai.com/better-exploration-with-parameter-noise/ )\n3. Value Distribution Modeling instead of prediction one average value ( https://deepmind.com/blog/going-beyond-average-reinforcement-learning/ )\n\n(I only follow the field occasionally, so excuse my ignorance on other recent research)", 
            "subreddit": "MachineLearning", 
            "title": "[D] On the combination of recent reinforcement learning research (PPO, Parameter Noise, Value Distribution)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xzw8c/d_on_the_combination_of_recent_reinforcement/"
        }, 
        {
            "author": "blauigris", 
            "created_utc": 1504525788.0, 
            "domain": "self.MachineLearning", 
            "id": "6xzv3h", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xzv3h/d_which_gpu_scheduler_are_you_using_in_your/", 
            "score": 16, 
            "selftext": "Hi everyone, \n\nI'm a PhD student. We just got a new machine with 8 gpus at the department. That's good news, but sadly, nobody thought about what low passions that this could unleash among computer-power-thirsty PhD students. We are like 15 or 20 people.The result has been that the gpus are monopolised by few people, which additionally we don't know who they are, since as we use docker all the processes are run as root. This has led to an atmosphere of accusations and suspiciousness which has darken the mood even more (if possible). I truly fear this will degenerate into an orgy of violence and death as the deadlines come closer :P\n\nSetting jokes aside, I just wanted to ask to who are using shared gpus which system are they using. I have googled a bit and I found TORQUE and kubernetes. I'd like to have some opinion on this if someone is using them. Thank you very much!\n\nP.D: Sorry for poor English. ", 
            "subreddit": "MachineLearning", 
            "title": "[D] Which GPU scheduler are you using in your multigpu machines?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xzv3h/d_which_gpu_scheduler_are_you_using_in_your/"
        }, 
        {
            "author": "Jean-Porte", 
            "created_utc": 1504522453.0, 
            "domain": "self.MachineLearning", 
            "id": "6xzney", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 34, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xzney/d_what_happenend_to_the_curse_of_dimensionality/", 
            "score": 118, 
            "selftext": "Sparse representations were popular a few years ago for their ability to fight curse of dimensionality. Now people train LSTM with 4096 dimensions with noise and no sparsity (which are considitions that magnify the curse of dimensionality).\n\n In a recent infertsent FB paper ( https://arxiv.org/abs/1705.02364 ), logistic regression is used on embedding of size 8192 and they get state of the art results on tasks with not so many training examples.\n\nAt what embedding/state size do you think curse of dimensionality is really a problem ? Are there heuristics with respect to training data size in context of neural networks ?\n\nDo you think the curse of dimensionality itself might have a regularizing effect of representations ?\n\nI'd like to know your opinons on this, thanks", 
            "subreddit": "MachineLearning", 
            "title": "[D] What happenend to the curse of dimensionality ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xzney/d_what_happenend_to_the_curse_of_dimensionality/"
        }, 
        {
            "author": "gunzdash", 
            "created_utc": 1504520305.0, 
            "domain": "self.MachineLearning", 
            "id": "6xzivo", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xzivo/p_search_hyperparameters_space/", 
            "score": 2, 
            "selftext": "Hi! I\u2019m working at a startup called [whitesmith](https://www.whitesmith.co/) and together with three colleagues we are starting to work on a new product. The product is an Open Source tool to autonomously search the hyper-parameters space efficiently using evolutionary algorithms. We had this idea because we felt that we lost too much time trying to find the right parameters, so we want to create a tool that makes you/us spend less time on parametrization and more time on the models and analysing the data.\nWe are still in the early stages of development, and we wanted to test the market and validate our idea. We would like to ask you to help us by answering a quick survey, that should take less than 5 minutes to fulfil. \nFeel free to give us feedback and ask whatever you want, I\u2019ll try to answer everything as soon as I can.\nLink to the survey - https://goo.gl/forms/CIgsBswAHUEl7LXE2", 
            "subreddit": "MachineLearning", 
            "title": "[P] Search hyper-parameters space automatically&efficiently", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xzivo/p_search_hyperparameters_space/"
        }, 
        {
            "author": "KamalChhetri", 
            "created_utc": 1504516749.0, 
            "domain": "self.MachineLearning", 
            "id": "6xzb7m", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xzb7m/can_i_create_my_own_neural_network_model_from/", 
            "score": 1, 
            "selftext": "Say i create a model with 8 conv layers 4 fc layers and pooling layers somewhere in between. I train it with imagenet data set. Later i use this model for my own data sets. Can we call this transfer learning??", 
            "subreddit": "MachineLearning", 
            "title": "Can i create my own neural network model from scratch and use it fir transfer learning?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xzb7m/can_i_create_my_own_neural_network_model_from/"
        }, 
        {
            "author": "evc123", 
            "created_utc": 1504513503.0, 
            "domain": "docs.google.com", 
            "id": "6xz43l", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 36, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xz43l/r_where_will_agi_come_from_andrej_karpathys_yconf/", 
            "score": 74, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Where will AGI come from? <-- Andrej Karpathy's YConf talk", 
            "url": "https://docs.google.com/presentation/d/119VW6ueBGLQXsw-jGMboGP2-WuOnyMAOYLgd44SL6xM/preview?slide=id.p"
        }, 
        {
            "author": "adbrebs", 
            "created_utc": 1504509181.0, 
            "domain": "lyrebird.ai", 
            "id": "6xyubx", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xyubx/d_lyrebird_beta_create_your_voice_avatar/", 
            "score": 19, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Lyrebird beta - Create your voice avatar", 
            "url": "https://lyrebird.ai/blog/lyrebird-is-back"
        }, 
        {
            "author": "axadify", 
            "created_utc": 1504495899.0, 
            "domain": "self.MachineLearning", 
            "id": "6xxuln", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xxuln/d_how_to_use_keras_to_learn_semantic_and/", 
            "score": 1, 
            "selftext": "I'm familiar with Keras for vision applications, but new to NLP. I have a text corpus and want to learn semantic and syntactic relationship present in it. I have followed several NLP examples that use pre trained GloVe embeddings to train a model for sentiment analysis or classification.\n\nBut I'm not sure how to use pre-trained GloVe embeddings to extract Semantic and Syntatic information present in data or how to label the data for this purpose?\n\nCan anyone point me in right direction?", 
            "subreddit": "MachineLearning", 
            "title": "[D] How to use Keras to learn semantic and syntactic information", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xxuln/d_how_to_use_keras_to_learn_semantic_and/"
        }, 
        {
            "author": "K0ruption", 
            "created_utc": 1504479575.0, 
            "domain": "self.MachineLearning", 
            "id": "6xwg53", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 38, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xwg53/d_why_are_neural_networks_good_function/", 
            "score": 47, 
            "selftext": "The only really rigorous result that I know of about neural nets is that their function class is dense in the space of continuous functions with compact domains (people call this the Universal approximation theorem). But such a result can easily be proven for many function classes (polynomial, trigonometric, gaussian, etc.)  as a corollary of the Stone-Weierstrass theorem. So why is it that neural networks work well and are so widely used? To be more concise, we can think of the problem of classification as learning a separatrix between high-dimensional manifolds, so why are neural networks a good approximator for such a separatrix? I understand this question may not have a definite answer, but I'm interested to see what you guys think about it and, it'd be great if you could link me to some good research done on this. Also please avoid the answer \"that's how the brain works.\" We all know it's bullshit. I'm looking for things that are actually grounded in math. Thanks.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Why are neural networks good function approximators?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xwg53/d_why_are_neural_networks_good_function/"
        }, 
        {
            "author": "orangeduck", 
            "created_utc": 1504471448.0, 
            "domain": "theorangeduck.com", 
            "id": "6xvnwo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 60, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xvnwo/d_my_neural_network_isnt_working_what_should_i_do/", 
            "score": 439, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] My Neural Network isn't working! What should I do? - A list of common mistakes made by newcomers to neural networks.", 
            "url": "http://theorangeduck.com/page/neural-network-not-working"
        }, 
        {
            "author": "laxatives", 
            "created_utc": 1504466015.0, 
            "domain": "github.com", 
            "id": "6xv4d9", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xv4d9/p_holographic_embeddings_for_graph_completion_in/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Holographic Embeddings for Graph Completion in Tensorflow", 
            "url": "https://github.com/laxatives/TensorFlow-TransX"
        }, 
        {
            "author": "MyMomSaysImHot", 
            "created_utc": 1504465821.0, 
            "domain": "youtube.com", 
            "id": "6xv3np", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": {
                "oembed": {
                    "author_name": "Two Minute Papers", 
                    "author_url": "https://www.youtube.com/user/keeroyz", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/mL3CzZcBJZU?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/mL3CzZcBJZU/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "DeepMind's AI Learns Audio And Video Concepts By Itself | Two Minute Papers #184", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xv3np/r_deepminds_ai_learns_audio_and_video_concepts_by/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] DeepMind's AI Learns Audio And Video Concepts By Itself | Two Minute Papers #184", 
            "url": "https://www.youtube.com/watch?v=mL3CzZcBJZU"
        }, 
        {
            "author": "CaHoop", 
            "created_utc": 1504453381.0, 
            "domain": "samcoope.com", 
            "id": "6xtu0i", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xtu0i/d_some_rules_i_try_to_keep_to_when_researching/", 
            "score": 14, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Some Rules I Try To Keep To When Researching", 
            "url": "http://www.samcoope.com/posts/machine_learning_research"
        }, 
        {
            "author": "longinglove", 
            "created_utc": 1504450041.0, 
            "domain": "github.com", 
            "id": "6xtivu", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xtivu/p_word_ordering_can_neural_networks_put_a/", 
            "score": 23, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Word Ordering: Can Neural Networks Put a Scramble of Words in Correct Order?", 
            "url": "https://github.com/Kyubyong/word_ordering"
        }, 
        {
            "author": "gau_mar", 
            "created_utc": 1504445284.0, 
            "domain": "gmarti.gitlab.io", 
            "id": "6xt4ou", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xt4ou/r_summary_of_datascience_summer_school_at/", 
            "score": 113, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Summary of DataScience Summer School at Polytechnique, with Bengio et al.", 
            "url": "https://gmarti.gitlab.io/ml/2017/09/02/ds3-datascience-polytechnique.html"
        }, 
        {
            "author": "Aqwis", 
            "created_utc": 1504423527.0, 
            "domain": "self.MachineLearning", 
            "id": "6xrvax", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xrvax/d_what_is_the_elements_of_statistical_learning_of/", 
            "score": 22, 
            "selftext": "I'm looking for a book to serve as a general overview of unsupervised learning (and clustering in particular), in the same way that Elements of Statistical Learning does it for supervised learning.\n\nI know that Elements does dedicate a chapter to unsupervised learning, but at just under 100 pages it's a bit brief and doesn't go much beyond K-means when it comes to clustering.", 
            "subreddit": "MachineLearning", 
            "title": "[D] What is the \"Elements of Statistical Learning\" of unsupervised learning?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xrvax/d_what_is_the_elements_of_statistical_learning_of/"
        }, 
        {
            "author": "slayerlob", 
            "created_utc": 1504419568.0, 
            "domain": "self.MachineLearning", 
            "id": "6xrnxf", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xrnxf/d_ml_and_model_based_testing/", 
            "score": 5, 
            "selftext": "This would be a new project or rather research for me. Wondering if I could make use of machine learning to predict points of risk within a model. \nI have an application which generates test cases based on a model (viewed as a flow chart). So extremely basic, then it moved to be able to attach automation scripts for objects within the model. \nI wanted to go further to see if I could use ML within model based testing. \nSearched for a few papers and wasn't convinced with an answer.. I am probably searching for the wrong terms on Google. \nSo wondering if trusty redditors had some opinions I could research or work on.", 
            "subreddit": "MachineLearning", 
            "title": "[D] ML and Model Based Testing", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xrnxf/d_ml_and_model_based_testing/"
        }, 
        {
            "author": "zergylord", 
            "created_utc": 1504418402.0, 
            "domain": "self.MachineLearning", 
            "id": "6xrlf3", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xrlf3/d_faceobject_generation_with_controllable_pose/", 
            "score": 3, 
            "selftext": "Looking to train a model to infer pose from images, but can't find any generation program with python bindings (something like [this](http://faces.cs.unibas.ch/bfm/main.php), but not in Matlab). Some [existing projects](https://github.com/willwhitney/dc-ign) share datasets of images and their associated poses, but I'd like to be able to set poses programmatically (necessary for active learning research). Given the popularity of vision-as-inverse-graphics in machine learning, I feel like a resource like this exists, but I simply can't find it.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Face/object generation with controllable pose parameters in python?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xrlf3/d_faceobject_generation_with_controllable_pose/"
        }, 
        {
            "author": "SubaruSenpai", 
            "created_utc": 1504407788.0, 
            "domain": "self.MachineLearning", 
            "id": "6xqty0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 168, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xqty0/d_what_common_misconceptions_about_machine/", 
            "score": 67, 
            "selftext": "For me it is the \"machines can only be as smart as their programmer\" one. So many things wrong with this.", 
            "subreddit": "MachineLearning", 
            "title": "[D] What common misconceptions about machine learning bother you most?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xqty0/d_what_common_misconceptions_about_machine/"
        }, 
        {
            "author": "cognitivedemons", 
            "created_utc": 1504397291.0, 
            "domain": "cognitivedemons.wordpress.com", 
            "id": "6xpy6l", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xpy6l/p_a_neural_network_in_10_lines_of_cuda_c_code/", 
            "score": 11, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] A Neural Network in 10 lines of CUDA C++ Code", 
            "url": "https://cognitivedemons.wordpress.com/2017/09/02/a-neural-network-in-10-lines-of-cuda-c-code/"
        }, 
        {
            "author": "iamwil", 
            "created_utc": 1504396365.0, 
            "domain": "self.MachineLearning", 
            "id": "6xpvh8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xpvh8/d_upload_weights_to_deploy_ml_models_on_the_web/", 
            "score": 23, 
            "selftext": "Being a web dev turned machine learning practitioner, I've deployed a few ML models into production (on the web). After doing it a few times, I found the process tedious--setting up web framework, containers, cloud services, SSL, etc. So I started building a little tool where I can just package the weights and the architecture together (for Keras), and it'd build and deploy it as an API on the web. \n\nSince then, I noticed that every once in awhile on this subreddit, someone will ask about deploying ML models on the web or how to build your own rig. Hence, maybe someone else would want to use it, so they don't have to learn web engineering or dev ops.\n\nI put together a [landing page with a template](https://www.helmspoint.com), but then I thought I'd just ask directly on the subreddit: Would this be useful for anyone else?\n\nIf it isn't, I can just keep it as a small personal tool. If it is, I'd be interested in building it out. In addition to a deploy and hosting, I also wanted it to gathered feedback data in the wild as new data to plow into the next iteration of my model. And with different iterations of my model, I want to deploy multiple models at the same time, and measure how they're doing, to make sure my model didn't regress.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Upload weights to deploy ML models on the web", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xpvh8/d_upload_weights_to_deploy_ml_models_on_the_web/"
        }, 
        {
            "author": "swagbitcoinmoney", 
            "created_utc": 1504389903.0, 
            "domain": "self.MachineLearning", 
            "id": "6xpbns", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 28, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xpbns/d_as_a_studententhusiast_for_machine_learning/", 
            "score": 17, 
            "selftext": "I'm going set up a rig for deep learning projects (especially reinforcement learning) and I have a limited budget. Does anybody know how much difference there would be between a 1070 ($400), 1080 ($550), and 1080ti ($700) in terms of speed for deep learning? ", 
            "subreddit": "MachineLearning", 
            "title": "[D] As a student/enthusiast for machine learning with limited budget, which GPU card is best for me?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xpbns/d_as_a_studententhusiast_for_machine_learning/"
        }, 
        {
            "author": "rbkillea", 
            "created_utc": 1504381688.0, 
            "domain": "youtube.com", 
            "id": "6xoh82", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": {
                "oembed": {
                    "author_name": "\u041a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0435 \u043d\u0430\u0443\u043a\u0438", 
                    "author_url": "https://www.youtube.com/channel/UCKFojzto0n4Ab3CRQRZ2zYA", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/bLqJHjXihK8?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/bLqJHjXihK8/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "18. Information Theory of Deep Learning. Naftali Tishby", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 24, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xoh82/r_information_theory_of_deep_learning_talk_at_tu/", 
            "score": 210, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Information Theory of Deep Learning (talk at TU Berlin DL Workshop by Naftali Tishby)", 
            "url": "https://www.youtube.com/watch?v=bLqJHjXihK8"
        }, 
        {
            "author": "risig_sag", 
            "created_utc": 1504381502.0, 
            "domain": "medium.com", 
            "id": "6xogjl", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xogjl/r_what_to_read_highlighted_projects_papers_and/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] What to Read: Highlighted Projects, Papers, and Videos over Last Weeks", 
            "url": "https://medium.com/@ml_review/machine-learning-weekly-review-7-93b9a7a7516c"
        }, 
        {
            "author": "sneezophile", 
            "created_utc": 1504379908.0, 
            "domain": "self.MachineLearning", 
            "id": "6xoaen", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xoaen/d_review_papers_on_modelling_of_high_dimensional/", 
            "score": 1, 
            "selftext": "I am very interested in this topic and was able to find several different types of models: [LDA](http://jmlr.csail.mit.edu/papers/v3/blei03a.html), [NADE](https://arxiv.org/abs/1605.02226), RBM, DBN, [MADE] (https://arxiv.org/abs/1502.03509) and a few others\n\nbut I wasn't able to find a review paper where methods like these are compared on different datasets. Can you recommend any?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Review papers on modelling of high dimensional discrete data", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xoaen/d_review_papers_on_modelling_of_high_dimensional/"
        }, 
        {
            "author": "maka89", 
            "created_utc": 1504366499.0, 
            "domain": "self.MachineLearning", 
            "id": "6xmp0k", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xmp0k/p_tutorial_on_deriving_backprop/", 
            "score": 16, 
            "selftext": "I have implemented backprop before, but never quite understood it. After much hard work I finally found out how to derive the backprop equations, in a way that I could understand. Wanted to share my approach. \n\n [Link](https://nbviewer.jupyter.org/github/maka89/backprop/blob/master/backprop.ipynb)", 
            "subreddit": "MachineLearning", 
            "title": "[P] Tutorial on deriving backprop", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xmp0k/p_tutorial_on_deriving_backprop/"
        }, 
        {
            "author": "bayes_preacher", 
            "created_utc": 1504363546.0, 
            "domain": "self.MachineLearning", 
            "id": "6xmd4j", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xmd4j/d_do_i_stand_a_chance_for_decent_graduate_school/", 
            "score": 1, 
            "selftext": "Hey /r/MachineLearning,\n\nI know this question has been asked many times, but I would like to ask it still.\n\nSo, I'm a student of applied math at European university, which is around 200-225th places in CS/Math in most of the ratings. I did and will do the following math coursework (I'm going to finish my Master studies next year):\n\n1) Real Analysis 1 (sequences, limits, differentiations)\n\n2) Real Analysis 2 (Taylor polynomial, Series, Power Series, Riemann integration, Improper Riemann integral)\n\n3) Real Analysis 3 (Functional sequences and series, Fourier series, Topology, Differential calculus in generalized spaces)\n\n4) Real Analysis 4 (Measure theory, Lebesgue integral, Differential forms, Surface/Line integrals, Exterior algebras, Basics of complex analysis)\n\n5) Linear Algebra 1 (Vector spaces, Bases, Subspaces, Linear transformations)\n\n6) Linear Algebra 2 (Inverse matrices, Determinants, Spectral theory, Quadratics forms and inner products, Riesz theorem, Intro to linear operator theory)\n\n7) Ordinary Differential Equations\n\n8) Numerical Math 1 (Numerical integration, Differentiation, Numerical linear algebra)\n\n9) Numerical Math 2 (Solving ODE, Nonlinear equations, Solving PDE)\n\n10) Modern Real Analysis (Intro to PDE theory, Generalized functions, Integral transformations, Integral equations, Fourier transformations)\n\n11) Complex Analysis\n\n12) Probability and Mathematical Statistics 1,2 \n\n13) General Algebra\n\n14) Topology\n\n15) Linear Programming\n\n16) Theory of Markov Chains\n\n17) Statistical Decision Theory\n\n18) Functional Analysis 1,2 - All about operators, Hilbert and Banach spaces\n\n19) Calculus of Variations\n\n20) Graph Theory\n\n21) Random Process Theory\n\n22) Advanced Numerical Linear Algebra\n\n23) Information Theory \n\n24) Regression Analysis\n\n25) Linear Models\n\n26) Probabilistic Artificial Intelligence\n\n27) Nonlinear Programming\n\n28) Statistical Learning Theory \n\nAnd few other specific math courses which are not so important. My US GPA will be around 3.5-3.6.\n\nAt research side, I've participated in Physics research during my undergraduate studies, which involved simulations of some ultra cold systems, thanks to that I've got 3 proceedings and 2 publications in impacted journal, but nothing \"world class\".\n\nSo, why I'm asking this is to realistically estimate my probabilities. From browsing sites such as Quora/Reddit I've got perception that my chances for good ML grad school are approximately 0, because I don't have GPA of 3.9+ nor JMLR publication during undergraduate studies nor I'm from TOP25 university. \n\nSo, how viable is my resum\u00e9, assuming I'm working hard on my GRE and TOEFL?\n\nThanks!\n\n\n", 
            "subreddit": "MachineLearning", 
            "title": "[D] Do I stand a chance for decent graduate school?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xmd4j/d_do_i_stand_a_chance_for_decent_graduate_school/"
        }, 
        {
            "author": "Corboner", 
            "created_utc": 1504362969.0, 
            "domain": "sigmoidal.io", 
            "id": "6xmavz", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xmavz/r_beginners_review_of_gan_architectures/", 
            "score": 147, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Beginner's review of Gan Architectures", 
            "url": "https://sigmoidal.io/beginners-review-of-gan-architectures/"
        }, 
        {
            "author": "wassname", 
            "created_utc": 1504359995.0, 
            "domain": "arxiv.org", 
            "id": "6xm0h4", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xm0h4/r_rnns_generate_yelp_reviews_that_evade_human/", 
            "score": 69, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] RNNs generate Yelp reviews that evade human detection", 
            "url": "https://arxiv.org/abs/1708.08151"
        }, 
        {
            "author": "themathstudent", 
            "created_utc": 1504356944.0, 
            "domain": "self.MachineLearning", 
            "id": "6xlr4y", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xlr4y/p_trump_tweets/", 
            "score": 0, 
            "selftext": "Using LSTMs to make a trump tweet generator.\n\nVideo: https://www.youtube.com/watch?v=neRzqYlFkR4\n\nCode: https://github.com/sachinruk/deepschool.io/blob/master/Lesson%2016%20-%20LSTM%20Trump%20Tweets%20-%20Solutions.ipynb\n\nSee especially cell 16. Lol tapping into his personality much? :P", 
            "subreddit": "MachineLearning", 
            "title": "[P] Trump Tweets", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xlr4y/p_trump_tweets/"
        }, 
        {
            "author": "__Julia", 
            "created_utc": 1504350068.0, 
            "domain": "self.MachineLearning", 
            "id": "6xl89y", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xl89y/d_how_can_we_use_data_from_different_modalities/", 
            "score": 3, 
            "selftext": "I am trying to understand how can we use data from different modalities (Text/Images) to build a better classifier. For the sake of the simplicity, and to illustrate the problem, let's say that we have data on animals. Part of the data is images (image of every animal), and some other text labels for the animals (weight, voice, is_this_animal_drink_milk or no, is_this_animal_fly or no, ... etc). How can we use both features to train Cov Neural Nets. What I was thinking, after reading [this survey](https://arxiv.org/pdf/1705.09406.pdf), is to get features for images of the last fully connected CNN + other text/labeled features (weight, voice, is_this_animal_drink_milk or no, is_this_animal_fly or no, ... etc) and use a simpler classifier like SVM to classify these animals. My question is, is there any architecture that I can use to feed data from different modalities into the classifier and instead of using SVM, I want to use end-to-end CovNeuralNets, providing an example/reference to a paper would be really helpful ?", 
            "subreddit": "MachineLearning", 
            "title": "[D] How can we use data from different modalities (Text/Images) to build NeuralNets Classifier ?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xl89y/d_how_can_we_use_data_from_different_modalities/"
        }, 
        {
            "author": "sleepybug_10", 
            "created_utc": 1504348548.0, 
            "domain": "self.MachineLearning", 
            "id": "6xl47q", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xl47q/dbuilding_a_multi_class_classifier_using/", 
            "score": 2, 
            "selftext": "I was stuck up with a problem of multi class classification on which I am bound to use an evolutionary algorithm. As far as I know these are optimization algorithms. I could find out that genetic algorithms have been used to evolve a set of weights/rules for classification models. Is there a way in which I can involve EAs in the classification process?", 
            "subreddit": "MachineLearning", 
            "title": "[D]Building a multi class classifier using evolutionary algorithms", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xl47q/dbuilding_a_multi_class_classifier_using/"
        }, 
        {
            "author": "fu_2016", 
            "created_utc": 1504336922.0, 
            "domain": "self.MachineLearning", 
            "id": "6xkeei", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xkeei/d_papers_on_outlier_detection_algorithms_theory/", 
            "score": 24, 
            "selftext": "What papers do you recommend that one read for outlier detection algorithms/theory ? \nI found a list of algorithms here - https://datascienceplus.com/outlier-app-an-interactive-visualization-of-outlier-algorithms/  (Mainly k-means and fuzzy k-means with different distance metrics, RF, Autoencoder )\nAny further suggestions are welcome. If possible please share your experience in using these algorithms on some public data-set.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Papers on outlier detection algorithms/ theory", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xkeei/d_papers_on_outlier_detection_algorithms_theory/"
        }, 
        {
            "author": "_alphamaximus_", 
            "created_utc": 1504315221.0, 
            "domain": "techdirt.com", 
            "id": "6xiwr4", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 71, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xiwr4/d_why_netflix_never_implemented_the_algorithm/", 
            "score": 317, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Why Netflix Never Implemented The Algorithm That Won The Netflix $1 Million Challenge", 
            "url": "https://www.techdirt.com/articles/20120409/03412518422/why-netflix-never-implemented-algorithm-that-won-netflix-1-million-challenge.shtml"
        }, 
        {
            "author": "PhysicsNovice", 
            "created_utc": 1504310324.0, 
            "domain": "self.MachineLearning", 
            "id": "6xih8c", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xih8c/d_dialectizer_with_word2vec/", 
            "score": 7, 
            "selftext": "What does a dialectizer look like in the context of word2vec? I was using word2vec as an initialization on a network that generates lines in a TV script (Udacity program) and I was thinking the RNN could be bolstered if the word2vec initializations were transformed in accordance with who was speaking. Is there a way to compare embedding spaces between a model trained on generic text and one trained on the text of a specific person? Could I invert the dialect of a single person so that its embedding space is generic and apply dialect after the model is trained to each line of text in accordance with who is speaking? ", 
            "subreddit": "MachineLearning", 
            "title": "[D] Dialectizer with word2vec", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xih8c/d_dialectizer_with_word2vec/"
        }, 
        {
            "author": "bfelbo", 
            "created_utc": 1504306503.0, 
            "domain": "self.MachineLearning", 
            "id": "6xi4tb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xi4tb/d_length_of_time_sequences_needed_for_language/", 
            "score": 3, 
            "selftext": "Perhaps a stupid question as I'm not that familiar with SoTA language modeling (LM), but why has there not been more interest in https://arxiv.org/abs/1702.04521 from the machine learning / NLP community? It seems surprising to me that they show great results with a RNN on 4-grams given that there's been such a big focus on LSTMs for LM. I'm assuming that these LSTMs are intended to model much longer dependencies than 4 words. Has there been any other research showing that having longer time sequences as input actually do help LM substantially?", 
            "subreddit": "MachineLearning", 
            "title": "[D] Length of time sequences needed for language modeling", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xi4tb/d_length_of_time_sequences_needed_for_language/"
        }, 
        {
            "author": "phobrain", 
            "created_utc": 1504295715.0, 
            "domain": "nypost.com", 
            "id": "6xh0pc", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xh0pc/d_whowhat_can_you_trust/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Who/what can you trust?", 
            "url": "http://nypost.com/2017/08/31/robots-learned-how-to-write-fake-yelp-reviews-like-a-human/"
        }, 
        {
            "author": "spline_reticulator", 
            "created_utc": 1504291373.0, 
            "domain": "self.MachineLearning", 
            "id": "6xgjzf", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xgjzf/d_question_about_reinforcement_learning_of_policy/", 
            "score": 1, 
            "selftext": "So if I understand the algo correctly. They first train a supervised policy conv net on the AlphaGo dataset p_sigma (a | s). This outputs an action a for a game state s. They then use sigma to instantiate a reinforcement policy network p_rho (a | s). The weights rho are then updated according to the formula\n\n\\Delta \\rho = z_t D_\\rho log p_\\rho (a_t | s_t)\n\nWhere z_t = \\pm 1 depending on if the policy wins or loses a game. What I'm unclear about is how z_t is calculated. Is computed through monte carlo simulation? I don't think they specify these details very clearly in the paper.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Question about Reinforcement learning of policy networks section in AlphaGo paper.", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xgjzf/d_question_about_reinforcement_learning_of_policy/"
        }, 
        {
            "author": "SkiddyX", 
            "created_utc": 1504291024.0, 
            "domain": "self.MachineLearning", 
            "id": "6xgio4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xgio4/d_confusion_implementing_retinanet/", 
            "score": 3, 
            "selftext": "I'm trying to implement [RetinaNet](https://arxiv.org/pdf/1708.02002.pdf), but having trouble implementing the [subnet structure](http://imgur.com/a/mZQbY). What size should the input and output the convolutional layers have? How do you calculate the loss on the anchor boxes, do you first need to offset them? Another confusion part of the implementation is they want to you apply a 3x3 conv to downsample C5, but it seems like this would result in a loss of information.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Confusion Implementing RetinaNet", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xgio4/d_confusion_implementing_retinanet/"
        }, 
        {
            "author": "breandan", 
            "created_utc": 1504284344.0, 
            "domain": "blog.acolyer.org", 
            "id": "6xfrox", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xfrox/r_automatic_database_management_system_tuning/", 
            "score": 5, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Automatic database management system tuning through large-scale machine learning", 
            "url": "https://blog.acolyer.org/2017/08/11/automatic-database-management-system-tuning-through-large-scale-machine-learning/"
        }, 
        {
            "author": "clockworkmischief", 
            "created_utc": 1504281305.0, 
            "domain": "nips.cc", 
            "id": "6xffbw", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xffbw/n_nips_2017_registration_opens/", 
            "score": 17, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] NIPS 2017 Registration Opens", 
            "url": "https://nips.cc/"
        }, 
        {
            "author": "Buck-Nasty", 
            "created_utc": 1504278221.0, 
            "domain": "youtube.com", 
            "id": "6xf3ag", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discusssion", 
            "media": {
                "oembed": {
                    "author_name": "This Week in Machine Learning & AI", 
                    "author_url": "https://www.youtube.com/channel/UC7kjWIK1H8tfmFlzZO-wHMw", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/BHrRep5DEzA?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/BHrRep5DEzA/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "J\u00fcrgen Schmidhuber Interview - LSTMs, Plus a Deep Learning History Lesson", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xf3ag/j\u00fcrgen_schmidhuber_interview_lstms_plus_a_deep/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "J\u00fcrgen Schmidhuber Interview - LSTMs, Plus a Deep Learning History Lesson", 
            "url": "https://www.youtube.com/watch?v=BHrRep5DEzA"
        }, 
        {
            "author": "bodhi_mind", 
            "created_utc": 1504276200.0, 
            "domain": "self.MachineLearning", 
            "id": "6xevr7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xevr7/d_looking_for_high_precision_low_recall/", 
            "score": 0, 
            "selftext": "*Disclaimer: I am not a statistician, but love to learn about data science, so any direction is appreciated!*\n\nI'm analyzing doctor office no show rate data. I've run chi squared analyses on potential features and have determined a bunch of statistically significant correlated features.\n\nLet's say that males are 5% less likely to show up to an appointment and under 50 age group are 5% less likely to show up. I'll be using a SVM to start out and my question is, should I be creating custom features for if a person is in multiple of these significant features or will the algorithm do that on it's own?\n\nIn my mind, I'd rather have an algorithm that has more precision even if I miss some true positives.", 
            "subreddit": "MachineLearning", 
            "title": "[D] Looking for high precision, low recall", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xevr7/d_looking_for_high_precision_low_recall/"
        }, 
        {
            "author": "Delthc", 
            "created_utc": 1504270837.0, 
            "domain": "arxiv.org", 
            "id": "6xecux", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xecux/r_google_deep_cross_network_for_ad_click/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[R] Google - Deep & Cross Network for Ad Click Predictions", 
            "url": "https://arxiv.org/pdf/1708.05123.pdf"
        }, 
        {
            "author": "XCaellaX", 
            "created_utc": 1504269403.0, 
            "domain": "appliedmldays.org", 
            "id": "6xe8g5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "two", 
            "link_flair_text": "News", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xe8g5/n_call_for_workshops_applied_machine_learning/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[N] Call for Workshops - Applied Machine Learning Days 2018", 
            "url": "https://www.appliedmldays.org/call_for_workshops"
        }, 
        {
            "author": "metacurse", 
            "created_utc": 1504268787.0, 
            "domain": "self.MachineLearning", 
            "id": "6xe6ns", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "three", 
            "link_flair_text": "Research", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xe6ns/r_psa_brace_yourselves_for_nips_2017_paper/", 
            "score": 119, 
            "selftext": "NIPS this year had a weird \"strict\" double blind review process. People were discouraged from any activity that could hurt the anonymity in the review process. So many submissions (probably most) that weren't public will get posted on arXiv following the acceptance decision. If you thought that Deep Learning research activity was cooling down these days then this was certainly one of the reasons. What this also means is that there is a chance that we may get to see some break-through stuff soon.\n\nAll of this and more, happening at an arXiv tab near you.. brace yourselves!", 
            "subreddit": "MachineLearning", 
            "title": "[R] PSA: Brace yourselves for NIPS 2017 paper hurricane -- author notification on Monday", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xe6ns/r_psa_brace_yourselves_for_nips_2017_paper/"
        }, 
        {
            "author": "nocortex", 
            "created_utc": 1504264285.0, 
            "domain": "elen.ucl.ac.be", 
            "id": "6xduma", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xduma/cfp_european_symposium_on_artificial_neural/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "CFP- European Symposium on Artificial Neural Networks,Computational Intelligence and Machine Learning", 
            "url": "https://www.elen.ucl.ac.be/esann/index.php?pg=submission"
        }, 
        {
            "author": "themathstudent", 
            "created_utc": 1504242887.0, 
            "domain": "self.MachineLearning", 
            "id": "6xchxb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xchxb/p_deepschoolio_deep_learning_learning/", 
            "score": 8, 
            "selftext": "Hi all,\n\nI'm trying to create a community around deep learning across the world similar to NodeSchool. The project is called www.DeepSchool.io (a github repo with 16 Jupyter Notebooks so far). \n\nThe blog explaining the idea is here: https://sachinruk.github.io/blog/DeepSchool.io/\n\nFeedback welcome. Checkout the trump tweets generator!", 
            "subreddit": "MachineLearning", 
            "title": "[P] DeepSchool.io; Deep Learning, Learning", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xchxb/p_deepschoolio_deep_learning_learning/"
        }, 
        {
            "author": "thehitchhiker2", 
            "created_utc": 1504235745.0, 
            "domain": "self.MachineLearning", 
            "id": "6xbxhd", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 17, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xbxhd/d_is_anyone_doing_freelance_machine_learning_if/", 
            "score": 67, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Is anyone doing freelance machine learning? If so, are you using a marketplace? and if so, which one? If you are not using a marketplace, why not?", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6xbxhd/d_is_anyone_doing_freelance_machine_learning_if/"
        }
    ], 
    "subreddit_creation_utc": 1248906884.0, 
    "subscribers": 139820, 
    "title": "Machine Learning", 
    "title_word_count_occurrences": {
        " r ": 2, 
        "amazon": 2, 
        "apple": 2, 
        "aws": 2, 
        "c#": 1, 
        "c++": 1, 
        "data science": 7, 
        "deep learning": 24, 
        "dell": 1, 
        "facebook": 6, 
        "github": 1, 
        "google": 5, 
        "haskell": 1, 
        "ibm": 1, 
        "intel": 4, 
        "linkedin": 1, 
        "machine learning": 32, 
        "microsoft": 2, 
        "netflix": 1, 
        "python": 14, 
        "software engineer": 1, 
        "tex": 5, 
        "twitter": 4, 
        "uber": 3, 
        "watson": 1
    }, 
    "top_score_submissions": [
        {
            "author": "jeffatgoogle", 
            "created_utc": 1505000459.0, 
            "domain": "self.MachineLearning", 
            "id": "6z51xb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 535, 
            "num_crossposts": 1, 
            "permalink": "/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/", 
            "score": 962, 
            "selftext": "We had so much fun at our [2016 AMA](https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/) that we\u2019re back again!\n\nWe are a group of research scientists and engineers that work on the Google Brain team. You can learn more about us and our work at [g.co/brain](http://g.co/brain), including a [list of our publications](https://research.google.com/pubs/BrainTeam.html), our [blog posts](https://research.googleblog.com/search/label/Google%20Brain), our [team's mission and culture](https://research.google.com/teams/brain/about.html), some of our particular areas of research, and can read about the experiences of our first cohort of [Google Brain Residents](http://g.co/brainresidency) who \u201cgraduated\u201d in June of 2017.\n\nYou can also learn more about the TensorFlow system that our group open-sourced at [tensorflow.org](http://tensorflow.org) in November, 2015.  In less than two years since its open-source release, TensorFlow has attracted a vibrant community of developers, machine learning researchers and practitioners from all across the globe.\n\nWe\u2019re excited to talk to you about our work, including topics like creating machines that [learn how to learn](https://research.google.com/pubs/pub45826.html), enabling people to [explore deep learning right in their browsers](https://research.googleblog.com/2017/08/harness-power-of-machine-learning-in.html), Google's custom machine learning TPU chips  and systems ([TPUv1](https://arxiv.org/abs/1704.04760) and [TPUv2](http://g.co/tpu)), use of machine learning for [robotics](http://g.co/brain/robotics) and [healthcare](http://g.co/brain/healthcare), our papers accepted to [ICLR 2017](https://research.googleblog.com/2017/04/research-at-google-and-iclr-2017.html), [ICML 2017](https://research.googleblog.com/2017/08/google-at-icml-2017.html) and NIPS 2017 (public list to be posted soon), and anything else you all want to discuss.\n\nWe're posting this a few days early to collect your questions here, and we\u2019ll be online for much of the day on September 13, 2017, starting at around 9 AM PDT to answer your questions.\n\nEdit: 9:05 AM PDT: A number of us have gathered across many locations including Mountain View, Montreal, Toronto, Cambridge (MA), and San Francisco.  Let's get this going!\n\nEdit 2: 1:49 PM PDT: We've mostly finished our large group question answering session.  Thanks for the great questions, everyone!  A few of us might continue to answer a few more questions throughout the day.\n\nWe are:\n\n* [Jeff](http://research.google.com/people/jeff) [Dean](https://scholar.google.com/citations?user=NMS69lQAAAAJ) (/u/jeffatgoogle)\n* [George](https://scholar.google.com/citations?user=ghbWy-0AAAAJ&hl=en) [Dahl](https://research.google.com/pubs/104884.html) (/u/gdahl)\n* [Samy Bengio](http://research.google.com/pubs/bengio.html) (/u/samybengio)\n* [Prajit Ramachandran](https://scholar.google.com/citations?user=ktKXDuMAAAAJ&hl=en) (/u/prajit)\n* [Alexandre Passos](https://scholar.google.com/citations?user=P3ER6nYAAAAJ&hl=en) (/u/alextp)\n* [Nicolas Le Roux](https://scholar.google.com/citations?user=LmKtwk8AAAAJ&hl=en) (/u/Nicolas_LeRoux)\n* [Sally Jesmonth](https://www.linkedin.com/in/sally-jesmonth-853b9624/) (/u/sallyjesm)\n* [Irwan Bello] (https://scholar.google.com/citations?user=mY6p8gcAAAAJ&hl=en) /u/irwan_brain)\n* [Danny Tarlow](https://scholar.google.com/citations?hl=en&user=oavgGaMAAAAJ&view_op=list_works&sortby=pubdate) (/u/dtarlow)\n* [Jasmine Hsu](https://scholar.google.com/citations?hl=en&user=WcXt6YQAAAAJ) (/u/hellojas)\n* [Vincent Vanhoucke](http://vincent.vanhoucke.com) (/u/vincentvanhoucke)\n* [Dumitru Erhan](https://scholar.google.com/citations?user=wfGiqXEAAAAJ&hl=en&oi=ao) (/u/doomie)\n* [Jascha Sohl-Dickstein](https://research.google.com/pubs/JaschaSohldickstein.html) (/u/jaschasd)\n* [Pi-Chuan Chang](https://scholar.google.com/citations?user=8_8omVoAAAAJ&hl=en) (/u/pichuan)\n* [Nick Frosst](https://scholar.google.ca/citations?user=1yVnaTgAAAAJ&hl=en) (/u/nick_frosst)\n* [Colin Raffel](https://scholar.google.com/citations?user=I66ZBYwAAAAJ&hl=en&oi=ao) (/u/craffel)\n* [Sara Hooker](https://www.linkedin.com/in/sararosehooker/) (/u/sara_brain)\n* [Greg Corrado](https://scholar.google.com/citations?user=HBtozdUAAAAJ&hl=en) (/u/gcorrado)\n* [Fernanda Vi\u00e9gas](http://hint.fm/) (/u/fernanda_viegas)\n* [Martin Wattenberg](http://hint.fm/) (/u/martin_wattenberg)\n* [Rajat Monga](https://research.google.com/pubs/RajatMonga.html) (/u/rajatmonga)\n* [Katherine Chou] (https://www.linkedin.com/in/katherinechou) (/u/katherinechou)\n* [Douglas Eck] (https://research.google.com/pubs/author39086.html) (/u/douglaseck)\n* [Jonathan Hseu] (https://www.linkedin.com/in/jonathan-hseu-38088521/) (/u/jhseu)\n* [David Dohan] (https://www.linkedin.com/in/ddohan) (/u/ddohan)\n* \u2026 and maybe others: we\u2019ll update if others become involved.", 
            "subreddit": "MachineLearning", 
            "title": "We are the Google Brain team. We'd love to answer your questions (again)", 
            "url": "https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/"
        }, 
        {
            "author": "j_lyf", 
            "created_utc": 1505749546.0, 
            "domain": "twitter.com", 
            "id": "70vuj5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 382, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/70vuj5/d_twitter_thread_on_andrew_ngs_transparent/", 
            "score": 825, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Twitter thread on Andrew Ng's transparent exploitation of young engineers in startup bubble", 
            "url": "https://twitter.com/betaorbust/status/908890982136942592"
        }, 
        {
            "author": "libreland", 
            "created_utc": 1506629763.0, 
            "domain": "groups.google.com", 
            "id": "732rxz", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 126, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/732rxz/d_theanos_dead/", 
            "score": 533, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] Theano's Dead", 
            "url": "https://groups.google.com/forum/#!topic/theano-users/7Poq8BZutbY"
        }, 
        {
            "author": "SerpentAI", 
            "created_utc": 1506124087.0, 
            "domain": "github.com", 
            "id": "71uxa5", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "four", 
            "link_flair_text": "Project", 
            "media": null, 
            "num_comments": 34, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/71uxa5/p_serpentai_game_agent_framework_turn_any_video/", 
            "score": 439, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[P] Serpent.AI - Game Agent Framework. Turn ANY video game in a sandbox environment for AI & Bot programming. (Beta Release)", 
            "url": "https://github.com/SerpentAI/SerpentAI"
        }, 
        {
            "author": "orangeduck", 
            "created_utc": 1504471448.0, 
            "domain": "theorangeduck.com", 
            "id": "6xvnwo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "one", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 60, 
            "num_crossposts": 0, 
            "permalink": "/r/MachineLearning/comments/6xvnwo/d_my_neural_network_isnt_working_what_should_i_do/", 
            "score": 439, 
            "selftext": "", 
            "subreddit": "MachineLearning", 
            "title": "[D] My Neural Network isn't working! What should I do? - A list of common mistakes made by newcomers to neural networks.", 
            "url": "http://theorangeduck.com/page/neural-network-not-working"
        }
    ], 
    "total_submissions": 412, 
    "utc_of_data_collection_completion": "2017-10-16 18:52:02", 
    "utc_of_data_collection_start": "2017-10-16 18:51:59"
}