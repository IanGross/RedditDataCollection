{
    "active_user_count": 161, 
    "advertiser_category": null, 
    "audience_target": "", 
    "avg_comment_num_per_submission": 6, 
    "avg_submission_score": 6, 
    "collection_range_end_unix_timestamp": 1506816000, 
    "collection_range_end_utc": "2017-10-01 00:00:00", 
    "collection_range_start_unix_timestamp": 1504224000, 
    "collection_range_start_utc": "2017-09-01 00:00:00", 
    "description": "### Homework questions are for [r/homeworkhelp](http://www.reddit.com/r/homeworkhelp); [How to ask a statistics question](http://www.statisticalanalysisconsulting.com/how-to-ask-a-statistics-question/); [Modmail us](http://www.reddit.com/message/compose?to=%2Fr%2Fstatistics) if your submission doesn't appear right away, it's probably in the spam filter.###\n\nThis is a subreddit for the discussion of statistical theory, software and application.\n_____________\n**Guidelines:**\n\n1. **This is not a subreddit for homework questions.** They will be swiftly removed, so don't waste your time!  Please kindly post those over at: [r/homeworkhelp](http://www.reddit.com/r/homeworkhelp) or /r/AskStatistics. Thank you.\n\n2. Please try to keep submissions on topic and of high quality.\n\n3. Just because it has a statistic in it doesn't make it statistics.\n\n4. Memes and image macros are not acceptable forms of content.\n\n5. Self posts with throwaway accounts will be deleted by AutoModerator\n_____________\n**Related subreddits:**\n\n* [r/askstatistics](http://www.reddit.com/r/askstatistics)\n* [r/machinelearning](http://www.reddit.com/r/machinelearning)\n* [r/probabilitytheory](http://www.reddit.com/r/probabilitytheory)\n* [r/datasets](http://www.reddit.com/r/datasets)\n* [r/opendata](http://www.reddit.com/r/opendata)\n* [r/rstats](http://www.reddit.com/r/rstats)\n* [r/econometrics](http://www.reddit.com/r/econometrics)\n* [r/dataisbeautiful](http://www.reddit.com/r/dataisbeautiful)\n* [r/sas/](http://www.reddit.com/r/sas)\n* [r/compsci/](http://www.reddit.com/r/compsci)\n* [r/ComputerScience/](http://www.reddit.com/r/ComputerScience)\n* [r/OnCourtAnalytics](http://www.reddit.com/r/OnCourtAnalytics)\n* [r/NBAanalytics/](http://www.reddit.com/r/NBAanalytics/)\n* [r/sportsanalytics/](https://www.reddit.com/r/sportsanalytics/)\n_____________\n**Data:**\n\n* [r/datasets](http://www.reddit.com/r/datasets/)\n* [KDnuggets Data Mining Data](http://www.kdnuggets.com/datasets/index.html)\n* [UC-Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n* [Datamob](http://datamob.org/)\n* [datasets package in R](http://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html)\n* [Kaggle](http://www.kaggle.com) <- also great for stats competitions\n* [CMU Data and Story Library](http://lib.stat.cmu.edu/DASL/)\n* [U.S. Government Data Portal](http://www.data.gov)\n* [St. Louis Fed. Reserve](http://research.stlouisfed.org/fred2/)\n* [Infochimps](http://www.infochimps.com/)\n* [AllenDowney's Stats Page](https://sites.google.com/site/thinkstats2011b/project)\n_____________\n**Useful resources for learning R:**\n\n* [r-bloggers](http://r-bloggers.com) - blog aggregator with statistics articles generally done with R software.\n* [Quick-R](http://www.statmethods.net/) - great R reference site.\n_____________\n**Related Software Links:**\n\n* [R](http://www.r-project.org/)\n* [R Studio](http://rstudio.org/)\n* [SAS](http://www.sas.com/)\n* [Stata](http://www.stata.com/)\n* [EViews](http://www.eviews.com/)\n* [JMP](http://www.jmp.com/)\n* [SPSS](http://www-01.ibm.com/software/analytics/spss/)\n* [Minitab](http://www.minitab.com)\n_____________\n**Advice for applying to grad school:**\n\n* [Submission 1](http://www.reddit.com/r/statistics/comments/ghfg0/advice_for_getting_an_ms_or_phd_in_stats/)\n_____________\n**Advice for undergrads:**\n\n* [Submission 1](http://www.reddit.com/r/statistics/comments/p7fsb/any_advice_for_a_freshman_stats_major/)\n_____________\n**Jobs and Internships**\n\nFor grads:\n\n* [ISU](http://www.stat.iastate.edu/employment/)\n\nFor undergrads:\n\n* [AMSTAT Internships](http://www.amstat.org/education/internships.cfm)\n* [NSF Internships](http://www.nsf.gov/crssprgm/reu/list_result.cfm?unitid=5044)", 
    "display_name": "statistics", 
    "domain_occurrences": {
        "academeblog.org": 1, 
        "arxiv.org": 1, 
        "bactra.org": 1, 
        "bl.ocks.org": 1, 
        "blog.almighty.press": 1, 
        "blog.statsbot.co": 1, 
        "census.gov": 1, 
        "chesterenergyandpolicy.com": 2, 
        "fastcodesign.com": 1, 
        "fivethirtyeight.com": 1, 
        "gis.washington.edu": 1, 
        "i.redd.it": 1, 
        "luckytoilet.wordpress.com": 1, 
        "medium.com": 1, 
        "mlwhiz.com": 1, 
        "paulvanderlaken.com": 1, 
        "replicationindex.wordpress.com": 1, 
        "sandipanweb.wordpress.com": 1, 
        "self.statistics": 215, 
        "shubhanshu.com": 1, 
        "statisticsbyjim.com": 1, 
        "stats.stackexchange.com": 2, 
        "sudeepraja.github.io": 2, 
        "theatlantic.com": 1, 
        "theoptimizednow.com": 1, 
        "thisisstatistics.org": 1, 
        "treasury.govt.nz": 1, 
        "youtu.be": 1, 
        "youtube.com": 2
    }, 
    "id": "2qhfi", 
    "num_external_website_posts": 32, 
    "num_text_posts": 215, 
    "public_description": "", 
    "submissions": [
        {
            "author": "Broker-Dealer", 
            "created_utc": 1506792726.0, 
            "domain": "self.statistics", 
            "id": "73g8qm", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73g8qm/anyone_have_case_studies_and_practical_examples/", 
            "score": 6, 
            "selftext": "Most of the problems I've been dealing with in my undergrad courses so far are about probability distributions and related topics. The math is just abstract, there are examples here and there, but they always assume some form of data, or the problem always works out nicely. \n\nAny redditor who is an employed statistician, researcher, or have modeled real world problems for someone shed some light on the process of how the work is done? \n\n", 
            "subreddit": "statistics", 
            "title": "Anyone have case studies and practical examples of how to model something using statistics? How do I actually *do* statistics?", 
            "url": "https://www.reddit.com/r/statistics/comments/73g8qm/anyone_have_case_studies_and_practical_examples/"
        }, 
        {
            "author": "SuddenInterest", 
            "created_utc": 1506785040.0, 
            "domain": "self.statistics", 
            "id": "73fhmy", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 17, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73fhmy/msc_in_statistics_from_european_institution/", 
            "score": 14, 
            "selftext": "I'm final year math undergrad at EU institution researching options for masters in statistics. Due to my own financial situation I must look where is cheapest, so probably be Belgium, Germany or Netherlands. How ever I don't necessarily want to work in these countries and would probably be at disadvantage as I only speak English.\n\n Wanted to ask if anyone could tell me if all European universities would be recognized when looking for statistic jobs in say UK or US?\n\nCan anyone recommend institutions or programmes? \n\nWould be great to hear form graduates form Masters of statistics form institution in either Germany, Belgium or Netherlands. Or someone who as hired( or chosen not to hire) graduates from these countries.", 
            "subreddit": "statistics", 
            "title": "MSc in Statistics from European institution", 
            "url": "https://www.reddit.com/r/statistics/comments/73fhmy/msc_in_statistics_from_european_institution/"
        }, 
        {
            "author": "zxxxuvc", 
            "created_utc": 1506752297.0, 
            "domain": "self.statistics", 
            "id": "73df3s", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73df3s/what_method_should_i_use_to_organize_all_of_my/", 
            "score": 2, 
            "selftext": "Hey everyone.\n\nI did a correlational survey in my school. Our hypothesis was \"Meat eaters with high GPAs are more likely to be sexually active.\"\n\nI'm really not sure how to organize all of the variables into a scatterplot or in a graph form.\n\nThe variables are: GPA, eating meat, and being sexually active.\n\nI was thinking doing a scatter plot with my X and Y being GPA and eating meat and using a different colored dot for if the respondent said that they were sexually active?\n\nThanks for the help.", 
            "subreddit": "statistics", 
            "title": "What method should I use to organize all of my data?", 
            "url": "https://www.reddit.com/r/statistics/comments/73df3s/what_method_should_i_use_to_organize_all_of_my/"
        }, 
        {
            "author": "gurmeet97", 
            "created_utc": 1506744462.0, 
            "domain": "self.statistics", 
            "id": "73cxhf", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73cxhf/i_want_to_switch_my_major_to_statisticsqm_have_a/", 
            "score": 3, 
            "selftext": "Currently doing accounting but I find the major to be boring. My school offers a Statistics and Quantitative Modeling. So here's what I want to know:\n1. I'm a big fan of computers and am also learning programming on my own so would a minor in CIS make me stand out more? (CS not offered at my school)\n2. Where can this degree take me?\n3. How's the pay?\n4. Is the degree in demand? I live in the east coast but plan on moving to the west.\n5. Any advice on what I should get my masters in?\n6. Is the work boring?", 
            "subreddit": "statistics", 
            "title": "I want to switch my major to statistics/QM. Have a few questions...", 
            "url": "https://www.reddit.com/r/statistics/comments/73cxhf/i_want_to_switch_my_major_to_statisticsqm_have_a/"
        }, 
        {
            "author": "ir88ed", 
            "created_utc": 1506742062.0, 
            "domain": "self.statistics", 
            "id": "73cr6i", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73cr6i/adjusting_pvalues_for_highreplicate_cancer_data/", 
            "score": 3, 
            "selftext": "I am currently working with TCGA cancer data and have a question regarding the calculation of pvalues.  I am looking at the expression of a single gene in two different conditions (tumor and normal tissue).  The tumor group has an n=1090 and the normal group has n=113.  Given the large number of replicates, my pvalues end up being very small.  Super! Right?  My question is whether I need to include some kind of multiple testing correction.  From what I have read, it sounds like this is necessary when many comparisons are made, but I am not sure if I am making 1090 comparisons, or only two.  Any help would be greatly appreciated.  I am also happy to make the data available, if necessary.  Below is a link to a simple graph of my data.\n\nhttps://i.imgur.com/q03BDob.jpg", 
            "subreddit": "statistics", 
            "title": "Adjusting pValues for High-replicate Cancer Data", 
            "url": "https://www.reddit.com/r/statistics/comments/73cr6i/adjusting_pvalues_for_highreplicate_cancer_data/"
        }, 
        {
            "author": "mnation2", 
            "created_utc": 1506736061.0, 
            "domain": "self.statistics", 
            "id": "73cakv", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73cakv/what_is_the_difference_between_random_slopes_by/", 
            "score": 7, 
            "selftext": "I've been starting to experiment with random slopes because I think treatment effect might vary according to a group variable. How is this different from interacting the treatment with the group in a vanilla OLS? Are these different ways of estimating the same thing (treatment heterogeneity)? Or are they substantively different?", 
            "subreddit": "statistics", 
            "title": "What is the difference between random slopes by group and interacting group with beta in a regular OLS?", 
            "url": "https://www.reddit.com/r/statistics/comments/73cakv/what_is_the_difference_between_random_slopes_by/"
        }, 
        {
            "author": "indigo_mints", 
            "created_utc": 1506732978.0, 
            "domain": "self.statistics", 
            "id": "73c1fj", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73c1fj/i_often_hear_if_you_do_statistics_you_can_work_in/", 
            "score": 8, 
            "selftext": "I thought you'd need education in stats PLUS whatever area you're putting it with (eg. biology for biostats).\n\nIt seems that for things like biomedical engineering (bio + engineering) you need to be equally strong in both areas. So why is it that in stats people tend to say you can go into any area even if you are purely a statistician?\n\nETA: [here](https://en.m.wikipedia.org/wiki/List_of_fields_of_application_of_statistics) is a nice list of areas which use stats. If you were just good at stats, could you move from area to area quite easily throughout your career? Or would you need to pick one area and stick with it?", 
            "subreddit": "statistics", 
            "title": "I often hear, \"If you do statistics, you can work in any field\". Why is this?", 
            "url": "https://www.reddit.com/r/statistics/comments/73c1fj/i_often_hear_if_you_do_statistics_you_can_work_in/"
        }, 
        {
            "author": "blockhead123", 
            "created_utc": 1506712205.0, 
            "domain": "self.statistics", 
            "id": "73a0c4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73a0c4/difference_in_writing_up_the_results_of_an_anova/", 
            "score": 6, 
            "selftext": "I'm looking at the following two models:\n\nanova(lm(Z ~ X*Y, data = data))\n\nanova(lmer(Z ~ X*Y + (1|ID), data = data))\n\nThe error terms of the second model are smaller than the first, but there is no difference between the two as far as significance of the main effects and interactions (X is significant, Y is not, and X:Y is significant). Would I interpret/write the results in same way, acknowledging that I included a random factor if I use the second? Are there additional considerations when using mixed effect models?", 
            "subreddit": "statistics", 
            "title": "Difference in writing up the results of an ANOVA that include random effects?", 
            "url": "https://www.reddit.com/r/statistics/comments/73a0c4/difference_in_writing_up_the_results_of_an_anova/"
        }, 
        {
            "author": "eight26", 
            "created_utc": 1506707707.0, 
            "domain": "self.statistics", 
            "id": "739in8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/739in8/estimate_standard_deviation_in_a_normal/", 
            "score": 4, 
            "selftext": "Google isn't helping me find what I'm looking for.  Is it possible to know/estimate the standard deviation of a normal distribution with a known mean?  If so, how?  Thanks.", 
            "subreddit": "statistics", 
            "title": "Estimate Standard Deviation in a Normal Distribution with a known Mean", 
            "url": "https://www.reddit.com/r/statistics/comments/739in8/estimate_standard_deviation_in_a_normal/"
        }, 
        {
            "author": "Hnikud", 
            "created_utc": 1506700303.0, 
            "domain": "self.statistics", 
            "id": "738ov9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/738ov9/where_can_i_find_a_breakdown_of_the_usa_defense/", 
            "score": 0, 
            "selftext": "  I have been struggling to get this information, and I need it for a research paper. I am trying to support the United States spending big on the army. ", 
            "subreddit": "statistics", 
            "title": "Where Can I Find a Breakdown of the U.S.A Defense Budget?", 
            "url": "https://www.reddit.com/r/statistics/comments/738ov9/where_can_i_find_a_breakdown_of_the_usa_defense/"
        }, 
        {
            "author": "SearchingForPotatoes", 
            "created_utc": 1506697462.0, 
            "domain": "self.statistics", 
            "id": "738dkw", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/738dkw/best_place_to_learn_statistics/", 
            "score": 8, 
            "selftext": "I have an awful professor for statistics and I was hoping I could get some recommendations for a good place to learn stats.\n\nEDIT: This is Stats I/probability", 
            "subreddit": "statistics", 
            "title": "Best place to learn statistics", 
            "url": "https://www.reddit.com/r/statistics/comments/738dkw/best_place_to_learn_statistics/"
        }, 
        {
            "author": "metatron301", 
            "created_utc": 1506696543.0, 
            "domain": "self.statistics", 
            "id": "7389tm", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/7389tm/correlations_of_small_count_data/", 
            "score": 4, 
            "selftext": "When working a small sample (usually less than 30) of count data that are themselves very small (<15), can you apply the basic correlation formula between the features?", 
            "subreddit": "statistics", 
            "title": "Correlations of small count data", 
            "url": "https://www.reddit.com/r/statistics/comments/7389tm/correlations_of_small_count_data/"
        }, 
        {
            "author": "obsoleeeet", 
            "created_utc": 1506691218.0, 
            "domain": "self.statistics", 
            "id": "737qlu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/737qlu/merging_two_samples/", 
            "score": 6, 
            "selftext": "For some odd reason, I'm drawing a blank at the moment. I have data from two surveys. Each were taken by students at two different universities. The survey asked Male and Female students if they liked golf.\n\n--------------------------------------------\n\nSurvey for School 1: Male Students = 3 liked golf and 98 didn't.  \nNumber of Males (nm1) = 101  \nProportion of Males that liked golf: 3/101= 0.03  \n      \nFemale Students = 11 liked golf and 125 didn't.  \nNumber of Females (nf1) = 136   \nProportion of Females that liked golf: 11/136 = 0.081   \n\n--------------------------------------------------\n\n\nSurvey for School 2: Male Students = 3 liked golf and 142 didn't.   \nNumber of Males (nm2) = 145            \nProportion of Males that liked golf: 3/145= 0.021       \n    \nFemale Students = 18 liked golf and 238 didn't.             \nNumber of Females (nf2) = 256             \nProportion of Females that liked golf: 18/256 = 0.07              \n\n-----------------------------------------------------\n\nIf I wanted to compare the proportions of all male students liking golf to all female students liking golf, can I combine the data of males and females from the different universities? \n\nIf so, is there a statistical test I need to perform to see if I can merge the sets?\n\nAny help is much appreciated!", 
            "subreddit": "statistics", 
            "title": "Merging two samples?", 
            "url": "https://www.reddit.com/r/statistics/comments/737qlu/merging_two_samples/"
        }, 
        {
            "author": "BobaFett04_13", 
            "created_utc": 1506664420.0, 
            "domain": "self.statistics", 
            "id": "735y53", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/735y53/going_for_masters_in_stats_after_econ_bachelors/", 
            "score": 2, 
            "selftext": "I\u2019m currently a junior in my undergrad in Economics, but I\u2019ve been shifting my interest to Statistics. Instead of switching majors and practically starting over, I\u2019m thinking of doing my upper-level electives in Stats and Math classes I\u2019ve seen that\u2019s needed for most Masters in Statistics degree. \n\nWill these courses prepare me enough for a Masters in Statistics? \n\nProbability and Statistics\nApplied Regression Analysis\nCalc I\nCalc II\nFoundations of Math\nFoundation of Analysis\nReal Analysis I\nReal Analysis II", 
            "subreddit": "statistics", 
            "title": "Going for Masters in Stats after Econ Bachelor's", 
            "url": "https://www.reddit.com/r/statistics/comments/735y53/going_for_masters_in_stats_after_econ_bachelors/"
        }, 
        {
            "author": "wilfordbremley", 
            "created_utc": 1506658978.0, 
            "domain": "self.statistics", 
            "id": "735k2e", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/735k2e/performing_a_withinsubjects_ttest_for_which_each/", 
            "score": 3, 
            "selftext": "Hi,\n\nI have a single pool of 60 human subjects for this experiment. Each subject will complete 8 trials during the experiment. After each trial, I will take measurements for 3 dependent variables from each subject. The first DV is a reference variable. The other two DVs are correlated with this reference variable (and also correlated with one another). My goal is to determine which of these latter two DVs is more strongly correlated with the reference DV. \n\nI know that one way I can run the analysis is with Williams' Modification of Hotelling's Test. This is a special type of t-test that compares two different correlation coefficients which share one variable (*r*(1,2) vs *r*(1,3)) and are drawn from the same sample. However, this would require computing the two correlation coefficients using each subject's mean score across the 8 trials (for each of the respective DVs). The separate measurements effectively serve to reduce error variance here. \n\nAlternatively, I could use the 8 separate measurements to generate a unique pair of correlation coefficients for each subject. Then, this pair of coefficients would serve as each subject's scores for the two respective samples in a within-subjects t-test. The t-test would compare the mean correlation coefficient for the 60 subjects for *r*(1,2) to the mean correlation coefficient for the same 60 subjects for *r*(1,3).\n\nMy question is whether this second technique is valid given the assumptions of a t-test. If so, any opinions are welcome regarding which is more appropriate. I believe the second would have much greater power. \n\nP.S.: I would never have designed a study this way purposefully -- this is a partial replication and I am trying to work around the original paradigm as best I can.", 
            "subreddit": "statistics", 
            "title": "Performing a within-subjects t-test for which each pair of scores are two correlation coefficients", 
            "url": "https://www.reddit.com/r/statistics/comments/735k2e/performing_a_withinsubjects_ttest_for_which_each/"
        }, 
        {
            "author": "fantasticsky_hng", 
            "created_utc": 1506655265.0, 
            "domain": "self.statistics", 
            "id": "73593j", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/73593j/critique_on_my_resume_for_data_analyst_internship/", 
            "score": 14, 
            "selftext": "Here is my resume: https://imgur.com/a/mt8ma\n(Typo: Related experience should be just \"experience\" as I talk about non-academic stuff)\n\nHi everyone! I have recently asked the question: \"How hard is it to get a Data Analyst internship/job?\" at https://redd.it/70ruk9. I am very thankful for our r/statistics community, as I received a lot of sincere and valuable feedback. If you think you also have some thoughts about that question, you are more than welcome to add to it. I strongly appreciate that!\nAnd, today, to make my dream of getting an internship this Summer, I again need your critiques on my resume. While I will definitely visit career centers (and I did once), I believe it is greatly beneficial to hear from experienced people like you.There are a few points that I particularly worry about:\n\n1. For my list of technical skills, I am referring to the languages that I can perform tasks from basic to intermediate (R, more advanced for Java) and the languages that I am on the process on learning them, and I see good progress, but of course in the end are still basic to slightly intermediate (Python, Excel, MySQL). For instance, if you refer to ddply or numpy, I can perform some task about them, but for deeper stuff, you still have a lot to learn. So should I just keep them as I wrote, or should I be more honest that: \"I know a few for X, a little more for Y, etc.\" \n\n2. I don't have a project about data analysis at this point, which I am afraid, they will question my ability. I am thinking of doing a few now, but as my schedule gets crazy, that may be a bit rushing. Do you think it is okay for the project part right now? Do the experience part balance things out a little bit\n\n3. How do you feel about the Career Objective? I kinda feel here and there with it!\n\nThank you so much for your time and input. I am grateful for everything you gave.\n\n   \n   ", 
            "subreddit": "statistics", 
            "title": "Critique on my resume for Data Analyst internship", 
            "url": "https://www.reddit.com/r/statistics/comments/73593j/critique_on_my_resume_for_data_analyst_internship/"
        }, 
        {
            "author": "dza76wutang", 
            "created_utc": 1506622120.0, 
            "domain": "self.statistics", 
            "id": "731x2q", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/731x2q/anyone_here_using_alteryx/", 
            "score": 0, 
            "selftext": "My team is strongly considering Alteryx as an analytics tool. I don't know anyone that uses it. Normally I'd advocate for R but based on what I have learned so far, Alteryx seems like a better product for this team as they are not quants by training and this looks quite user friendly. \n\nI'd like to get some candid perspective from a user so I can help this team ramp up and get the most value out of Alteryx.\n\nSome key considerations for me:\n\n1. Can it function as a database / data warehouse or does it have to sit on top of one?\n\n2. How are its visualization capabilities without Tableau?\n\n3. What kind of statistical tools come prepackaged?\n\n4. How is the linking compared to SQL joining?\n\n5. How easy is it to refresh workflows with new data? For example if a workflow has a 2016 data set and one wants to swap in a 2017 data set, does the workflow take it in seamlessly or is redevelopment needed?\n\nThanks in advance, trying to pierce through the marketing material is very time consuming.", 
            "subreddit": "statistics", 
            "title": "Anyone Here Using Alteryx?", 
            "url": "https://www.reddit.com/r/statistics/comments/731x2q/anyone_here_using_alteryx/"
        }, 
        {
            "author": "hareyakana", 
            "created_utc": 1506617949.0, 
            "domain": "self.statistics", 
            "id": "731fsp", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/731fsp/some_help_needed_in_representing_small_difference/", 
            "score": 2, 
            "selftext": "So I have a problem trying to highlight the difference of two similar time series but with small distinctions between them\n\nIn a perfect world scenario, the time series I have will have difference that is noticeable at small scale , [a zoom in portion of the whole series](https://imgur.com/lYaYlLE) \n\nHowever in reality the time series I have will have noise, [so it end up something like this](https://imgur.com/cJPnJbN)\n\nI look into various methods in represent the time series, autoregression, lag plots, etc.\n\nI cannot seem to find a solid method that magnifies the theoretic difference between a noised series.\n\nAny idea or suggestions how I can approach this problem.", 
            "subreddit": "statistics", 
            "title": "Some help needed in representing small difference in time series", 
            "url": "https://www.reddit.com/r/statistics/comments/731fsp/some_help_needed_in_representing_small_difference/"
        }, 
        {
            "author": "Aiptasia_Sucks", 
            "created_utc": 1506607748.0, 
            "domain": "self.statistics", 
            "id": "730b78", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/730b78/pam_dataset/", 
            "score": 1, 
            "selftext": "I have to plot data on a map, but it is packed in a PAM dataset (or at least I think it is). How can I transform this into something that I can use in Carto? \n\nA csv file for example. Or SVG/shapefile.\n\nLink to dataset in question: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/29634\n\nMany thanks!", 
            "subreddit": "statistics", 
            "title": "PAM dataset", 
            "url": "https://www.reddit.com/r/statistics/comments/730b78/pam_dataset/"
        }, 
        {
            "author": "waveskipper", 
            "created_utc": 1506604261.0, 
            "domain": "self.statistics", 
            "id": "72zyso", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72zyso/shapirowilk_test_when_samplepopulation/", 
            "score": 6, 
            "selftext": "I have two pieces of equipment(E1 and E2) for taking measurements and i wish to compare them. I have the real measurements made by much more precise equipment so i can actually create for instance a data sheet with the errors from E1 and E2. \nAll good, i was happy, but i was told it was necessary to make shapiro-wilk test to my samples. So i studied a little bit of R, watched some videos about it on youtube but one doubt still remains.. since i haven't take that many measurements, i'm using my whole sample in the tests... so isn't my population and my sample the same thing? is this test still valid?\n\n\nas you can see, i'm lost hahah. any help is much appreciated", 
            "subreddit": "statistics", 
            "title": "Shapiro-wilk test when sample=population", 
            "url": "https://www.reddit.com/r/statistics/comments/72zyso/shapirowilk_test_when_samplepopulation/"
        }, 
        {
            "author": "bluecancan", 
            "created_utc": 1506603967.0, 
            "domain": "self.statistics", 
            "id": "72zxth", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72zxth/ztest_for_proportions/", 
            "score": 3, 
            "selftext": "Hi, I want to compare whether cells treated with a chemical produced more abnormal cells at the end of the treatment compared to when treatment began. I have the percentage of abnormal cells found at both the beginning and end of the treatment, can I use a z-test for 2 proportions to compare these two percentages and see if there was a significant difference in the amount of abnormalities? Many thanks :) ", 
            "subreddit": "statistics", 
            "title": "Z-test for proportions?", 
            "url": "https://www.reddit.com/r/statistics/comments/72zxth/ztest_for_proportions/"
        }, 
        {
            "author": "serenade4strings", 
            "created_utc": 1506595786.0, 
            "domain": "self.statistics", 
            "id": "72zale", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72zale/best_statistics_job_for_extroverts_sorry_had_to/", 
            "score": 19, 
            "selftext": "I have the opposite question as a previous poster. ", 
            "subreddit": "statistics", 
            "title": "Best statistics job for *extroverts* (sorry, had to)", 
            "url": "https://www.reddit.com/r/statistics/comments/72zale/best_statistics_job_for_extroverts_sorry_had_to/"
        }, 
        {
            "author": "indigo_mints", 
            "created_utc": 1506567395.0, 
            "domain": "self.statistics", 
            "id": "72xdaw", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72xdaw/best_statistics_jobs_for_introverts/", 
            "score": 6, 
            "selftext": "Of course, there are many different jobs and different workplaces, but in general, which statistics jobs seem to have the least contact with people?", 
            "subreddit": "statistics", 
            "title": "Best statistics jobs for introverts?", 
            "url": "https://www.reddit.com/r/statistics/comments/72xdaw/best_statistics_jobs_for_introverts/"
        }, 
        {
            "author": "jorge_hg87", 
            "created_utc": 1506553490.0, 
            "domain": "self.statistics", 
            "id": "72w2l8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72w2l8/difference_of_means_but_no_correlation/", 
            "score": 4, 
            "selftext": "Hi guys.  \nI'm running a quick study at my job. We are trying to evaluate the validity of a placement exam that we have for students that want to jump up a couple of courses in the academic program.  \nWhat I'm doing is basically compare\u00bfing the results of two groups of students:  \n1. Students that got to the course via regular path (ie. going through every course prior to the course of evaluation).  \n2. Students that got to the course via placement (took the placement exam and landed on the course of evaluation).  \n\n\nThe data is population data, and it was not a normal distribution so I ran a Mann-Whitney U test on SPSS to test mean difference. In all courses, the students that got there via placement exam had better grades. And the difference was significant (though I don't know how important significance is in a mean test if I'm working with population data, not a sample).  \n\nAnyway, it was pretty clear that placement was a factor that favoured students in terms of their academic achievement, but when I ran Spearman RHO to check correlations between the course score and a dychotomic variable (0 regular students; 1 placement students), all of them had minimal coefficients (between 0.05 and 0.15).  \n\n\nI'm really confused by this. If there's a significant difference of means between groups, them being a regular or placement student should have some sort of relation to grades, right?  \nThanks for your help.", 
            "subreddit": "statistics", 
            "title": "Difference of means but no correlation?", 
            "url": "https://www.reddit.com/r/statistics/comments/72w2l8/difference_of_means_but_no_correlation/"
        }, 
        {
            "author": "Series_of_Accidents", 
            "created_utc": 1506539299.0, 
            "domain": "self.statistics", 
            "id": "72uj4z", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72uj4z/is_rmsea_a_standardized_or_unstandarized_measure/", 
            "score": 1, 
            "selftext": "I've always considered a standardized measure to be one where the units were transformed via division, and where the units are no longer in the original units. The formula for RMSEA divides by df, but also contains a radical. It's bounded by 0 and 1, so it's not going to be in the original units either. \n\nSo I'm reading a paper and it was called an unstandardized effect size and I was hoping someone could explain that to me a bit. It's a Chinese paper, so I was wondering if the author maybe meant that it's simply less interpretable since it's so affected by sample size (a fact I've read many times). Or is it really an unstandardized measure? ", 
            "subreddit": "statistics", 
            "title": "Is RMSEA a standardized or unstandarized measure?", 
            "url": "https://www.reddit.com/r/statistics/comments/72uj4z/is_rmsea_a_standardized_or_unstandarized_measure/"
        }, 
        {
            "author": "lakenp", 
            "created_utc": 1506534965.0, 
            "domain": "paulvanderlaken.com", 
            "id": "72u0yu", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72u0yu/simpsons_paradox_two_hr_examples_with_r_code/", 
            "score": 17, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Simpson's Paradox: Two HR examples with R code.", 
            "url": "https://paulvanderlaken.com/2017/09/27/simpsons-paradox-two-hr-examples-with-r-code/"
        }, 
        {
            "author": "gerradisgod", 
            "created_utc": 1506528288.0, 
            "domain": "self.statistics", 
            "id": "72t990", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72t990/what_is_it_like_to_work_at_statistics_canada_what/", 
            "score": 28, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "What is it like to work at Statistics Canada? What type of work does an analyst/economist/statistician do there?", 
            "url": "https://www.reddit.com/r/statistics/comments/72t990/what_is_it_like_to_work_at_statistics_canada_what/"
        }, 
        {
            "author": "rawtangles", 
            "created_utc": 1506527741.0, 
            "domain": "self.statistics", 
            "id": "72t6v0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72t6v0/why_its_common_to_remove_insignificant/", 
            "score": 1, 
            "selftext": "In hypothesis testing normally we fix an Type I Error threshold (alpha) and try to reject the Null Hypothesis; failing to reject the Null Hypothesis means we can't conclude anything, since we're only accounting for Type I Error. \n\nWhy is this conclusion not used in the context of Linear Models?", 
            "subreddit": "statistics", 
            "title": "Why it's common to remove insignificant coefficients from a linear model, not accounting for Type 2 Error?", 
            "url": "https://www.reddit.com/r/statistics/comments/72t6v0/why_its_common_to_remove_insignificant/"
        }, 
        {
            "author": "lukepighetti", 
            "created_utc": 1506521654.0, 
            "domain": "i.redd.it", 
            "id": "72situ", 
            "is_reddit_media_domain": true, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72situ/help_with_creating_a_two_variable_function_from/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Help with creating a two variable function from table?", 
            "url": "https://i.redd.it/vneaalcsofoz.png"
        }, 
        {
            "author": "CatSimulator", 
            "created_utc": 1506508382.0, 
            "domain": "self.statistics", 
            "id": "72re4s", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72re4s/does_this_sound_correct_r2_values/", 
            "score": 2, 
            "selftext": "Hi guys, I'm fine with everything else other than the statistics part in.. generally everything. Anyway I've got to describe the r2 values from my regression models and I'm not entirely sure as to how or exactly what they mean and I'm not sure if this is correct. I've said:\n\n\"Although all values are significant, all four response values had weak R2 values. The lowest being albedo (Figure 3.1, R2=0.073) and vegetation cover (Figure 1.1, R2=0.0772) whilst biomass (Figure 4.1, R2=0.122) and FHD (Figure 2.1, R2=0.167) were slightly higher they did not perfectly describe the presentation of the regression models as they don\u2019t explain the variability due to the low R2 values\"\n\nAlso, is the F value the individual response between two response variables? If anyone can point me in the right direction or give me any help at all, that would be very much obliged. Sorry if I'm a bit confusing but I'm even more confused! \n\nT.I.A guys, any help will be much appreciated. I think my brains about to explode.", 
            "subreddit": "statistics", 
            "title": "Does this sound correct? R2 values", 
            "url": "https://www.reddit.com/r/statistics/comments/72re4s/does_this_sound_correct_r2_values/"
        }, 
        {
            "author": "greenpea1", 
            "created_utc": 1506499501.0, 
            "domain": "statisticsbyjim.com", 
            "id": "72qumb", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72qumb/check_your_residual_plots_to_ensure_trustworthy/", 
            "score": 11, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Check Your Residual Plots to Ensure Trustworthy Regression Results!", 
            "url": "http://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/"
        }, 
        {
            "author": "qabadai", 
            "created_utc": 1506496724.0, 
            "domain": "self.statistics", 
            "id": "72qon9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 20, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72qon9/lpt_everything_you_do_must_be_reproducible/", 
            "score": 79, 
            "selftext": "I don't really come from a statistics/data science background, and don't have a lot of formal training outside online classes, but have ended up in a position where I do a ton of data analysis on a daily basis. And goddamn do I wish someone sat me down and explained this to me sooner.\n\nThe reality is it is SO EASY to mess something up and end up with data that looks right but isn't. Forgetting to enable the weights in a survey, messing up the filter in a SQL query, using count instead of distinct count in a pivot table, etc.\n\nI've had clients come back to me four months after a project saying they got different results when they ran the data. Fuck if I know, time to spend 5 hours re-running everything. SAVE YOUR WORK.\n\nI once got to the end of 6 weeks worth of data runs and analyses for a client. Went to double check the data in the final report and couldn't reproduce it. I had no idea what I had run and didn't know if I messed up early or was messing up now. Turns out it was earlier and had to re-do everything. SAVE YOUR WORK.\n\nMost of my day to day work is in excel and SPSS. Unfortunately that means a lot of it is in GUIs that can't be reproduced, so I've been slowly integrating more and more R.\n\nI know is one of things that most of you are going to be like \"duh,\" but on a deadline or just doing exploratory analysis, it's so easy to take shortcuts and forget to do it.", 
            "subreddit": "statistics", 
            "title": "LPT: Everything you do must be reproducible", 
            "url": "https://www.reddit.com/r/statistics/comments/72qon9/lpt_everything_you_do_must_be_reproducible/"
        }, 
        {
            "author": "diggsentme", 
            "created_utc": 1506490115.0, 
            "domain": "self.statistics", 
            "id": "72q8zx", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72q8zx/p_value_is_worthless/", 
            "score": 7, 
            "selftext": "Attended a stats seminar this weekend, and the presenter spent a good fifteen minutes explaining there has been a decades old argument that a \u2018p\u2019 value does not provide a good measure of evidence regarding a model or hypothesis.  That given the same ratio in the variables, the more you increase the sample size, the larger larger the differences are, the larger the p-value is, which would then indicate the HO is true.\n\nMaybe I'm not completely understanding the nuts and bolts here...can someone shed some light?", 
            "subreddit": "statistics", 
            "title": "P value is worthless?", 
            "url": "https://www.reddit.com/r/statistics/comments/72q8zx/p_value_is_worthless/"
        }, 
        {
            "author": "surrahc", 
            "created_utc": 1506489265.0, 
            "domain": "self.statistics", 
            "id": "72q6st", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 35, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72q6st/boyfriend_and_i_are_arguing_about_his_toe/", 
            "score": 8, 
            "selftext": "So he had a procedure done on his toe where the chance of it failing is 1%. It wound up failing, and he says if he gets it done again it'll be a 1% chance of failure overall. \nI'm saying that it would be a .0001% chance of failure overall, since the procedure being done twice combines the chances. He says it resets and that it's a 1% chance. \nWho is correct here? We realize this is the dumbest debate ever, but we're both stubborn people. :| ", 
            "subreddit": "statistics", 
            "title": "Boyfriend and I are arguing about his toe", 
            "url": "https://www.reddit.com/r/statistics/comments/72q6st/boyfriend_and_i_are_arguing_about_his_toe/"
        }, 
        {
            "author": "grassrootbeer", 
            "created_utc": 1506465663.0, 
            "domain": "academeblog.org", 
            "id": "72o2lg", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72o2lg/debunking_a_junk_science_survey_of_student_views/", 
            "score": 13, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Debunking a \u201cJunk Science\u201d Survey of Student Views on Free Speech", 
            "url": "https://academeblog.org/2017/09/23/debunking-a-junk-science-survey-of-student-views-on-free-speech/"
        }, 
        {
            "author": "superfuji57", 
            "created_utc": 1506464889.0, 
            "domain": "self.statistics", 
            "id": "72nzwy", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72nzwy/how_to_test_for_significant_difference_in/", 
            "score": 7, 
            "selftext": "For example, the local McDonalds has a 4.5 rating on Yelp. How can we see if a 4.2 rating over the last 7 days is statistically significant? ", 
            "subreddit": "statistics", 
            "title": "How to test for significant difference in five-star ratings?", 
            "url": "https://www.reddit.com/r/statistics/comments/72nzwy/how_to_test_for_significant_difference_in/"
        }, 
        {
            "author": "thinkofanamefast", 
            "created_utc": 1506460321.0, 
            "domain": "self.statistics", 
            "id": "72ni1q", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72ni1q/do_these_numbers_make_sense_stats_for_survival/", 
            "score": 1, 
            "selftext": "The median survival was 21.5 months for patients with resected IPMNs, ranging from 2 to 124 months, and 14 months in non-resected IPMN patients, ranging from 5.5 to 70 months. There is no significant survival difference between the resected and non-resected groups, with a 5-year survival of 69.8% in resected IPMNs and 59.8% in non-resected IPMNs, P = 0.347. https://link.springer.com/article/10.1007/s00268-005-0035-8", 
            "subreddit": "statistics", 
            "title": "Do these numbers make sense? Stats for survival for pancreatic \"IPMN\" cysts when resected (operated on) vs. not resected. How can 5 years survival be over 50% if median survival is 2 years or less? Thx. x-post from R/badstats", 
            "url": "https://www.reddit.com/r/statistics/comments/72ni1q/do_these_numbers_make_sense_stats_for_survival/"
        }, 
        {
            "author": "Vladimir09", 
            "created_utc": 1506455810.0, 
            "domain": "self.statistics", 
            "id": "72mzdy", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72mzdy/most_bs_way_usage_of_statistics_on_healthcare/", 
            "score": 7, 
            "selftext": "http://www.businessinsider.com/healthcare-spending-out-of-pocket-by-state-2017-9\n\nThis article touts that they analyzed over 2 million customer banking accounts and from this data they derrived that Healthcare out of pocket costs do not exceed $1,000. This is complete bullshit way of using data. The data set only contains a fraction of required data to make such analysis and is lacking some obvious ways their customers could have spent money out of pocket such as credit cards at other banks or FSA which is deducted from paycheck and never reaches the bank. And they even have the audacity to say they are over estimating the total out of pocket cost... Also there is no data on the high cost of monthly payments.\n\nThis will only make those burried in medical bills depressed by lying that their neighbors aren't experiencing similar difficulties.", 
            "subreddit": "statistics", 
            "title": "Most BS way usage of statistics on Healthcare costs", 
            "url": "https://www.reddit.com/r/statistics/comments/72mzdy/most_bs_way_usage_of_statistics_on_healthcare/"
        }, 
        {
            "author": "ToasterWaffles", 
            "created_utc": 1506446042.0, 
            "domain": "self.statistics", 
            "id": "72lu4t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72lu4t/eli5_tolerance_intervals_plus_confidence_and/", 
            "score": 1, 
            "selftext": "I never learned any of this in school but I've been using these in my job for a couple years but I feel like a need a really simple explanation to tie my understanding all together.  \n\nPlaying around in Minitab it seems like a tolerance interval with 50% confidence/ 50% reliability is just the average of a set of data but I don't feel like I understand how each confidence and reliability affects the output.  \n\nDoes a 95% reliability mean my result will always be above the result 95% of the time, but only with 95% confidence that the data that gave me that result was correct?  ", 
            "subreddit": "statistics", 
            "title": "ELI5 Tolerance Intervals (plus confidence and reliability)", 
            "url": "https://www.reddit.com/r/statistics/comments/72lu4t/eli5_tolerance_intervals_plus_confidence_and/"
        }, 
        {
            "author": "gurghet", 
            "created_utc": 1506444791.0, 
            "domain": "stats.stackexchange.com", 
            "id": "72loxe", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72loxe/if_i_want_to_have_95_chance_that_less_than_1/", 
            "score": 7, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "If I want to have 95% chance that less than 1% objects are faulty, how many samples do I need?", 
            "url": "https://stats.stackexchange.com/questions/304886/if-i-want-to-have-95-chance-that-less-than-1-objects-are-faulty-how-many-samp?atw=1"
        }, 
        {
            "author": "tacforall", 
            "created_utc": 1506438828.0, 
            "domain": "self.statistics", 
            "id": "72l02o", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72l02o/ovulation_cycles_of_women/", 
            "score": 1, 
            "selftext": "It's a well known fact that women are more sexual during ovulation days. I also heard that women in the same environment tend to synchronize their ovulation cycle with other women. Does this sychronization forms into some sort of macro pattern, and could this be represented by common (median) ovulation days per country?\n\nFor science of course.", 
            "subreddit": "statistics", 
            "title": "Ovulation cycles of women", 
            "url": "https://www.reddit.com/r/statistics/comments/72l02o/ovulation_cycles_of_women/"
        }, 
        {
            "author": "slammaster", 
            "created_utc": 1506427625.0, 
            "domain": "self.statistics", 
            "id": "72jw2t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 38, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72jw2t/good_example_of_1tailed_ttest/", 
            "score": 3, 
            "selftext": "When I teach my intro stats course I tell my students that you should almost never use a 1-tailed t-test, that the 2-tailed version is almost always more appropriate.  Nevertheless I feel like I should give them an example of where it is appropriate, but I can't find any on the web, and I'd prefer to use a real-life example if possible.\n\nDoes anyone on here have a good example of a 1-tailed t-test that is appropriately used?  Every example I find on the web seems contrived to demonstrate the math, and not the concept.", 
            "subreddit": "statistics", 
            "title": "Good example of 1-tailed t-test", 
            "url": "https://www.reddit.com/r/statistics/comments/72jw2t/good_example_of_1tailed_ttest/"
        }, 
        {
            "author": "Synthemesque", 
            "created_utc": 1506417522.0, 
            "domain": "self.statistics", 
            "id": "72j6z2", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72j6z2/cox_regression_how_to_calculate_baseline_hazard/", 
            "score": 9, 
            "selftext": "Hi everybody. \nFirst of all, please show mercy, I'm just a simple med student trying to do some basic research. \n\nI'm trying to look at certain risk factors and how they affect the 10-year risk of developing a certain form of cancer. To do this I'm using the Cox regression model to estimate hazard ratios for my risk factors, or covariates. \n\nAs far as I understand it; if I can calculate the HR for these risk factors, or covariates, and find out the baseline 10 year cumulative hazard, where these covariates have the lowest possible value, I can estimate the 10-year hazard rate given different combinations of covariates.\n\nHowever, I have no clue how to calculate the baseline cumulative risk, as SPSS only allows me to do this at the 'means of covariates'. \n\nHow is the hazard function for 'means of covariates' connected to the hazard ratios? It was my understanding that the hazard ratios were a comparison between the absence(0) and the presence(1) of the covariates I entered. To estimate the absolute risk increase I have no use of the baseline hazard rate for covariate means, right?\n\nI hope I made any sense with this. I appreciate any help possible.", 
            "subreddit": "statistics", 
            "title": "[COX regression] How to calculate baseline hazard rate not using means of covariates in SPSS.", 
            "url": "https://www.reddit.com/r/statistics/comments/72j6z2/cox_regression_how_to_calculate_baseline_hazard/"
        }, 
        {
            "author": "mecolema", 
            "created_utc": 1506383803.0, 
            "domain": "self.statistics", 
            "id": "72gjmc", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72gjmc/how_can_a_mediator_explain_over_100_of_a/", 
            "score": 1, 
            "selftext": "After adding several potential mediators to a regression model, I've found that the coefficient on my main explanatory variable has decreased by over 100% and is now negative. How am I to interpret this finding? \n\nThe purpose of my model is to measure attenuation in coefficients as potential mediators are added to the regression, but I can't plausibly say that the mediators explain \"over 100%\" of the relationship between x and y...", 
            "subreddit": "statistics", 
            "title": "How can a mediator explain over 100% of a relationship?", 
            "url": "https://www.reddit.com/r/statistics/comments/72gjmc/how_can_a_mediator_explain_over_100_of_a/"
        }, 
        {
            "author": "More_Momus", 
            "created_utc": 1506382875.0, 
            "domain": "self.statistics", 
            "id": "72ggfh", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72ggfh/how_to_deal_with_a_regression_where_i_have_an/", 
            "score": 5, 
            "selftext": "OK, first off, I wasn't involved in the research methods/data analysis planning process. I was brought in after the data was collected, when they found out they didn't have anyone who could actually perform the analysis (i.e. >5 yr ago). \n\nSo, I kinda threw out their plan for the analysis they wrote into the research plan, because I didn't think it was the best way to go about answering the question they wanted answered. (that and because they typically use softwares, such as excel or systat, which are my non-preferred tools)\n\n**Description of the Data:**\n\n* The goal was to determine the effects of a drug *in vivo* based on an *in vitro* assay. \n* Both the *in vivo* and *in vitro* data is discretely measured with a scale [0,+infinity), but the cutoff for clinical efficacy is values <6\n* The data was collected in healthy volunteers and in patients. So there are three major groups of subjects:\n   1. patients whose in vitro says they are 'sensitive' and  the in vivo matches (because they are on a correct dose)\n   2. patients whose in vitro says they are 'sensitive' and the in vivo doesn't match (because they are on too low of a dose)\n   3. patients whose in vitro says they are 'resistant' and the in vivo \"should\" never match (mechanism of drug resistance means that no increase in dose will help)\n* They pulled the in vivo data in quadruplicate, but the in vitro data in triplicate (I don't know why)\n* The healthy volunteers have in vitro data taken from when on and off the drug; whereas the patients only have in vitro data taken when on the drug \n\n**My Current Plan:**\n\n* Was going to do a regression in R (they originally only planned for an ANOVA/ANCOVA, but seeing how the goal was to make predictions, I figured a full multivariate regression model was more appropriate)\n* Was going to build the model that basically looked like:\n\n         lme(InVivo ~ InVitro * Dose,\n             random = 1|ID) \n* considering also doing a logistic model for resistant/sensitive based on the clinical cutoff we use, but figured that a regression on the actual values would be more appropriate, then leave the clinical decision for whoever is doing the prediction\n\n**Major Questions:**\n\n1. How do I account for the fact that, because the the number of InVivo measures doesn't match the InVitro measures for a given covariate (mainly dose)? As of now I've wrangled the data into a long format with both In vivo and In vitro measures all in the same column, but marked by a 1/0 flag indicator in a \"Vitro_1\" column of the data frame.\n\n2. How can I modify the random effect portion of the lme model to account for intra-assay variability?\n\n3. Are there any other considerations that might be appropriate for this analysis? (I'm currently considering post-hoc tests for temporal effects, but they *should* be nominal given a washouts between dosing level changes)\n\n\n\nP.S. I'm so sorry about how long this is, I just want to make sure that whatever superhero decides this post is worth responding to has all the info they might need. I really appreciate any and all help; I work with data analysis that is similar to this, but I kinda picked up this project as a charity case. That and it was supposed to be a quick way to get a publication for myself haha. \n\n", 
            "subreddit": "statistics", 
            "title": "How to deal with a regression where I have an unbalanced design with an unequal number of replicates between dependent and independent variables.", 
            "url": "https://www.reddit.com/r/statistics/comments/72ggfh/how_to_deal_with_a_regression_where_i_have_an/"
        }, 
        {
            "author": "lukamodric1", 
            "created_utc": 1506365221.0, 
            "domain": "self.statistics", 
            "id": "72ejci", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72ejci/can_you_help_me_with_preparing_panel_data_in/", 
            "score": 5, 
            "selftext": "Hi,\nI want to perform a gravity model for migration using panel data in STATA, but I am having a hard time preparing data in Excel. This is not my first time using STATA, I am familiar with the basic panel models, but I have never encountered a problem with bilateral flows. From what I have read so far, many have used the count panel data models, which I think I will manage to estimate once I arrange data to be imported. \n\nMy model consists of the following variables:\nDependent variable - Immigration inflow by citizenship from country i (origin) to country j (destination) in period 1998-2015\nExplanatory:\n1. GDP of country i in period 1995-2016\n2. GDP of country j in period 1995-2016\n3. Unemployment in country i in period 1995-2016\n4. Unemployment in country j in period 1995-2016\n5. Life expectancy in country i in period 1995-2016\n6. Young people in country i in period 1995-2016\n7. Number of foreigners in country j from country i in period 1998-2015\n8. Dummy if countries are neighbors (time invariant)\n9. Distance between countries (time invariant)\n\nAs you can see, there are a lot of variables of different types, so I would be extremely grateful for any advice and/or Excel example on how to prepare it all for importing in STATA.\n\nP.S. I can send you my data if it helps.\n\nThanks in advance!\n", 
            "subreddit": "statistics", 
            "title": "Can you help me with Preparing Panel Data in Excel for Gravity Model in STATA?", 
            "url": "https://www.reddit.com/r/statistics/comments/72ejci/can_you_help_me_with_preparing_panel_data_in/"
        }, 
        {
            "author": "hamstersmagic", 
            "created_utc": 1506357373.0, 
            "domain": "self.statistics", 
            "id": "72dm9j", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72dm9j/gaussian_process_regression_for_large_datasets/", 
            "score": 8, 
            "selftext": "My boss specifically wants to do Guassian Process Regression for our dataset. It uses time to predict course grades. We want more accurate confidence intervals to model the uncertainty around timeslots that we don't have data for. (For example classes start at 8am and 9am but we don't have any data for classes that start at 8:30am) \n\nHowever all the Gaussian Process Regression package only work with maybe sub 750 data points and after that it's just too slow. Are there any python or R packages that I can use for this? The dataset is around 1 million entries. Is this even possible? I've been googling around a lot but haven't figured anything out. \n\nedit: will probably go with bayes trees. thanks again to /u/frequentlybayes", 
            "subreddit": "statistics", 
            "title": "Gaussian Process Regression for Large Datasets", 
            "url": "https://www.reddit.com/r/statistics/comments/72dm9j/gaussian_process_regression_for_large_datasets/"
        }, 
        {
            "author": "honoursurvey", 
            "created_utc": 1506337763.0, 
            "domain": "self.statistics", 
            "id": "72bpsu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72bpsu/spss_qualitative_coding_question/", 
            "score": 7, 
            "selftext": "I am having some trouble with qualitative data in my thesis. I have got five variables with qualitative responses, the questions were 'list things you find stressful' (over five different time periods). I am not concerned regarding the responses themselves rather if they wrote a response or left the question blank.\n\nIs there a way to create a new variable which shows their level of response, for example they would be given a value of '1' if they only responded to one of the five stress related questions.\n\nI thought possibly giving the empty responses a '1' value then creating a new value with the mean of all five questions. However it did not work because the data is both string and numerical.\n\nThe aim is to exclude people who completed equal to or less than 2 of the responses. \n\nI am aware I can manually code this however there are over 300 responses so I am wondering if there is a faster way.\n\nThank you! If this is the wrong forum let me know and I will remove! ", 
            "subreddit": "statistics", 
            "title": "SPSS qualitative coding question", 
            "url": "https://www.reddit.com/r/statistics/comments/72bpsu/spss_qualitative_coding_question/"
        }, 
        {
            "author": "hareyakana", 
            "created_utc": 1506326097.0, 
            "domain": "self.statistics", 
            "id": "72azz6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72azz6/arima_for_classification_problems/", 
            "score": 7, 
            "selftext": "Note: I am a physicist by profession so pardon for my mistakes/unclear explanation.\n\nI have a large set of data where each entry corresponds to a [waveform](https://imgur.com/a/07k2r). I am still trying to wrap my head around this topic with the usage of ARIMA in statsmodels in python. \n\nSomeone suggested to me I can use ARIMA for this classification problem of mine. However I am still unable to wrap my head around it how it works. \n\nAny expert here could give a word of advice on this issue?", 
            "subreddit": "statistics", 
            "title": "ARIMA for Classification problems", 
            "url": "https://www.reddit.com/r/statistics/comments/72azz6/arima_for_classification_problems/"
        }, 
        {
            "author": "indigo_mints", 
            "created_utc": 1506308025.0, 
            "domain": "self.statistics", 
            "id": "729qru", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/729qru/whats_a_good_way_to_move_forward_if_you_are/", 
            "score": 7, 
            "selftext": "I heard a lot of stats careers needs Masters degrees at least, but if you're not sure you want to go that route yet, and you don't want to waste time, what are some things you can do in the mean time? Is doing online courses in stats something good to have on your CV or does it mean practically nothing to employers unless you have that Masters?\n\nAlso, would you recommend learning straight stats, or learning a ton of different applications (like biostats)? I'm just not sure where to start, or if I'm wasting my time.", 
            "subreddit": "statistics", 
            "title": "What's a good way to move forward if you are unsure if a stats career is something you want to do?", 
            "url": "https://www.reddit.com/r/statistics/comments/729qru/whats_a_good_way_to_move_forward_if_you_are/"
        }, 
        {
            "author": "peahensharmila", 
            "created_utc": 1506307047.0, 
            "domain": "youtube.com", 
            "id": "729o5a", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": {
                "oembed": {
                    "author_name": "Datawithanalyticstats", 
                    "author_url": "https://www.youtube.com/channel/UCYloMLuMW_mf2HUihs7O14w", 
                    "description": "How to Export CSPro Data File to STATA", 
                    "height": 338, 
                    "html": "<iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FyGVWBxH5zJQ%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DyGVWBxH5zJQ&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FyGVWBxH5zJQ%2Fhqdefault.jpg&key=522baf40bd3911e08d854040d3dc5c07&type=text%2Fhtml&schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/yGVWBxH5zJQ/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "How to Export CSPro Data File to STATA", 
                    "type": "video", 
                    "url": "http://www.youtube.com/watch?v=yGVWBxH5zJQ", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/729o5a/how_to_export_cspro_data_file_to_stata/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "How to Export CSPro Data File to STATA", 
            "url": "https://www.youtube.com/attribution_link?a=x2n37goWDp4&u=%2Fwatch%3Fv%3DyGVWBxH5zJQ%26feature%3Dshare"
        }, 
        {
            "author": "SeraphsCall", 
            "created_utc": 1506304721.0, 
            "domain": "self.statistics", 
            "id": "729htb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/729htb/what_statistical_methodanalysis_should_i_use/", 
            "score": 1, 
            "selftext": "Hey! I'm running an experiment and need help selecting what statistical analysis method I should use. :)\n\nExperiment is looking at vision - specifically colour discrimination at varying visual field angles.\n\nIndependent Variables (both are categorical):\n\n- IV.1 - Colour [x4]\n\n- IV.2 - Angle of Visual Field [x4]\n\nDependent Variables (both are ratio/scale variables):\n\n- DV.1 - Stimulus Seen\n\n- DV.2 - Color Identified\n\nMain Aims/Hypotheses\n\n1 - Are there any significant differences between when the stimulus was seen [DV1] and when the color was identified [DV2] - do this for each colour [IV1] at every angle [IV2]\n\n2 - Are there any signifiant differences between when the color was identified [DV2] based on the angle the shape was presented [DV2] - do this for each colour [IV1]\n\nAny idea what statistical test would be best used to solve this? I want to avoid running multiple tests that increase the risk of making a Type 1 Error.", 
            "subreddit": "statistics", 
            "title": "What statistical method/analysis should I use?", 
            "url": "https://www.reddit.com/r/statistics/comments/729htb/what_statistical_methodanalysis_should_i_use/"
        }, 
        {
            "author": "Damian89Berlin", 
            "created_utc": 1506276868.0, 
            "domain": "self.statistics", 
            "id": "726p4j", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/726p4j/check_similarityconsistancy_between_five_training/", 
            "score": 8, 
            "selftext": "Hi guys,\ndo you guys know a fast method to check if a dataset (out of currently five) is similar two the other datasets. I'd like to check if a part of the data should or should not be considered as training data.\n\nBest Regards", 
            "subreddit": "statistics", 
            "title": "Check similarity/consistancy between five training sets", 
            "url": "https://www.reddit.com/r/statistics/comments/726p4j/check_similarityconsistancy_between_five_training/"
        }, 
        {
            "author": "ThatsInappropriate", 
            "created_utc": 1506267128.0, 
            "domain": "self.statistics", 
            "id": "725o5t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/725o5t/help_comparing_three_sets_of_time_data/", 
            "score": 3, 
            "selftext": "I need some help considering the best statistical test to compare three sets of 24-hr data. What I am interested in looking at the number is occurrences of an event during each hour over a 24-hr span, for three different groups. I want to know if there is a significant difference between the hourly occurrences of the event among the three groups (and if possible, on an individual group basis e.g. group 3 v group 1 or group 1 v group 2). I tried a chi-squared contingency table but can't figure out why that won't work.\n\ngroup1 <- c('0'=2, '1'=3, '2'=1, '3'=2, '4'=3, '5'=0, '6'=2, '7'=1, '8'=1, '9'=0, '10'=1, '11'=0, '12'=1, '13'=0, '14'=0, '15'=1, '16'=0, '17'=1, '18'=1, '19'=1, '20'=1, '21'=1, '22'=4, '23'=3)\n\ngroup2 <- c('0'=2, '1'=0, '2'=2, '3'=5, '4'=3, '5'=1, '6'=2, '7'=3, '8'=1, '9'=0, '10'=4, '11'=0, '12'=0, '13'=1, '14'=2, '15'=0, '16'=0, '17'=2, '18'=3, '19'=0, '20'=3, '21'=2, '22'=1, '23'=2)\n\ngroup3 <- c('0'=1, '1'=0, '2'=5, '3'=5, '4'=5, '5'=3, '6'=2, '7'=2, '8'=0, '9'=1, '10'=0, '11'=0, '12'=0, '13'=0, '14'=0, '15'=0, '16'=0, '17'=0, '18'=1, '19'=3, '20'=1, '21'=2, '22'=1, '23'=0)\n\nAm I organizing the data correctly? Any advice on a good statistical test to use, or how to format the data? I am using R for analysis, if that helps.\n\nThanks!", 
            "subreddit": "statistics", 
            "title": "Help comparing three sets of time data", 
            "url": "https://www.reddit.com/r/statistics/comments/725o5t/help_comparing_three_sets_of_time_data/"
        }, 
        {
            "author": "KingTalis", 
            "created_utc": 1506246614.0, 
            "domain": "self.statistics", 
            "id": "7247x0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/7247x0/the_chance_of_rolling_the_same_number_twice_in_a/", 
            "score": 1, 
            "selftext": "Sorry if this is out of place, but it has come up multiple times recently between myself and friends about the probability of something happening twice in a row. In the actual discussion we have been having the chances are much slimmer than 1/6, but I figured I could just apply what I learn here to the bigger numbers.\n\n(I looked online for the formula, but was unable to find it.)\n\nThank you in advance.", 
            "subreddit": "statistics", 
            "title": "The chance of rolling the same number twice in a row on a six-sided die in 100 throws.", 
            "url": "https://www.reddit.com/r/statistics/comments/7247x0/the_chance_of_rolling_the_same_number_twice_in_a/"
        }, 
        {
            "author": "suttons27", 
            "created_utc": 1506232395.0, 
            "domain": "self.statistics", 
            "id": "723gk1", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 27, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/723gk1/choosing_the_correct_number/", 
            "score": 0, 
            "selftext": "Hi, had an issue that has been bothering me all week. At work I/we were analyzing a bottleneck step in our workflow.  We pulled the last 90 days of data (n=1200ish) and the results were mode = 14 minutes, median = 18 minutes, mean = 21 minutes, std. dev = 43 minutes and I think the 3rd quartile was 60 minutes. We are trying to set the baseline number to determine cost, allocation of resources, and any additional training. As the previous number was set at 8, several years prior. VP of Mfg wanted (mode) 14 minutes and CEO wanted (median) 18 minutes and both actually got into a pretty heated debate. They looked to me for an opinion and I suggested a split at 16 (but I don\u2019t make decisions, I just pull the data and put it in histogram form). Right now we are settled on 16 but you can sense that this will change. \n\nHow would a statistician resolve this?  Is there other formulas that could resolve the optimal number or at this point we just need to choose based on our own experience and knowledge the mean, median, or mode? Thanks in advance, this has literally bothered me all week but figured I would reach out to the experts to see how this should be handled.", 
            "subreddit": "statistics", 
            "title": "Choosing the correct number", 
            "url": "https://www.reddit.com/r/statistics/comments/723gk1/choosing_the_correct_number/"
        }, 
        {
            "author": "D-Juice", 
            "created_utc": 1506202525.0, 
            "domain": "fastcodesign.com", 
            "id": "7213kb", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/7213kb/this_font_makes_graphics_out_of_numbers_in_seconds/", 
            "score": 58, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "This Font Makes Graphics Out Of Numbers In Seconds", 
            "url": "https://www.fastcodesign.com/90139343/this-font-makes-graphics-out-of-numbers-in-seconds"
        }, 
        {
            "author": "life_xpantion_pack", 
            "created_utc": 1506195586.0, 
            "domain": "self.statistics", 
            "id": "720exy", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/720exy/does_multilevel_polynomial_regression_exist/", 
            "score": 9, 
            "selftext": "Hi all,\n\nI've heard of growth curve models, where change takes place over a function of time, I've also heard of polynomial regression and multilevel modelling.\n\nMy question is, can I use the same mathematical principals of growth curve but have my lowest level independent variable be something else? Also, can I have multiple first level variables in this set up?\n\nThanks in advance for your answers.", 
            "subreddit": "statistics", 
            "title": "Does multilevel polynomial regression exist?", 
            "url": "https://www.reddit.com/r/statistics/comments/720exy/does_multilevel_polynomial_regression_exist/"
        }, 
        {
            "author": "dancingsandwhich", 
            "created_utc": 1506188736.0, 
            "domain": "self.statistics", 
            "id": "71zq0r", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71zq0r/computer_helpstataspss/", 
            "score": 0, 
            "selftext": "Hello, I've been using STATA and SPSS on my laptop for about three years, but it recently died, and the people doing my tech support don't think that it can come back to life.\n\nI was just wondering what people use for a desktop setup? RAM and Processor specifically. \n\nSorry if this is not appropriate just not sure where to ask this!!!", 
            "subreddit": "statistics", 
            "title": "Computer Help-STATA/SPSS", 
            "url": "https://www.reddit.com/r/statistics/comments/71zq0r/computer_helpstataspss/"
        }, 
        {
            "author": "TheGoodinator", 
            "created_utc": 1506167532.0, 
            "domain": "self.statistics", 
            "id": "71xuh2", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71xuh2/im_a_recent_psychology_graduate_who_would_like_a/", 
            "score": 4, 
            "selftext": "I've just finished my MSc in social psychology and, in the future, would like to be a data scientist. As part of my psych degrees I had to learn statistics, and only then did I discover something I was good at and thoroughly enjoyed. The thing is that my stats training in my degrees aren't good enough qualifications for a data science job. A large part of me laments not doing a maths undergrad, but I was 18 and didn't even know what data science was.\n\nMy quantitative skills are strong, I have a knack for maths and coding/logic and am eager to learn. Is there anything you guys might recommend? I'm teaching myself python at the moment and have been desperately looking for jobs where I can use my stats skills (no success thus far) but apart from that, I'm not sure what to do.", 
            "subreddit": "statistics", 
            "title": "I'm a recent psychology graduate who would like a job in data science - asking for advice", 
            "url": "https://www.reddit.com/r/statistics/comments/71xuh2/im_a_recent_psychology_graduate_who_would_like_a/"
        }, 
        {
            "author": "bot_robot", 
            "created_utc": 1506151539.0, 
            "domain": "self.statistics", 
            "id": "71wya0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71wya0/why_24_out_of_24_is_mentioned_as_99_success_than/", 
            "score": 0, 
            "selftext": "Recently Scientists [had found a vaccine for HIV](https://www.youtube.com/watch?v=bBOO4lJonjI).\nIn the sample test/study, they tested it with 24 monkeys, and it worked fine for all 24 monkeys. But they are [reporting it](http://www.bbc.com/news/health-41351159) as 99%. Can you clarify/explain it ?", 
            "subreddit": "statistics", 
            "title": "Why 24 out of 24 is mentioned as 99% success than 100% in HIV study/test sampling?", 
            "url": "https://www.reddit.com/r/statistics/comments/71wya0/why_24_out_of_24_is_mentioned_as_99_success_than/"
        }, 
        {
            "author": "Xx420noskwopexX", 
            "created_utc": 1506149951.0, 
            "domain": "self.statistics", 
            "id": "71wuxm", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71wuxm/id_like_to_use_a_bs_degree_in_statistics_to_get/", 
            "score": 24, 
            "selftext": "I am going to be majoring in statistics. How can I use a degree in statistics to enter the basketball/baseball/sports industries, more specifically making analysis about players, how much they are worth etc.... ", 
            "subreddit": "statistics", 
            "title": "I'd like to use a BS degree in Statistics to get into the sports industry.", 
            "url": "https://www.reddit.com/r/statistics/comments/71wuxm/id_like_to_use_a_bs_degree_in_statistics_to_get/"
        }, 
        {
            "author": "RawCS", 
            "created_utc": 1506122812.0, 
            "domain": "self.statistics", 
            "id": "71ut9n", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71ut9n/which_undergrad_math_courses_help_the_most_with/", 
            "score": 4, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Which undergrad math courses help the most with preparing for a masters/PhD in statistics?", 
            "url": "https://www.reddit.com/r/statistics/comments/71ut9n/which_undergrad_math_courses_help_the_most_with/"
        }, 
        {
            "author": "omgouda", 
            "created_utc": 1506119898.0, 
            "domain": "self.statistics", 
            "id": "71ujkp", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71ujkp/does_anyone_know_how_to_bound_a_dependent/", 
            "score": 0, 
            "selftext": "Please help\n\nEDIT: I am not sure, the question was not clearly explained.\nI have a regression model - lm() - and i have four independents, one is a categorical midpoint (age), the other three are binary, (smoker, drinker, gender). The dependent variable is the distress level.\nThe question states: \"The dependent variable is bound at 0, run a summary lm(), how do the results change?\"", 
            "subreddit": "statistics", 
            "title": "Does anyone know how to \"bound\" a dependent variable in R?", 
            "url": "https://www.reddit.com/r/statistics/comments/71ujkp/does_anyone_know_how_to_bound_a_dependent/"
        }, 
        {
            "author": "Darkclokz", 
            "created_utc": 1506109875.0, 
            "domain": "self.statistics", 
            "id": "71ti63", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71ti63/statistics_on_infant_mortality_rate/", 
            "score": 0, 
            "selftext": "My first baby is due coming up in November and I keep hearing in parenthood circles people echoing sentiments they've seen in the media that America has a horribly high infant mortality rate compared to other wealthy developed countries. In looking at the collected data i'm seeing some strange metrics that people are touting as statistics but it doesn't seem like these data sets would normally pass for credible data. I would love it if someone who is slightly more versed in statistical theory could validate or reject my thoughts on it.\n\n\n\n1.) Infant mortality rate is calculated as number of infants that perish per thousand, among children under the age of one. The sample size of the United States is almost 4,000,000 vs the sample size of Japan being just over 1,000,000 or vs Luxembourg with a sample size of just over 6,000. I'm under the impression that you need equivalent sample sizes to begin to rule out anomalies and accurately predict trends and patterns.\n\n\n\n2.) Infant mortality rate is not calculated using the same metrics in many other countries that the United States is being compared to. This leads to an inconsistency in how the data is collected. Due to many varying ways the data is measured it makes it difficult to determine whether the numbers are comparable. For instance, in many countries we are compared with, babies born under 1lb or before 22 weeks of gestation are not considered to be part of the infant mortality rate statistics and are considered still birth instead. Also, in the United States babies at even 22 weeks of gestation are considered viable to try to save, where in many other countries we are compared with, they are not considered viable and the attempt to save isn't made further increasing our negative statistic.\n\n\n\n3.) Reporting of the infant mortality rate from other countries is collected differently, leading to inaccuracies in the data. Reporting data for many other countries is collected in different, possibly faulty ways than it is collected in the United States. Some countries we are compared to collect data after the fact, sometimes after a significant length of time by in home interview or voluntary data. It is to note, that this likely isn't the case with many countries like the UK, or France who similarly document infant mortality at the hospital where it occurred.\n\n\n\nAs an aside to the entire question, even with Mexico having one of the higher listed infant mortality rates at 13.2 deaths per 1000 infants under the age of one, this is still only 1.32% of all infants born will perish before the age of one which seems like a very low percentage. Expressed as the actual number it's about 31,000 per 2,353,000 babies born will perish before the age of one. This last paragraph is purely subjective, but one I felt like including.\n\nTL;DR The statistics surrounding the Infant mortality rate among countries seems to be flawed as a valid statistical analysis. Please validate or reject my theory.", 
            "subreddit": "statistics", 
            "title": "Statistics on infant mortality rate", 
            "url": "https://www.reddit.com/r/statistics/comments/71ti63/statistics_on_infant_mortality_rate/"
        }, 
        {
            "author": "hippopede", 
            "created_utc": 1506098775.0, 
            "domain": "self.statistics", 
            "id": "71s8r5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71s8r5/strange_probability_question/", 
            "score": 3, 
            "selftext": "I'm stuck with something in a side project I'm working on, but I think I can figure it out if I can solve the following simplified problem.\n\nI have a bunch of machines. Each machine contains a deck of 10 numbered cards which also have a color, red or green. Cards 1-4 are always green, and cards 5-10 are always red. The probability that each machine will dispense each card varies across machines. To use a machine, you press the button and it dispenses a card. If it's a green card, you get one point and must press again. If it's a red card, you lose a point and can no longer press the button. \n\nOnce a card has been dispensed from the machine, it cannot be dispensed again, but the relative probabilities among the remaining cards stay constant. I don't think this is relevant, but in the actual scenario the initial probabilities for the green cards will all be higher than those for the red cards.\n\nI want to know expected value of each machine. I'm not particularly experienced with probability problems so I'm not even sure how to begin modeling the stopping effect of a red card, in part because the impact of stopping depends largely on how many green cards you have already gotten. Thanks for any help!", 
            "subreddit": "statistics", 
            "title": "Strange Probability Question", 
            "url": "https://www.reddit.com/r/statistics/comments/71s8r5/strange_probability_question/"
        }, 
        {
            "author": "NewAccountingKid", 
            "created_utc": 1506097350.0, 
            "domain": "self.statistics", 
            "id": "71s2zg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71s2zg/should_i_take_a_summer_off_work_during_undergrad/", 
            "score": 0, 
            "selftext": "I'm a business major, but want to move more into a data analyst type role. However, I've found our university courses to really be lacking and think I'd be better off taking courses through Course era / Khan Academy or something like that.\n\nHow negatively would that period be viewed if I'm not working at an internship or anything?", 
            "subreddit": "statistics", 
            "title": "Should I take a summer off work during undergrad to learn SQL/some python?", 
            "url": "https://www.reddit.com/r/statistics/comments/71s2zg/should_i_take_a_summer_off_work_during_undergrad/"
        }, 
        {
            "author": "perignem", 
            "created_utc": 1506091416.0, 
            "domain": "self.statistics", 
            "id": "71rfcl", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71rfcl/good_online_course_for_advanced_undergraduate/", 
            "score": 1, 
            "selftext": "I just started my first semester of my Masters and am a few weeks into a graduate level course in ANOVA. It has been several years since my undergraduate data analysis courses and I am having a hard time keeping up. I am debating dropping the course and spending the semester reviewing the stuff I am supposed to already know. Is anyone able to recommend a good online course that perhaps does a quick run through of introductory concepts and then moves into the type of material you would find in an upper-level undergraduate social sciences data analysis course? Ideally it would be something geared towards psych (though this isn't critical) that I could complete by the end of December. Thanks!\n\n\n\nTL;DR I'm looking for an online upper level social sciences stats course.", 
            "subreddit": "statistics", 
            "title": "Good Online Course for \"Advanced\" Undergraduate Statistics (Psychology)", 
            "url": "https://www.reddit.com/r/statistics/comments/71rfcl/good_online_course_for_advanced_undergraduate/"
        }, 
        {
            "author": "ChesterEnergy", 
            "created_utc": 1506087150.0, 
            "domain": "fivethirtyeight.com", 
            "id": "71qzud", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 22, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71qzud/the_media_has_a_probability_problem/", 
            "score": 75, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "The Media Has A Probability Problem", 
            "url": "https://fivethirtyeight.com/features/the-media-has-a-probability-problem/"
        }, 
        {
            "author": "aaronfd24", 
            "created_utc": 1506060241.0, 
            "domain": "self.statistics", 
            "id": "71p4p4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71p4p4/specialization_in_college/", 
            "score": 2, 
            "selftext": "Hello! I am a freshman in college and I am considering pursuing a career in statistics. I was wondering what the best plan of attack is for specializing in agricultural statistics. In this scenario know I want to get a MS in Applied Statistics after my undergrad. Is it unwise to get my bachelors degree in an unrelated field (related to agriculture /bio) that I want to apply my later Masters degree to? I would take the required prerequisites for grad school in this scenario. Or on the other hand should I double up on statistics for both my undergrad and graduate degree? Thank you so much for any input.", 
            "subreddit": "statistics", 
            "title": "Specialization in College", 
            "url": "https://www.reddit.com/r/statistics/comments/71p4p4/specialization_in_college/"
        }, 
        {
            "author": "serenade4strings", 
            "created_utc": 1506037302.0, 
            "domain": "self.statistics", 
            "id": "71n6ud", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71n6ud/statistics_vs_biostatistics/", 
            "score": 21, 
            "selftext": "I'm looking to go back to school and start a new career after being a stay at home mom for too long and I would really like to be a biostatistician. I don't want to uproot my family in order to pursue grad school at a different school, but the local university does not have a biostatistics program (but they do have a MS in statistical computing). Does it matter? How much easier would it be to get a job in biostats if I had a biostats degree instead of a regular stats degree? \n\nAnother element to my question is I know I would do much better in a classroom setting, but if I *really* wanted to get a biostats degree and not move my family, there is an online program available. I much prefer to be in the classroom though. Besides my preference, I'm concerned about how people - especially the people that do the hiring - in the field of biostats feel about online degree graduates. Not long ago online degrees seemed to be viewed as something negative, but it seems to me this has been changing (and I also imagine it's dependent on the industry). I'm hoping some of you could provide me with some insight. \n\nTL;DR I want to be a biostatistician. My options are an ONLINE MS in Biostatistics or a traditional MS in statistical computing. Can I get easily hired as a biostatistician with either of these? Or is going for a traditional (as opposed to online) MS in biostats highly advised? ", 
            "subreddit": "statistics", 
            "title": "Statistics vs Biostatistics", 
            "url": "https://www.reddit.com/r/statistics/comments/71n6ud/statistics_vs_biostatistics/"
        }, 
        {
            "author": "AverageJoeCrew", 
            "created_utc": 1506023142.0, 
            "domain": "self.statistics", 
            "id": "71lowb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71lowb/how_to_study_statisticsprobability/", 
            "score": 9, 
            "selftext": "Hi Guys. \n\nStudent taking an intro probability and statistics class for engineers.\nWhile the class makes sense, the homework is quite daunting and I just can't seem to wrap my head around the wording of problem and the problem solving approach. I have tried reading our textbook but it seems to be at a much more advanced level than our class. \n\nCould you offer some statistics video lectures or textbooks that are a little more beginner friendly. I am struggling quite a bit and don't want to be forced to look up answers. I really want to understand what I am doing more, but I seem to hit a brick wall. \n\nI am most confused by conditional probability at the moment. \n\nThanks! ", 
            "subreddit": "statistics", 
            "title": "How to study statistics/probability", 
            "url": "https://www.reddit.com/r/statistics/comments/71lowb/how_to_study_statisticsprobability/"
        }, 
        {
            "author": "neuronet", 
            "created_utc": 1506023124.0, 
            "domain": "self.statistics", 
            "id": "71lot8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71lot8/is_there_an_analog_of_the_data_processing/", 
            "score": 1, 
            "selftext": "In information theory, the data processing inequality states:\n\n> if  X->Y->Z is a Markov chain, then I(X,Z) <= I(X,Y), where I is mutual information\n\nI'm wondering if there is an analogous theorem for performance (percent correct) of a minimum error classifier (discrete case). Let Px(Z) be the probability of correctly predicting which X produced Z, using a Bayesian (minimum error) classifier. Does the following hold:\n\n> If X->Y->Z is a Markov chain, then Px(Z) <= Py(Z)\n\nThis seems intuitively obvious, but I have not seen it proven.\n", 
            "subreddit": "statistics", 
            "title": "Is there an analog of the data processing inequality for minimum error (Bayes) classifiers?", 
            "url": "https://www.reddit.com/r/statistics/comments/71lot8/is_there_an_analog_of_the_data_processing/"
        }, 
        {
            "author": "wordboyhere", 
            "created_utc": 1506021211.0, 
            "domain": "self.statistics", 
            "id": "71lgzi", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71lgzi/is_there_anything_statisticians_cant_do/", 
            "score": 0, 
            "selftext": "Every college in my university has an intro stats class. The engineers need one, the bio majors need one, even the sociologists have one. \n\nEvery subject uses or has latched onto it. Criminology used to be hopelessly qualitative until Gary Becker introduced a rational choice framework, already used in economic models.  \n\nEvery team of scientists needs someone that, at the very least, understands how to run an ANOVA, and a peer reviewer in journals that can okay it. \n\nFinancial firms of all kinds are hooking up with big data analysis and machine learning, which are old statistical methods rehashed. \n\nStatisticians are kind of like the modern world's renaissance man? ", 
            "subreddit": "statistics", 
            "title": "Is there anything statisticians can't do?", 
            "url": "https://www.reddit.com/r/statistics/comments/71lgzi/is_there_anything_statisticians_cant_do/"
        }, 
        {
            "author": "Hellkyte", 
            "created_utc": 1506015314.0, 
            "domain": "self.statistics", 
            "id": "71krw2", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71krw2/betweenwithin_vs_overall_standard/", 
            "score": 1, 
            "selftext": "I'm working on a statistical quality control tool and realizing I need to be using a \"between/within\" (I-MR-R) chart.  One of the issues with that is that you use a few different forms of variance in it.  \n\nThe first is the \"within\" variance, so that would be the variance within a specific subgroup/batch of data.  Second you have the \"between\" variance, or the variance between each subgroup/batch.  Then you have the Between/Within variance, which is the combination of those two.\n\nFinally you have the \"Overall\" variance, and that's the one I am having some difficulty with.  It seems that often times the Overall variance is larger than the Between/Within variance.  Apparently this difference is supposed to be a sign that there is something wrong with your system (not in control). \n\nBut what I don't understand is what other variance there could be other than Between/within.  Or is it meant to be kind of like how an ANOVA is calculated, and saying that if the overall variance of the system can't be explained by the Between/Within variance then you have a non-random factor at play causing the difference seen? \n\nAny thoughts?\n\n\nEd:  also, I've run a Nested Anova on this data, the way it's structured it makes sense to me that the Between level is the top level, and the within level is the bottom/Nested level, but the numbers I'm getting are slightly different than expected (and there doesn't seem to be an \"overall\" version).  ", 
            "subreddit": "statistics", 
            "title": "Between/Within vs Overall (standard deviation/capability analysis)", 
            "url": "https://www.reddit.com/r/statistics/comments/71krw2/betweenwithin_vs_overall_standard/"
        }, 
        {
            "author": "YooooBroo", 
            "created_utc": 1506014073.0, 
            "domain": "self.statistics", 
            "id": "71kmhk", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71kmhk/confused_about_autocorrelation/", 
            "score": 1, 
            "selftext": "Hi Guys,\n\nI have a question regarding this [paper](http://people.stern.nyu.edu/churvich/Forecasting/Handouts/Chapt3.1.pdf). I'm confused with [this section](https://imgur.com/a/YEkPr). If there is a strong autocorrelation between the series Xt and Xt-j, then we would expect that the forecast for Xt would go up/down depending on Xt-j and on whether the autocorrelation was positive or negative. But if this is the case, how can the E[Xt] still be equal to 0? Was hoping someone could clear this up. Thanks guys. ", 
            "subreddit": "statistics", 
            "title": "Confused about Autocorrelation?", 
            "url": "https://www.reddit.com/r/statistics/comments/71kmhk/confused_about_autocorrelation/"
        }, 
        {
            "author": "manic_panic", 
            "created_utc": 1506013504.0, 
            "domain": "self.statistics", 
            "id": "71kk0j", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71kk0j/please_help_me_compare_these_two_logistic/", 
            "score": 1, 
            "selftext": "Hi All, thanks for any assistance! \n\nI have a binary outcome of interest, and I have 9 binary predictors - the predictors are 9 different events having occurred (or not). So I want to ask the question 'did the customer buy anything?', with the predictors of 1) whether or not they got a phone call, 2) a letter, 3) met with representative in a store, etc (clearly they could have had multiples of these events happen). In the first model, I entered all the predictors as class variables. In model 2 I summed their binary occurrence, and entered that summative predictor by itself (reflecting for example, that any person might have been exposed to 0-9 of the 9 things).\n\nI want to compare these models - not the coefficients but the model fit (I'm actually trying to answer the question of which method of treating the predictor is better). \n\nCan I use the chi-square distribution for the difference in -2LL? Some references say the models must be nested (I cant tell if these count as nested or not), while other references only say 'compare deviance between other models fitted in the same data', which these clearly are.\n\nThe models produce very large deviance statistics, and very large differences between them (-2LLmodel1 (-) -2LLmodel2 = 358565, with delta df = 10).\n\nIs this really indicating that the models are meaningfully different in terms of fit? The c-statistics do vary but only on order of about .02-.03.\n\n(I have a very large sample if that makes a difference (>25,000)).\n\nThanks for reading. ", 
            "subreddit": "statistics", 
            "title": "Please help me compare these two logistic regression models!", 
            "url": "https://www.reddit.com/r/statistics/comments/71kk0j/please_help_me_compare_these_two_logistic/"
        }, 
        {
            "author": "ChesterEnergy", 
            "created_utc": 1506007884.0, 
            "domain": "chesterenergyandpolicy.com", 
            "id": "71jwjn", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71jwjn/correlating_energy_data_sets_the_right_way_and/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Correlating Energy Data Sets: The Right Way and the Wrong Way", 
            "url": "http://chesterenergyandpolicy.com/2017/09/21/correlating-energy-data-sets-the-right-way-and-the-wrong-way/"
        }, 
        {
            "author": "necksnapper", 
            "created_utc": 1506002397.0, 
            "domain": "self.statistics", 
            "id": "71jart", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 24, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71jart/modelling_the_probability_that_a_cancer_patient/", 
            "score": 5, 
            "selftext": "Hi everyone,\n\nI am trying to model a phenomenon.  It isnt actually about cancer patient, but this is the best analogy I can come up with.\n\n~~I am wondering what would be the best way to model if a cancer patient will survive a cancer given a set of variables, such as time since beginning of treatment and other characteristics of the patient. ~~\n\nI don't care about the duration of the treatment, only if the persons will  beat the cancer or die.\n\nI have multiple observations for each patient: one per year, until they either die or beat cancer.\n\nShould I just do a logistic regression, or should I do some survival analysis ?   If I do a logistic regression, how would you treat the fact that patients are there multiple observations per patients?   If survival analysis, can you point me in the right direction?\n\nWhen predicting, I will want to take into account if a patient has been treated for 1 year or 10.\n\nbest,\n\n\n\n-----\nedit: \nI'm afraid cancer patient was a bad example .   Let's try this instead:\n\nI have a technical ticket open.   It's a \"minor case\".  I want to know how likely we will be able to close that as a \"minor case\",vs seeing it degenerate as a \"major case\" before we can close it.  \n\nAgain, a case can last for multiple time periods, and will have observations for each time periods where y will be one of these :\n\n- censored (open, currently estimated as either minor or major)\n- 1 (closed as major)\n- 0 (closed as minor)\n\n\n\nticket id|date|status|outcome (estimate if not closed)|month since start|x1|x2|\n:--|:--|:--|:--|:--|:--|:--|\nAA|jan1|open|minor|0|1|1|\nAA|feb1|open|major|1|1|2|\nAA|mar1|closed|major|2|1|2|\nBB|jan1|open|minor|0|2|2|\nBB|feb1|closed|minor|1|2|2|\n\nSo my actual data set will look like this.  The point is that today I have a data set with a bunch of tickets and I want to predict the most likely outcome (status when closed).  \n", 
            "subreddit": "statistics", 
            "title": "Modelling the probability that a cancer patient will survive vs die from cancer", 
            "url": "https://www.reddit.com/r/statistics/comments/71jart/modelling_the_probability_that_a_cancer_patient/"
        }, 
        {
            "author": "ChesterEnergy", 
            "created_utc": 1506000962.0, 
            "domain": "medium.com", 
            "id": "71j5jt", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 20, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71j5jt/the_ten_fallacies_of_data_science_towards_data/", 
            "score": 33, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "The Ten Fallacies of Data Science - Towards Data Science - Medium", 
            "url": "https://medium.com/towards-data-science/the-ten-fallacies-of-data-science-9b2af78a1862"
        }, 
        {
            "author": "chubchubpenguin", 
            "created_utc": 1505966002.0, 
            "domain": "self.statistics", 
            "id": "71gmtu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71gmtu/question_how_to_balance_scores_from_low_sample_of/", 
            "score": 5, 
            "selftext": "I'm trying to compare boardgame scores between people.\n\nSome people played a lot of games; some played very few. For the \"few games played\" people, their average score is essentially their actual score from playing the game once. Is there a way to compare the scores of the many played games vs. the few played games? I can of course say, I'm only comparing the scores of people who've played 5 times but the number 5 is ultimately arbitrary? What's the none-arbitrary number that would work as the cutoff? Is there way for me to use the data in a comparison of the few games played people?", 
            "subreddit": "statistics", 
            "title": "Question: How to \"Balance\" Scores from Low Sample of People [Read for more info]", 
            "url": "https://www.reddit.com/r/statistics/comments/71gmtu/question_how_to_balance_scores_from_low_sample_of/"
        }, 
        {
            "author": "Masterz4099", 
            "created_utc": 1505964021.0, 
            "domain": "self.statistics", 
            "id": "71ggzf", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71ggzf/difference_between_zscore_and_empirical_rule/", 
            "score": 4, 
            "selftext": "The empirical rule says that 1 standard deviation is equal to 68% of the data. But if you have a z-score of 1, it is not equal to 68%, but 84.13%. Why do they have different values if they both equal to 1 standard deviation?", 
            "subreddit": "statistics", 
            "title": "Difference between z-score and empirical rule", 
            "url": "https://www.reddit.com/r/statistics/comments/71ggzf/difference_between_zscore_and_empirical_rule/"
        }, 
        {
            "author": "sudeepraja", 
            "created_utc": 1505950837.0, 
            "domain": "sudeepraja.github.io", 
            "id": "71f82i", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71f82i/a_family_of_efronstein_like_lower_bounds_for/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "A Family of Efron-Stein Like Lower bounds for Variance", 
            "url": "https://sudeepraja.github.io/EfronStein2/"
        }, 
        {
            "author": "ThisisStatisticsASA", 
            "created_utc": 1505938038.0, 
            "domain": "thisisstatistics.org", 
            "id": "71dsp6", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71dsp6/announcing_the_firstever_police_data_challenge/", 
            "score": 27, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Announcing the First-Ever Police Data Challenge", 
            "url": "http://thisisstatistics.org/announcing-the-police-data-challenge/"
        }, 
        {
            "author": "yesman89", 
            "created_utc": 1505937583.0, 
            "domain": "self.statistics", 
            "id": "71dqq8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71dqq8/question_about_observational_studies/", 
            "score": 1, 
            "selftext": "Hello everyone, I am a undergraduate student and in one of my class we are asked to do statistical analysis on an observational study to find if smokers and non smokers have higher chance respiratory viral infections in a certain demographic. I have no idea how to do one and was never taught how to do one and wanted to ask if a someone here could help with this predicament. I personally am not collecting the data but I have to guide another group on what kind of data to collect. My question is what kind of different observational studies are there that would work with my study, and what requirements with the data are to be met to have a do a good analysis on the data? Also any other general tips when conducting an observational study?", 
            "subreddit": "statistics", 
            "title": "Question about observational studies", 
            "url": "https://www.reddit.com/r/statistics/comments/71dqq8/question_about_observational_studies/"
        }, 
        {
            "author": "omgouda", 
            "created_utc": 1505927989.0, 
            "domain": "self.statistics", 
            "id": "71cll7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71cll7/question_about_categorizing_data_for_regression/", 
            "score": 3, 
            "selftext": "So im a beginner here, student, trying my best to learn but i'm stumped on what to do here and im sure its a simple fix:\n\nI have some data on whether an individual smokes or not, where 1 - yes, 2 - no, 3 - don't know, 4 - did not respond.\n\nI want to regress distress level against smokers, so i only care if you smoke or not. \n\nI thought i could just put 0 for all answers that are not yes (1) but apparently this is wrong.\n\nWhat would you do?", 
            "subreddit": "statistics", 
            "title": "Question about categorizing data for regression", 
            "url": "https://www.reddit.com/r/statistics/comments/71cll7/question_about_categorizing_data_for_regression/"
        }, 
        {
            "author": "GeorgeBrettLawrie", 
            "created_utc": 1505923641.0, 
            "domain": "self.statistics", 
            "id": "71c3ai", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71c3ai/comparing_two_scatter_plots/", 
            "score": 1, 
            "selftext": "Data was collected at multiple time points for two sets of subjects. From the data, I've made two scatters plots of, let's say, subject health vs. time.\n\nIt's fairly trivial to calculate if there is a significant difference between each (or any) time point using multiple t-tests. However, is there a way to test for a significant different between the plots as a whole? I can't seem to find an answer but I may be asking the wrong questions.\n\np.s. Apologies if this should be posted elsewhere. Just let me know. It's work-work not homework, if that makes a lick of difference.", 
            "subreddit": "statistics", 
            "title": "Comparing two scatter plots", 
            "url": "https://www.reddit.com/r/statistics/comments/71c3ai/comparing_two_scatter_plots/"
        }, 
        {
            "author": "sudeepraja", 
            "created_utc": 1505900102.0, 
            "domain": "sudeepraja.github.io", 
            "id": "719xy7", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/719xy7/an_efronstein_like_lower_bound_for_variance_is/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "An Efron-Stein Like Lower bound for Variance. Is this new?", 
            "url": "https://sudeepraja.github.io/EfronStein/"
        }, 
        {
            "author": "K4k4shi", 
            "created_utc": 1505897091.0, 
            "domain": "self.statistics", 
            "id": "719rhg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/719rhg/have_date_and_time_on_the_same_axis_in_r_studio/", 
            "score": 1, 
            "selftext": "This is a moisture data. I want to have date and time on X axis and moisture on Y axis. Thanks\nExample data\n\n    Date\t           W5\t        W7\n    6/24/2017 12:00 AM\t0.333\t0.326\n    6/24/2017 12:30 AM\t0.333\t0.332\n    6/24/2017 1:00 AM\t0.334\t0.351\n    6/24/2017 1:30 AM\t0.334\t0.351\n    6/24/2017 2:00 AM\t0.334\t0.352\n    6/24/2017 2:30 AM\t0.334\t0.352\n    6/24/2017 3:00 AM\t0.335\t0.352\n    6/24/2017 3:30 AM\t0.335\t0.351\n    6/24/2017 4:00 AM\t0.335\t0.351\n    6/24/2017 4:30 AM\t0.335\t0.351\n    6/24/2017 5:00 AM\t0.336\t0.350\n    6/24/2017 5:30 AM\t0.336\t0.350\n    6/24/2017 6:00 AM\t0.336\t0.349\n    6/24/2017 6:30 AM\t0.336\t0.349\n    6/24/2017 7:00 AM\t0.336\t0.348\n    6/24/2017 7:30 AM\t0.336\t0.348\n    6/24/2017 8:00 AM\t0.336\t0.348\n    6/24/2017 8:30 AM\t0.336\t0.347\n    6/24/2017 9:00 AM\t0.336\t0.347\n\n\nThe script I am using right now just for date\n\n    ggplot(data, aes(x = Date))+ geom_line(aes(y = W5, colour = \"W5\"))+ geom_line(aes(y = W7, colour = \"W7\"))\n", 
            "subreddit": "statistics", 
            "title": "Have Date and time on the same axis in R studio.", 
            "url": "https://www.reddit.com/r/statistics/comments/719rhg/have_date_and_time_on_the_same_axis_in_r_studio/"
        }, 
        {
            "author": "__compactsupport__", 
            "created_utc": 1505896022.0, 
            "domain": "self.statistics", 
            "id": "719p7y", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/719p7y/likelihood_ratio_test_vs_coefficient_significance/", 
            "score": 2, 
            "selftext": "\n\t\n\nI ran a regression on some data I collected. The data has one independent variable and looks as if a quadratic function would fit it well.\n\nI fit both a quadratic and a quartic model to the data and performed a likelihood ratio test. Seems that I should prefer the quartic model (p<0.001).\n\nHowever, the coefficients of my quartic model are not statistically significant.\n\nShould I prefer the quadratic model because it's coefficients are statistically significant, or prefer the quartic model from the results of the liklihood ratio test?\n", 
            "subreddit": "statistics", 
            "title": "Likelihood Ratio Test vs Coefficient Significance", 
            "url": "https://www.reddit.com/r/statistics/comments/719p7y/likelihood_ratio_test_vs_coefficient_significance/"
        }, 
        {
            "author": "jonathon481", 
            "created_utc": 1505880554.0, 
            "domain": "self.statistics", 
            "id": "718mjx", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/718mjx/significance_in_a_randomized_block_designs/", 
            "score": 0, 
            "selftext": "I am currently testing whether seasons has an significant effect on motor-vehicle fatalities within Australia from 1989 to 2016. I have conducted an ANOVA and found a significant result (p = 0.0469) but when conducting a post-hoc Tukey's test I found no significant difference between any of the seasons. What can I conclude? ", 
            "subreddit": "statistics", 
            "title": "Significance in a Randomized Block Designs", 
            "url": "https://www.reddit.com/r/statistics/comments/718mjx/significance_in_a_randomized_block_designs/"
        }, 
        {
            "author": "clamman", 
            "created_utc": 1505871062.0, 
            "domain": "self.statistics", 
            "id": "717rke", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/717rke/how_do_you_analyze_the_ratio_of_two_time_series/", 
            "score": 7, 
            "selftext": "Say you had two time series variables X and Y, and you were interested in performing a time series analysis for X / Y. For example Y = Number of customers at a store in a day and X = Number of units of a product sold in a day, and you were interested in X / Y = rate of units sold per customer per day. What would be some good approaches for constructing a time series model on the rate? The purpose of this model would be to make predictions of the rate/anomaly detection.\n\nSome ideas I had:\n\n* Naively construct an AR model on the rates directly.\n* Construct an AR model on X and Y jointly. Then predict X and Y and use those predictions to construct a prediction for X / Y. Then, I can use the estimated covariance matrix in the model to estimate CoV(X / Y) to do anomaly detection.\n\nI feel like the the first approach is probably the incorrect approach and that the second one is the better one, but I'm not convinced it's entirely principled. Also, I would love to hear about other suggestions. Further, how would this analysis change if we knew that X / Y was in the interval [0, 1]? Conceivably, you could construct a model according to method 2 above where we predict X > Y. Thanks in advance.", 
            "subreddit": "statistics", 
            "title": "How do you Analyze the Ratio of Two Time Series Variables?", 
            "url": "https://www.reddit.com/r/statistics/comments/717rke/how_do_you_analyze_the_ratio_of_two_time_series/"
        }, 
        {
            "author": "wordboyhere", 
            "created_utc": 1505856582.0, 
            "domain": "gis.washington.edu", 
            "id": "716aqh", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/716aqh/basus_elephants_pdf/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Basu's Elephants [PDF]", 
            "url": "http://gis.washington.edu/phurvitz/phd/courses/2008/01_winter/csss529/basu.pdf"
        }, 
        {
            "author": "AnomalyXIII", 
            "created_utc": 1505855065.0, 
            "domain": "self.statistics", 
            "id": "7164j5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/7164j5/what_standard_deviations_equate_to_sample/", 
            "score": 3, 
            "selftext": "I am trying to build a cost history report for my job, breaking down each bid we receive and using standard deviation to determine any tasks/projects that are significant. It's been a LONG time since statistics, so I'm trying to remember how do I determine what SD value equals a sample %'age?\n\nIf I'm not explaining myself correctly, I remember the 68-95-99.7 rule, but that's just too broad of a range for what I'm trying to accomplish. I am looking for something more like 10-30-45-60. Is there a calculator somewhere for this? Every Google search is not helping me!", 
            "subreddit": "statistics", 
            "title": "What Standard Deviations Equate to Sample %?", 
            "url": "https://www.reddit.com/r/statistics/comments/7164j5/what_standard_deviations_equate_to_sample/"
        }, 
        {
            "author": "tcln1456", 
            "created_utc": 1505843260.0, 
            "domain": "self.statistics", 
            "id": "714pr5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/714pr5/statistical_test_for_nonordered_categories/", 
            "score": 5, 
            "selftext": "I have a dataset with messages sent among 3 groups. Each message is assigned one or more categories based on the content.\n\nIs there a test for significance between what % of group A's messages were labeled category 1 vs. what % of group B's messages were labeled category 1? There is no expected distribution of categories for each group.\n\nHere is an example:\n\n  | Cat1 | Cat2 | Cat3 | Cat4 | Cat5\n---------|----------|----------|----------|----------|----------\nHigh Schoolers | 14.8% | 9.2%|6.6% |26.2% |7.4%\nCollege students | 24.2%| 11.4%|10.6%|15.2%|12.9%|\nPost-grad | 23.2%| 4.5%|2.3%|20.3%|7.9%|\n\nIn this case, I want to see if there is a significant difference between high schooler (14.8%) and Post-grad (23.2%) messages that were categorized as Cat1.", 
            "subreddit": "statistics", 
            "title": "Statistical test for non-ordered categories?", 
            "url": "https://www.reddit.com/r/statistics/comments/714pr5/statistical_test_for_nonordered_categories/"
        }, 
        {
            "author": "moomoomeow2", 
            "created_utc": 1505834852.0, 
            "domain": "self.statistics", 
            "id": "713pui", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/713pui/probability_help/", 
            "score": 4, 
            "selftext": "Are there any good resources online to better understand probability? I'm currently stuck on the \"choosing\" nonsense.\nFor example: you draw five cards without replacement. What are the odds of getting two kings, a joker, and an ace?\nThat's just an example, but problems like that are my bane. Are they any suggestions on how to better understand them?", 
            "subreddit": "statistics", 
            "title": "Probability Help", 
            "url": "https://www.reddit.com/r/statistics/comments/713pui/probability_help/"
        }, 
        {
            "author": "mynameisfoong", 
            "created_utc": 1505834300.0, 
            "domain": "self.statistics", 
            "id": "713nl6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/713nl6/how_do_i_check_whether_a_data_set_is_a_paired/", 
            "score": 6, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "How do I check whether a data set is a paired data or independent samples", 
            "url": "https://www.reddit.com/r/statistics/comments/713nl6/how_do_i_check_whether_a_data_set_is_a_paired/"
        }, 
        {
            "author": "fizzpop4u", 
            "created_utc": 1505816333.0, 
            "domain": "treasury.govt.nz", 
            "id": "7120s4", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/7120s4/is_this_graph_statistically_correct_surely_it/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Is this graph Statistically correct? Surely it must be questionable!?", 
            "url": "http://www.treasury.govt.nz/budget/forecasts/prefu2017/005.htm/prefu17-01.gif"
        }, 
        {
            "author": "davecarpenter", 
            "created_utc": 1505813366.0, 
            "domain": "self.statistics", 
            "id": "711tv2", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/711tv2/has_anybody_ever_tried_answerminer_do_you_have/", 
            "score": 1, 
            "selftext": "I've just come across this data analyzer tool and want to know that has anybody tried it and what is her/his opinion.\n\nhttps://app.answerminer.com/auth/login", 
            "subreddit": "statistics", 
            "title": "Has anybody ever tried Answerminer? Do you have any experience with it?", 
            "url": "https://www.reddit.com/r/statistics/comments/711tv2/has_anybody_ever_tried_answerminer_do_you_have/"
        }, 
        {
            "author": "nelechini", 
            "created_utc": 1505812006.0, 
            "domain": "self.statistics", 
            "id": "711qug", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/711qug/do_u_prefer_r_or_stata/", 
            "score": 0, 
            "selftext": "I have never used stata, I use R and MatLab, anyone of you do u use stata? Do u think it is worthy than R or Matlab?", 
            "subreddit": "statistics", 
            "title": "Do u prefer R or stata?", 
            "url": "https://www.reddit.com/r/statistics/comments/711qug/do_u_prefer_r_or_stata/"
        }, 
        {
            "author": "anthraxwar", 
            "created_utc": 1505793987.0, 
            "domain": "self.statistics", 
            "id": "710ie4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/710ie4/efx_fex/", 
            "score": 1, 
            "selftext": "does this hold?", 
            "subreddit": "statistics", 
            "title": "E[f(x)] =?= f(E[x])", 
            "url": "https://www.reddit.com/r/statistics/comments/710ie4/efx_fex/"
        }, 
        {
            "author": "Zugzwangpoe", 
            "created_utc": 1505785899.0, 
            "domain": "self.statistics", 
            "id": "70zs5l", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70zs5l/what_test_can_you_use_to_see_if_your_new_results/", 
            "score": 2, 
            "selftext": "As per the title if you have new results come in how can you tell if they invalidate the ex ante distribution either by size/ magnitude of the new inputs or by frequency? ", 
            "subreddit": "statistics", 
            "title": "What test can you use to see if your new results invalidate your previous expectation for the distribution?", 
            "url": "https://www.reddit.com/r/statistics/comments/70zs5l/what_test_can_you_use_to_see_if_your_new_results/"
        }, 
        {
            "author": "smokiesmokesmoke", 
            "created_utc": 1505781234.0, 
            "domain": "blog.almighty.press", 
            "id": "70zblg", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70zblg/how_to_create_viral_content_with_studies_in/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "How to Create Viral Content with Studies in Neuroscience and Past Data", 
            "url": "http://blog.almighty.press/how-to-create-viral-content-with-studies-in-neuroscience-and-past-data/"
        }, 
        {
            "author": "blockhead123", 
            "created_utc": 1505772659.0, 
            "domain": "self.statistics", 
            "id": "70ygad", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70ygad/a_series_of_paired_ttests/", 
            "score": 2, 
            "selftext": "**Straightforward question:**\n\nWe collected treatment and control data on 4 measures for each participant, and plan on running four paired t-test on them. We needed a minimum sample of n=40, but ended up with n=50 for some the measures due to participants not completing all 4 measures. \n\nSo measure X has 40 pairs, measure Y has 50 pairs, and the measurements are coming from the same sample. For some reason, past RAs removed all data from Y when the participant didn't complete X, making the sample size 40 for all measures. \n\nIs there a reason one would do this? Obviously you need even treatment/control data for any given measure to run a paired t-test, but why would you throw out data for one measure if it was missing on the other? \n\n**Bonus:** \n\nThe code I was provided includes two test for each measure - a standard paired t-test, and a two one-sided test (TOST) for equivalence. Usually, a positive TOST (Alternative hypothesis == equivalence) is associated in a failure to reject the null in the standard t-test, and a rejected null on the standard t-test is associated with failure to reject H0 in the TOST - but not always. Past reports have deemed results to be inconclusive if they don't match. It seems questionable to me to test similar hypotheses using two different tests and label the results inconclusive if they don't match, but I can't really find anything on the matter. Has anyone encountered this before?", 
            "subreddit": "statistics", 
            "title": "A series of paired t-tests...", 
            "url": "https://www.reddit.com/r/statistics/comments/70ygad/a_series_of_paired_ttests/"
        }, 
        {
            "author": "mecolema", 
            "created_utc": 1505771194.0, 
            "domain": "self.statistics", 
            "id": "70yaja", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70yaja/question_about_risk_ratios/", 
            "score": 3, 
            "selftext": "When using a log-binomial regression model, how should I interpret the risk ratio (aka relative risk)?\n\nExample:\nSay I have a continuous predictor (number of kids), and a binary outcome (bankruptcy). I use a log-binomial regression model. If the risk ratio for \"number of kids\" is 1.5, I can say that someone with an extra kid has a 50% increased risk compared to someone without an extra kid. \n\nBut what if someone has three extra kids? Would the relative risk be 1.5^3, just like it would if we were working with odds ratios? ", 
            "subreddit": "statistics", 
            "title": "Question about risk ratios", 
            "url": "https://www.reddit.com/r/statistics/comments/70yaja/question_about_risk_ratios/"
        }, 
        {
            "author": "indigo_mints", 
            "created_utc": 1505767890.0, 
            "domain": "self.statistics", 
            "id": "70xx2t", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70xx2t/a_day_in_the_life_of_a_statistical_geneticist/", 
            "score": 25, 
            "selftext": "What kinds of projects do you get to work on? And what were your qualifications (eg. I heard it's necessary to have a Masters and maybe a PhD)?", 
            "subreddit": "statistics", 
            "title": "A day in the life of a statistical geneticist?", 
            "url": "https://www.reddit.com/r/statistics/comments/70xx2t/a_day_in_the_life_of_a_statistical_geneticist/"
        }, 
        {
            "author": "IM_BOAT", 
            "created_utc": 1505766022.0, 
            "domain": "self.statistics", 
            "id": "70xpdl", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70xpdl/help_probability_theory_proof/", 
            "score": 1, 
            "selftext": "Let E(n) and F(n) be increasing sequences of events. \n\ni.e. E(1) is a subset of E(2), E(2) is a subset of E(3) and so on.\n\nWhere E(n) --> E \n\nand F(n) --> F, as n --> inf\n\nAdditionally, let E(k) be independent of F(k) for all k = 1, 2, 3 ... \n\nShow that E is independent of F.\n\n---------------------------------------------------------------------\n\nSo correct me if I'm wrong. We're basically given \n\nP(E(1) intersect F(1)) = P(E(1))*P(F(1) \n\n....\n\nP(E(n) intersect F(n)) = P(E(n))*P(F(n))\n\n\n....\n\nand we have to show that P(E intersect F) = P(E)*P(F)\n\nFrom P(E(n) intersect F(n)) = P(E(n))*P(F(n)), if we take the limit as n --> inf then the right hand side is satisfied.\n\nBut how can I show E(n) intersect F(n) converges to E intersect F? \n\nThanks in advanced guys. If this is the wrong place to post can someone direct me to a more appropriate site for questions? \n\n", 
            "subreddit": "statistics", 
            "title": "[Help] Probability Theory proof.", 
            "url": "https://www.reddit.com/r/statistics/comments/70xpdl/help_probability_theory_proof/"
        }, 
        {
            "author": "stml", 
            "created_utc": 1505764248.0, 
            "domain": "self.statistics", 
            "id": "70xhye", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70xhye/debate_on_reddit_over_the_chances_of_drawing_the/", 
            "score": 6, 
            "selftext": "Here's the thread: https://www.reddit.com/r/todayilearned/comments/70un9r/til_that_in_2010_israels_weekly_state_lottery/\n\nAnybody wish to weigh in on this? ", 
            "subreddit": "statistics", 
            "title": "Debate on Reddit over the chances of drawing the same lottery numbers twice.", 
            "url": "https://www.reddit.com/r/statistics/comments/70xhye/debate_on_reddit_over_the_chances_of_drawing_the/"
        }, 
        {
            "author": "KaesekopfNW", 
            "created_utc": 1505763934.0, 
            "domain": "self.statistics", 
            "id": "70xgks", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70xgks/why_is_spss_slowing_down_so_much_after_deleting/", 
            "score": 0, 
            "selftext": "So I've got a huge dataset I'm working with on SPSS. It's got around 470,000 cases and 670 variables and sits at about 14 GB. I've noticed with this dataset that if I open it up and sort a variable, that variable sorts appropriately (albeit slowly). But if open up the dataset and first delete several variables (in this case, I deleted all but 11 of them) and then try sort the same variable, the process is excruciatingly slow. I also can't save, since that takes just as long. \n\nDoes anyone know what's causing this? I would think the file would be much smaller after deleting 600-some variables, thereby making it run faster when sorting variables or saving, but the opposite is occurring. At this point, I'm stuck on this dataset until this issue is resolved, because the rate at which this crawls is unmanageable.\n\nOh, and if it does take too long, I end up getting an error in SPSS about the processor being unavailable. The weird thing is that I was able to work in this dataset before this week, and now I can't, no matter what computer I use.\n\nAny help is greatly appreciated! Thanks!", 
            "subreddit": "statistics", 
            "title": "Why is SPSS slowing down so much after deleting variables?", 
            "url": "https://www.reddit.com/r/statistics/comments/70xgks/why_is_spss_slowing_down_so_much_after_deleting/"
        }, 
        {
            "author": "cangri1788", 
            "created_utc": 1505759188.0, 
            "domain": "self.statistics", 
            "id": "70wwz0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70wwz0/how_to_find_quartile_locations_distances_on_excel/", 
            "score": 1, 
            "selftext": "I need to find the location (distance) of quartiles using data on a table i have on Excel.  ", 
            "subreddit": "statistics", 
            "title": "How to find quartile locations (distances) on Excel?", 
            "url": "https://www.reddit.com/r/statistics/comments/70wwz0/how_to_find_quartile_locations_distances_on_excel/"
        }, 
        {
            "author": "bluecancan", 
            "created_utc": 1505749453.0, 
            "domain": "self.statistics", 
            "id": "70vu7m", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70vu7m/which_test_do_i_use/", 
            "score": 2, 
            "selftext": "Hi, I have four different samples of human cells which were treated with different things. For each sample we had to record how many of the cells had a specific abnormality. Each group analysed a different number of cells so I've recorded how often the abnormality was seen as a percentage. What statistical test could I do to compare these percentages to show if any of the treatments caused more abnormalities? Thanks :) ", 
            "subreddit": "statistics", 
            "title": "Which test do I use?", 
            "url": "https://www.reddit.com/r/statistics/comments/70vu7m/which_test_do_i_use/"
        }, 
        {
            "author": "davecarpenter", 
            "created_utc": 1505742639.0, 
            "domain": "self.statistics", 
            "id": "70v41c", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70v41c/whats_the_opposite_of_aggregated_data/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "What's the opposite of aggregated data? Non-aggregated?", 
            "url": "https://www.reddit.com/r/statistics/comments/70v41c/whats_the_opposite_of_aggregated_data/"
        }, 
        {
            "author": "squeeky602", 
            "created_utc": 1505733923.0, 
            "domain": "self.statistics", 
            "id": "70ucv6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70ucv6/pspp_help/", 
            "score": 7, 
            "selftext": "evening fellas, i'm just trying to figure out how i make pspp display my means and standard deviation into 3 decimal places. i'm really getting frustrated by this, any help would be really appreciated.", 
            "subreddit": "statistics", 
            "title": "Pspp help", 
            "url": "https://www.reddit.com/r/statistics/comments/70ucv6/pspp_help/"
        }, 
        {
            "author": "ifasongcouldgetmeyou", 
            "created_utc": 1505728502.0, 
            "domain": "self.statistics", 
            "id": "70tzff", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70tzff/which_test_to_use/", 
            "score": 10, 
            "selftext": "I had never a statistics course and i want to do it right. So i hope someone can help me and explain it to me, so I can learn. \n\nI need to do a test, I have 6 different conditions (n=5). I have to find out if there is an difference between these condtitions. Someone suggested to an ANOVA combined with Scheffe or Bonferonni. I would like to know why these? and what is the difference and what is the best to do?\n\nI hope someone can help me", 
            "subreddit": "statistics", 
            "title": "Which test to use?", 
            "url": "https://www.reddit.com/r/statistics/comments/70tzff/which_test_to_use/"
        }, 
        {
            "author": "NeuroBill", 
            "created_utc": 1505714532.0, 
            "domain": "self.statistics", 
            "id": "70t4h8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70t4h8/any_ideas_on_how_to_cluster_this_data/", 
            "score": 2, 
            "selftext": "Hi,\n\nI'm a neuroscientist. I've recorded time-series data, which is essentially a video of the activity of neurons. The data looks a awful lot like [this](https://www.youtube.com/watch?v=vqGPp4d3LGw). Each bright \"blob\" is one brain cell.\n\nI want to know how the \"activity\" (which is essentially a brightness value which is a 16 bit integer) varies over time. When I start the data is a big R^3 matrix, which is 512 (pixels) wide, 512 (pixels) high and as many frames as a captured deep (something like 10000).\n\nSoftware has already already automatically created \"Regions of interest\" (ROIs)  which are arbitrary bounding boxes placed around some of the pixels, and the average value of all those pixels in each frame is calculated, so I am left with a much smaller R^2 matrix, that is now <number of ROIs> by <number of frames>.\n\nThe problem is that the software sometimes splits the pixels that belong to one cell into two, three or even more ROIs. The data for each of these ROIs usually matches pretty well (real data [Here](https://i.imgur.com/B1nWZsf.png) each colour is a different ROI belonging to the same cell). However, it doesn't always match that perfectly.\n\nWhat I have been doing is correlating all the data from every ROI with every other ROI and calculating a Pearson correlation coefficient. Then I arbitrarily set a threshold and say that all ROIs with a correlation coefficient above X (usually something like 0.7) belong to the same cell.\n\nMY PROBLEM: This arbitrary correlation coefficient threshold is pretty gross. If I bump it from 0.7 to 0.71, sometimes I get different data, and because there are often 100s of ROIs it is very hard to go through manually and cluster by hand (cluster, as in, say that ROIs A, B and C all belong to the same cell). Is there a smarter way of doing this? I'm trying to imagine some cost function that I am trying to minimize, but most of the cost functions I can come up with are minimized when each ROI is treated as it's own cell (because I'm thinking the data for each group of ROIs is well clustered if each ROI does not deviate much from the mean of all the ROIs. And you achieve that perfectly if each ROI is in its own cluster). I suppose I should mention that sometimes each ROI IS it's own cluster (i.e. the software originally drew the ROI perfectly). I know there is lots of data published on 'clustering' but this isn't really the same thing. How can I more intelligently look at the data and say \"these arrays of numbers are very similar, and probably belong together\"\n\nAny ideas appreciated.", 
            "subreddit": "statistics", 
            "title": "Any ideas on how to \"cluster\" this data? Neuroscientist wants to go beyond an arbitrary R^2 threshold.", 
            "url": "https://www.reddit.com/r/statistics/comments/70t4h8/any_ideas_on_how_to_cluster_this_data/"
        }, 
        {
            "author": "Broship_Rajor", 
            "created_utc": 1505713085.0, 
            "domain": "self.statistics", 
            "id": "70t0sv", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70t0sv/what_should_be_expected_trying_to_find_a_job_in/", 
            "score": 15, 
            "selftext": "Is a BS in mathematical statistics enough to open some opportunities or is an MS needed.\n\nI will be graduating this spring with BS in both Mathematical Statistics and Physics and am currently working on my programming skills.", 
            "subreddit": "statistics", 
            "title": "What should be expected trying to find a job in stats with a Mathematical Statistics BS?", 
            "url": "https://www.reddit.com/r/statistics/comments/70t0sv/what_should_be_expected_trying_to_find_a_job_in/"
        }, 
        {
            "author": "hevilla14", 
            "created_utc": 1505707512.0, 
            "domain": "self.statistics", 
            "id": "70slj0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70slj0/how_to_demonstrate_that_the_difference_between/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "How to demonstrate that the difference between two independent and normally distributed sample means also has a normal distribution?", 
            "url": "https://www.reddit.com/r/statistics/comments/70slj0/how_to_demonstrate_that_the_difference_between/"
        }, 
        {
            "author": "Aftermath12345", 
            "created_utc": 1505705841.0, 
            "domain": "self.statistics", 
            "id": "70sglh", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70sglh/name_for_a_modified_score_statistic/", 
            "score": 3, 
            "selftext": "I'm not too familiar with statistical terminology, and I can't find the information online.\n\nI have a family of densities with 4 parameters, let's say parameter a, b and 2 others (c and d) which are location and scale parameters respectively.\n\nIf c and d are known, one possibility to test the null hypothesis that the i.i.d. data X_1,X_2,...,X_n have the density above with parameters a_0, b_0, c, d, is the Rao's score test, which would be based on the score statistic, namely the 2-component vector of the derivative (with respect to a and b) of the log-density, evaluated at a_0,b_0,c,d.\n\nNow, assume that c and d are unknown.\nI could create the same Rao's score test based on a modified score statistic where everything is the same except that c and d are replaced by their maximum likelihood estimators (under the null hypothesis).\n\nIs there a common name for this \"modified score statistic\" and the consequent \"modified Rao's score test\" ?\n\nIs this a common thing to do in practice ?\n\nCan someone point me to some useful books on this subject ?\n\n(I'm writing a journal article where I derive the asymptotic distribution of a modified score statistic.)\n", 
            "subreddit": "statistics", 
            "title": "Name for a modified score statistic", 
            "url": "https://www.reddit.com/r/statistics/comments/70sglh/name_for_a_modified_score_statistic/"
        }, 
        {
            "author": "fantasticsky_hng", 
            "created_utc": 1505698952.0, 
            "domain": "self.statistics", 
            "id": "70ruk9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 31, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70ruk9/how_hard_is_it_to_get_a_data_analyst_internshipjob/", 
            "score": 23, 
            "selftext": "Hi everyone! I am currently a student in the Bay Area (San Jose, CA). I am studying Statistics, with a minor in Computer Science. My ultimate dream job is data scientist, but realistically speaking, it is quite hard for an undergraduate (and I consider myself an average student) to get such a job after getting my bachelor degree. So, my current plan is to earn work experience as a data analyst a few years, then go back to school to get a master in Statistics. Recently, my brother-in-law has had a talk with me. He said I should be mentally prepared that I will not be able to get a job with my major. I am sure he was not trying to bring me down or something. It is just because he had some friends with Math degrees (even Master) could not get a job, and he truly wanted to give me something to keep in mind. Before the talk, I had done some research, and what I've learned is that it is not too hard to find a Stat job in general as business always needs someone to analyze their data. So, that brought me here with some concerns.\n\n1.Which of the 2 stories I mentioned seems to be relevant to reality, especially in the Bay Area? I know companies said they need Stat people, but what is their need of data analysts who mostly just have a bachelor degree?\n\n2. Current data analysts or past data analysts. How is/was your job like?\n\n3. What are some most popular technical skills desired by recruiters? I looked at Udacity Data Analyst degree, it said Excel, TAbleau, Python and SQL. In my case, I know R, Python, SAS and SQL. For Excel and Tableau, I think I can pick up quite quickly.\n\n4. I know I will get a figure if I look this one up, but I still want to ask what is my expected salary as a data analyst? I don't expect to earn a lot at this point, but since I plan to go to grad school, so I need information to financially prepare from now.\n\nPlease let me know if you need additional information regarding my questions. I am hugely thankful for any thoughts, comments or feedback. Also, if you think you have a quite similar situation to me, would you mind if we get into a conversation later on? That will be great for me. Thank you!", 
            "subreddit": "statistics", 
            "title": "How hard is it to get a Data Analyst internship/job?", 
            "url": "https://www.reddit.com/r/statistics/comments/70ruk9/how_hard_is_it_to_get_a_data_analyst_internshipjob/"
        }, 
        {
            "author": "SacoETrampa", 
            "created_utc": 1505613486.0, 
            "domain": "self.statistics", 
            "id": "70kwmr", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70kwmr/how_to_convert_survival_analysis_parametric/", 
            "score": 1, 
            "selftext": "Hello all. If I have the following coefficients from a survival analysis regression and I wan to turn them into a vector of probabilities that a person with certain covariates will experience failure in steps t+1...t+n, how do I do that ?\nFor background, i'm estimating attrition from a company. div's are dummies for the division a person is in and startsal is starting salary.\nHere are the coefficients, i'm using STATA.\n    \n                failure _d:  fail\n       analysis time _t:  tenure\n    Fitting full model:\n    \n    Iteration 0:   log likelihood = -8322.0526  \n    Iteration 1:   log likelihood = -4849.4285  \n    Iteration 2:   log likelihood =  -3487.921  \n    Iteration 3:   log likelihood = -3453.8074  \n    Iteration 4:   log likelihood = -3453.4414  \n    Iteration 5:   log likelihood = -3453.4414  \n    \n    Gompertz regression -- log relative-hazard form \n    \n    No. of subjects =        3,963                  Number of obs    =       3,963\n    No. of failures =        2,685\n    Time at risk    =       798141\n                                                    Wald chi2(18)    =    24795.65\n    Log likelihood  =   -3453.4414                  Prob > chi2      =      0.0000\n    \n    ------------------------------------------------------------------------------\n              _t | Haz. Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n    -------------+----------------------------------------------------------------\n        startsal |   .9999875   6.27e-07   -19.89   0.000     .9999863    .9999888\n           divd1 |   .0019133    .000213   -56.23   0.000     .0015382    .0023797\n           divd2 |   .0011844    .000378   -21.12   0.000     .0006337    .0022138\n           divd3 |   .0011969   .0001174   -68.62   0.000     .0009877    .0014505\n           divd4 |   .0011721   .0000727  -108.86   0.000      .001038    .0013236\n           divd5 |   .0013884    .000143   -63.86   0.000     .0011345     .001699\n           divd6 |   .0012063   .0001062   -76.36   0.000     .0010152    .0014334\n           divd7 |    .001033   .0000625  -113.58   0.000     .0009175    .0011631\n           divd8 |    .001443   .0001554   -60.75   0.000     .0011685    .0017821\n           divd9 |   .0013139   .0001296   -67.25   0.000     .0010829    .0015942\n          divd10 |   .0015388   .0002064   -48.30   0.000     .0011831    .0020014\n          divd11 |   .0022684   .0001593   -86.69   0.000     .0019767    .0026031\n          divd12 |   .0031117   .0007373   -24.36   0.000     .0019558    .0049508\n          divd13 |    .004918   .0006136   -42.60   0.000     .0038512    .0062803\n          divd14 |   .0014269   .0001593   -58.69   0.000     .0011465    .0017759\n          divd15 |   .0010063    .000095   -73.10   0.000     .0008364    .0012109\n          divd16 |   .0010003    .000185   -37.35   0.000     .0006961    .0014373\n          divd17 |   .0604247   .0061647   -27.51   0.000     .0494736    .0737999\n    -------------+----------------------------------------------------------------\n          /gamma |   .0097019   .0002192    44.25   0.000     .0092722    .0101316\n    ------------------------------------------------------------------------------\n    note: no constant term was estimated in the main equation", 
            "subreddit": "statistics", 
            "title": "How to convert Survival Analysis Parametric Regression Coefficients to hazard estimates", 
            "url": "https://www.reddit.com/r/statistics/comments/70kwmr/how_to_convert_survival_analysis_parametric/"
        }, 
        {
            "author": "coip", 
            "created_utc": 1505612244.0, 
            "domain": "self.statistics", 
            "id": "70kt44", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70kt44/what_are_the_benefits_of_the_zscore/", 
            "score": 2, 
            "selftext": "Logit vs. probit is often a big debate. Many prefer logit simply because the coefficients can easily be converted into odds ratios, which are \"more intuitive\" to interpret than the z-score interpretation of probit regression analyses. \n\nOf course, many probit users bypass this issue by calculating predicted probabilities instead, but I'm curious to hear if there are any benefits to the raw z-score interpretation. Does the latter have any merits over the odds ratios of the logistic regression, in terms of interpretation of the model?", 
            "subreddit": "statistics", 
            "title": "What are the benefits of the z-score interpretation of probit regression coefficients?", 
            "url": "https://www.reddit.com/r/statistics/comments/70kt44/what_are_the_benefits_of_the_zscore/"
        }, 
        {
            "author": "CsStan", 
            "created_utc": 1505610023.0, 
            "domain": "self.statistics", 
            "id": "70kmq9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70kmq9/was_curious_how_exactly_do_people_figure_out/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Was curious, how exactly do people figure out these critical values for all these test? (z, t, chi-square, f, etc.)", 
            "url": "https://www.reddit.com/r/statistics/comments/70kmq9/was_curious_how_exactly_do_people_figure_out/"
        }, 
        {
            "author": "NooootyPoo", 
            "created_utc": 1505609777.0, 
            "domain": "self.statistics", 
            "id": "70klzq", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70klzq/i_just_had_this_idea_would_wonder_if_you_guys/", 
            "score": 0, 
            "selftext": "Hey,\n\nJust thought about if there's a statistic of comparison between the average relationship length from people with a big and people with a small dick. That would show if penis length actually matters. I couldn't anything.", 
            "subreddit": "statistics", 
            "title": "I just had this idea, would wonder if you guys could help me find something like that :)", 
            "url": "https://www.reddit.com/r/statistics/comments/70klzq/i_just_had_this_idea_would_wonder_if_you_guys/"
        }, 
        {
            "author": "wordboyhere", 
            "created_utc": 1505599887.0, 
            "domain": "bactra.org", 
            "id": "70jr6f", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70jr6f/g_a_statistical_myth/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "g, a Statistical Myth", 
            "url": "http://bactra.org/weblog/523.html"
        }, 
        {
            "author": "UsesMemesAtWrongTime", 
            "created_utc": 1505598145.0, 
            "domain": "self.statistics", 
            "id": "70jlib", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70jlib/newbie_not_sure_which_tests_to_use_for_spss/", 
            "score": 1, 
            "selftext": "Hey, I have a big spreadsheet of data with observations recorded.\n\nFor each event, there is one supervisor (Mr. X, Mr. Y, or Mr. Z) and one student (Mr. A, Mr. B, or Mr. C) recording their observations. The names are not recorded into this spreadsheet. Both the supervisor and the student observe 3 different phases of the event and record a score for each phase (e.g. 0, 25, 50, 75, 100) for a scoring system (we'll call it Barney) as well as a score for each phase (e.g. 0, 1, 2, 3, 4) for another scoring system (we'll call it Gumby)\n\nMy end goal is to\n\n1. compare interobserver agreement between supervisors and students for each scoring system (Barney and Gumbo) and possibly compare this interobserver agreement between the 2 systems to see which system has better interobserver agreement\n\n2. correlate/prove there is agreement between the Barney and Gumby systems. For instance, event #6 scored a Supervisor-Student average of 51 on the Barney scale which is statistically significantly close to the Supervisor-Student average of 2.1 on the  Gumby scale.\n\n\nI'm thinking fleiss kappa might be good here for #1 but I'm not sure. ", 
            "subreddit": "statistics", 
            "title": "Newbie not sure which tests to use for SPSS", 
            "url": "https://www.reddit.com/r/statistics/comments/70jlib/newbie_not_sure_which_tests_to_use_for_spss/"
        }, 
        {
            "author": "Muirbequ", 
            "created_utc": 1505590733.0, 
            "domain": "self.statistics", 
            "id": "70iwx9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70iwx9/statistics_over_input_distributions/", 
            "score": 1, 
            "selftext": "Is there a name for statistical methods which directly operate over distributions rather than point samples from distributions? For example, say you wanted to model the price of an item. You could return a point estimate by returning a MLE for all buyer prices. However, this may not account for the shape of the input distribution.\n\nLet's say the market can be characterized as bullish by looking at the shape of the input distribution. Knowing a market is bullish would result in a different price for items than if it was not. Is there a methodology for utilizing the shape of the input distribution as an input? Can it deal with distributions of different sizes? Are priors needed?\n\nThe machine learning community uses convolutional neural networks to operate over distributions of pixels. However, techniques like this still require a prior on the dimensions of the inputs. Are there techniques better suited for variable length distributions?\n\nThanks!", 
            "subreddit": "statistics", 
            "title": "Statistics over Input Distributions", 
            "url": "https://www.reddit.com/r/statistics/comments/70iwx9/statistics_over_input_distributions/"
        }, 
        {
            "author": "IM_BOAT", 
            "created_utc": 1505537261.0, 
            "domain": "self.statistics", 
            "id": "70f2hr", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 10, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70f2hr/hello_everyone_i_just_want_some_advice_recently/", 
            "score": 16, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Hello everyone, I just want some advice. Recently came to the realization that I enjoy probability theory more than Applied Statistics. What options do I have as a career?", 
            "url": "https://www.reddit.com/r/statistics/comments/70f2hr/hello_everyone_i_just_want_some_advice_recently/"
        }, 
        {
            "author": "victorlinguist", 
            "created_utc": 1505530608.0, 
            "domain": "self.statistics", 
            "id": "70eldp", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70eldp/nsize_for_simple_mean_difference_comparison/", 
            "score": 3, 
            "selftext": "I am comparing the difference in scores for students in a pre-test and a post-test (matched samples). I am not doing a t-test or anything like that; it's a simple mean difference in score. For statistical significant tests, there are many guidelines for calculating sample size required. However, for such a simple scenario like mine, are there any guidelines for a minimum n-size that would make the mean difference somewhat reliable?", 
            "subreddit": "statistics", 
            "title": "N-size for simple mean difference comparison", 
            "url": "https://www.reddit.com/r/statistics/comments/70eldp/nsize_for_simple_mean_difference_comparison/"
        }, 
        {
            "author": "ChainedMarkov", 
            "created_utc": 1505520869.0, 
            "domain": "self.statistics", 
            "id": "70dtig", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70dtig/when_working_on_a_big_project_in_r_how_many_lines/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "When working on a big project in R, how many lines do you like your scripts? 500? 2000?", 
            "url": "https://www.reddit.com/r/statistics/comments/70dtig/when_working_on_a_big_project_in_r_how_many_lines/"
        }, 
        {
            "author": "CsStan", 
            "created_utc": 1505503262.0, 
            "domain": "self.statistics", 
            "id": "70c3am", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70c3am/how_to_calculate_the_power_of_a_t_test/", 
            "score": 3, 
            "selftext": "Hey, \n\nI was wondering how you would calculate the power of a one sided and two sided t test and whether it differed from the z test.\n\nI know for a z test it is p(z < z_alpha - abs(ua - u0)/(s/sqrt(n))) for one sided and for two sided you replace the z_alpha by z_alpha/2.\n\nFor a t test would it be the same thing but the z_alpha part is replaced by t_alpha or is there something else that differs?\n\nAlso on a side not how would you do it in R?\n\nThank you for reading", 
            "subreddit": "statistics", 
            "title": "How to calculate the power of a t test?", 
            "url": "https://www.reddit.com/r/statistics/comments/70c3am/how_to_calculate_the_power_of_a_t_test/"
        }, 
        {
            "author": "mathmare", 
            "created_utc": 1505503152.0, 
            "domain": "youtu.be", 
            "id": "70c2uv", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": {
                "oembed": {
                    "author_name": "Shane Killian", 
                    "author_url": "https://www.youtube.com/user/shanedk", 
                    "height": 338, 
                    "html": "<iframe width=\"600\" height=\"338\" src=\"https://www.youtube.com/embed/g1Ll244JADw?feature=oembed\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/g1Ll244JADw/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Quickie: Statistical Paradoxes", 
                    "type": "video", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70c2uv/quickie_statistical_paradoxes/", 
            "score": 21, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Quickie: Statistical Paradoxes", 
            "url": "https://youtu.be/g1Ll244JADw"
        }, 
        {
            "author": "napsternxg", 
            "created_utc": 1505499621.0, 
            "domain": "shubhanshu.com", 
            "id": "70bp5z", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70bp5z/time_dependent_linear_models_in_sklearn_python/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Time dependent linear models in sklearn python", 
            "url": "http://shubhanshu.com/blog/blog/time-dependent-models.html#time-dependent-models"
        }, 
        {
            "author": "third_rate_economist", 
            "created_utc": 1505491021.0, 
            "domain": "self.statistics", 
            "id": "70aqq1", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70aqq1/panel_data_question/", 
            "score": 1, 
            "selftext": "Hi Everyone,\n\nI'm working on a research project that is part of a randomized control trial. A basic description is that patients are given tablets in a hospital. Some patients are left alone to do as they please and other patients are shown how to use the tablet. For starters, I'm just looking at how frequently each patient uses the tablet on each day of their stay.\n\nHere's the part I'm unsure about modeling. The treatment effect (education) is not given before the patient gets the tablet, it is given at a non-standardized time after the patient has already started using the tablet. I thought I would look at the data as a panel. If anyone has insight that would be cool! Here are the big questions I have.\n\n1) Can I keep patients with different panel lengths in the same model? Say a patient with day 0 to day 3 and a patient with day 0 to day 15?\n\n2) Is it acceptable to have the treatment effects happening at different time intervals? \n\n3) The usage data appears to have some sort of natural trend - like high usage in the first day followed by usage that trails off over time. Will mixing different stay lengths, treatment times, and natural trends cause additional headaches?\n\nIf you have an ideas, I'd be happy to hear them!", 
            "subreddit": "statistics", 
            "title": "Panel Data Question", 
            "url": "https://www.reddit.com/r/statistics/comments/70aqq1/panel_data_question/"
        }, 
        {
            "author": "xX-DataGuy-Xx", 
            "created_utc": 1505490668.0, 
            "domain": "self.statistics", 
            "id": "70apet", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/70apet/newbie_question_calculate_probability_of_getting/", 
            "score": 1, 
            "selftext": "Beginning learning statistics and probabilities. Was wondering on way into work how I would calculate my probability of getting into wreck. I figured since there is a car in front of me, in back and on each side,  and if this configuration remains for the entire trip, wouldn't my probability be 1 in 4? or 1 in 5 if you include NOT hitting any car? Be gentle, i'm just learning :)\n", 
            "subreddit": "statistics", 
            "title": "Newbie Question - Calculate probability of getting into car accident on way to work", 
            "url": "https://www.reddit.com/r/statistics/comments/70apet/newbie_question_calculate_probability_of_getting/"
        }, 
        {
            "author": "Moomootank", 
            "created_utc": 1505461247.0, 
            "domain": "self.statistics", 
            "id": "708e0u", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/708e0u/difference_of_two_proportions_hypothesis_test/", 
            "score": 4, 
            "selftext": "I have survey data which contain respondents' answers to several questions. As the survey contained a disproportionate number of people from certain demographic groups, the survey results are weighted by race, sex and age.\n\nI have responses to the same questions for two years (eg. 2016 and 2017), and am trying to find out if the proportion of people who responded \"yes\" to a particular question has fallen. That is, I have calculated the weighted agreement rate to the question in 2016 (p1) and the weighted agreement rate to the question in 2017 (p2), and am trying to see if p1 - p2 = 0.\n\nI think I have a good idea of how to perform the simple hypothesis test for a difference between two proportions is  (described at https://onlinecourses.science.psu.edu/stat414/node/268). However, I do not have a formal background in statistics (only have some experience in introductory college courses and econometrics). Thus, I am wondering if: \n\n1) Weighting the samples by demographic variables has changed the standard error; thus, a more complicated hypothesis test formula is required. If so, what is this formula?\n\n2) Whether there are other methods, other than applying this possibly more complicated formula, to rigorously test for a difference between the two proportions. For instance, are there non parametric hypothesis tests that can be used?\n\nThanks for your help!", 
            "subreddit": "statistics", 
            "title": "Difference of Two Proportions Hypothesis Test with Weighted Sample Data", 
            "url": "https://www.reddit.com/r/statistics/comments/708e0u/difference_of_two_proportions_hypothesis_test/"
        }, 
        {
            "author": "wordboyhere", 
            "created_utc": 1505441414.0, 
            "domain": "self.statistics", 
            "id": "706y0f", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 153, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/706y0f/what_is_a_controversial_opinion_that_you_have/", 
            "score": 62, 
            "selftext": "Anything vaguely related to statistics. ", 
            "subreddit": "statistics", 
            "title": "What is a controversial opinion that you have?", 
            "url": "https://www.reddit.com/r/statistics/comments/706y0f/what_is_a_controversial_opinion_that_you_have/"
        }, 
        {
            "author": "magicroot75", 
            "created_utc": 1505441270.0, 
            "domain": "theoptimizednow.com", 
            "id": "706xkk", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/706xkk/rating_the_restaurant_raters_yelp_vs_google_vs/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Rating the restaurant raters: Yelp vs. Google vs. Facebook", 
            "url": "http://www.theoptimizednow.com/restaurantraters/"
        }, 
        {
            "author": "Memo_moto", 
            "created_utc": 1505431419.0, 
            "domain": "self.statistics", 
            "id": "7060i3", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/7060i3/have_most_of_you_self_taught_yourself_in_r_or/", 
            "score": 4, 
            "selftext": "Hey guys, \n\nJust got a quick question. I am soon to be starting an **MSc in Social Anthropology** later this month in which I will have the option of choosing 3 optional modules. 1 module I have been considering is called **Statistics and Causal Analysis for Qualitative Social Scientists**  \n\n\n\nMy question to you, is if there is any point in me choosing this as a module choice when there appears to be a colossal amount of online resources that I could do in my own time to learn R. \n\n\n\nThere are quite a few other interesting and relevant modules that I would like to enrol on (I'm planning on getting onto a research degree after this year with a focus on conflict, security and terrorism) and have been wondering if it would be a waste of a module choice to choose statistics when I can do it in my own time. \n\n\n\nLet me know your opinions, especially if you feel that it would be much better for me to get a proper grounding of the language within an academic environment. The module is described with the following on my university's module choice directory: \n\n\n\n>This course introduces statistics and the R language from their very basics. The course assumes no background knowledge of either statistics or statistical software. Topics covered include an introduction to statistics in R, distributions, hypothesis\ntesting (t-tests, proportion tests, ANOVA), correlation, linear regression, multivariate statistics (multiple regression, PCA, discriminant analysis) and logistic regression.\"\n\nThank you!", 
            "subreddit": "statistics", 
            "title": "Have most of you self taught yourself in R or other statistical languages?", 
            "url": "https://www.reddit.com/r/statistics/comments/7060i3/have_most_of_you_self_taught_yourself_in_r_or/"
        }, 
        {
            "author": "IamScuzzlebut", 
            "created_utc": 1505415478.0, 
            "domain": "self.statistics", 
            "id": "704clc", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Meta", 
            "media": null, 
            "num_comments": 14, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/704clc/what_are_great_videos_to_get_back_into_statistics/", 
            "score": 14, 
            "selftext": "Hey Redditstaticians,\n\nMy girlfriend is starting a study where statistics play a major role. She used to be really good at it, but finds it daunting to start again after a couple years not using the knowledge. Ofcourse she has her books and everything, but I think she needs someone explaining it to her in a TL;DR fashion.\n\nSo do you guys have some youtubevideos she can watch to get it going again? Preferably by a professor that explains it in a somewhat pleasant way (some people have that gift). \nHer study is in health/psychology research, so it would be best if the video is somewhat aimed in that direction.", 
            "subreddit": "statistics", 
            "title": "What are great videos to get back into statistics?", 
            "url": "https://www.reddit.com/r/statistics/comments/704clc/what_are_great_videos_to_get_back_into_statistics/"
        }, 
        {
            "author": "Snowy_Mass", 
            "created_utc": 1505415032.0, 
            "domain": "self.statistics", 
            "id": "704ayu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/704ayu/i_lost_my_ti_nspire_cx_cas_and_a_statistics_test/", 
            "score": 1, 
            "selftext": "Hello, a bit of context.\n\nSo I'm a freshman in college, and my teacher in high school said we should get the highest end calculator so we could get used to it for college. So my folks got a Ti Nspire CX CAS for my future math since I wanted to do a scientific degree. I grew to have a good knowledge of the device and loved working with it. Surprisingly enough once I was in college I found that my degree (nursing) only had me take elementary Statistics. \n\nThe first test is on Monday and we're allowed to bring a calculator. So I look to bring my Nspire buddy and I can't find him anywhere. I could potentially if I order it now get a new one in time for the test, or we have scientific calculator (looking at the TI-36X Pro Scientific Calculator) which the professor said was all we needed for the test. So I'm asking you reddit, is it worth the 177.99 dollars to get a new Nspire?\n\nEDIT: Found it, now I just need to find a charger.", 
            "subreddit": "statistics", 
            "title": "I lost my TI Nspire CX CAS and a Statistics test is coming, is it worth replacing? (xover w r/Nspire)", 
            "url": "https://www.reddit.com/r/statistics/comments/704ayu/i_lost_my_ti_nspire_cx_cas_and_a_statistics_test/"
        }, 
        {
            "author": "MandolorianWookie89", 
            "created_utc": 1505407460.0, 
            "domain": "self.statistics", 
            "id": "703gda", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/703gda/understanding_finding_the_p_value_of_the_critical/", 
            "score": 3, 
            "selftext": "I am having issues with finding the p value of the critical value on ti-84. Here is the problem I am working on:\n\nThe director of research and development is testing a new medicine. She wants to know if there is evidence at the 0.05. level that the medicine relieves pain in more than 349 seconds. For a sample of 69 patients, the mean time in which the medicine relieved pain was 352 seconds. Assume the population standard deviation is 21. Find the P-value of the test statistic. Round your answer to four decimal places.\n\nusing z table on ti-84 = 1.187\n\nthen im going to tcdf lower limit 1.187, upper: 10000000000, df 68 = .1196\n\nBut the actual answer is: 0.1170.\n\nNot sure what I am doing wrong on the calculator.\n\nThanks\n", 
            "subreddit": "statistics", 
            "title": "Understanding finding the P value of the critical value", 
            "url": "https://www.reddit.com/r/statistics/comments/703gda/understanding_finding_the_p_value_of_the_critical/"
        }, 
        {
            "author": "lastofyou88", 
            "created_utc": 1505406714.0, 
            "domain": "self.statistics", 
            "id": "703dge", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/703dge/designing_a_drug_treatmentprotein_expression/", 
            "score": 2, 
            "selftext": "HI guys, \n\nI have a project that I am interested in and the easiest way to describe it is that I am going to treat cells with vehicle, drug A, and drug B for 2 hours and also vehicle, drug A, and drug B for 48 hours and look at protein expression changes.\n\n I want to compare responses to:\n\ndrug A vs drug B at each timepoint. \n\ndrug A at 2 hours vs drug A at 48 hours and the same for drug B.\n\nI also would like to be able to compare each drug to vehicle at each timepoint. \n\nThere are a lot more comparisons going on here than I am used to. We usually just do simple t tests and repeated measures ANOVA in our lab. My stats are pretty weak, so I am wondering if someone could help point me in the right direction to help me figure out what kind of tests I would need to perform here. Thanks!\n", 
            "subreddit": "statistics", 
            "title": "Designing a drug treatment/protein expression study, need help with choosing stats", 
            "url": "https://www.reddit.com/r/statistics/comments/703dge/designing_a_drug_treatmentprotein_expression/"
        }, 
        {
            "author": "redditmaster21", 
            "created_utc": 1505394548.0, 
            "domain": "self.statistics", 
            "id": "7023s6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/7023s6/stats_data_science_vs_double_major_stats_and/", 
            "score": 0, 
            "selftext": "I am currently majoring in stats, and my uni allows us to choose a field of specialisation. I have narrowed it down to either Data Science or Business Stats. However, I am also thinking if I should take up a second major in CompSci.\n\nI understand that CompSci is also in high demand and is very complementary with stats. The good thing is that I am interested in creating programmes and writing code that can help with stats problems. At the same time, I would also like to do finance-related statistics which the Business Stats specialisation is all about. \n\nHence, my question is: what are the pros and cons of sticking with stats and specialising in data science, against specialising in business stats and taking up a second major in compsci?", 
            "subreddit": "statistics", 
            "title": "Stats (Data Science) Vs Double Major (Stats and CompSci)", 
            "url": "https://www.reddit.com/r/statistics/comments/7023s6/stats_data_science_vs_double_major_stats_and/"
        }, 
        {
            "author": "kiser_soze", 
            "created_utc": 1505378769.0, 
            "domain": "mlwhiz.com", 
            "id": "701009", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/701009/good_feature_building_techniques_tricks_for_kaggle/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Good Feature Building Techniques - Tricks for Kaggle", 
            "url": "http://mlwhiz.com/blog/2017/09/14/kaggle_tricks/"
        }, 
        {
            "author": "LetsEndSuffering", 
            "created_utc": 1505367254.0, 
            "domain": "self.statistics", 
            "id": "700b1c", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 32, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/700b1c/what_jobs_do_holders_of_masters_in_statistics_get/", 
            "score": 27, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "what jobs do holders of Master's in Statistics get? What jobs did you get?", 
            "url": "https://www.reddit.com/r/statistics/comments/700b1c/what_jobs_do_holders_of_masters_in_statistics_get/"
        }, 
        {
            "author": "mecolema", 
            "created_utc": 1505352646.0, 
            "domain": "self.statistics", 
            "id": "6zz4yg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zz4yg/interpreting_odds_ratios_in_logistic_models/", 
            "score": 2, 
            "selftext": "When running a logistic regression model, I need a way to measure effects that are partialed out when a mediator is added. The easiest way to do this in OLS is to observe the decline in the slope coefficient (e.g., a decline from 4 to 3 suggests a 25% decrease in effect size once the mediator is added). \n\nWhat I'm wondering is: can the same be done with odds ratios? Or would you need to convert to logged odds before calculating the reduction in effect size? ", 
            "subreddit": "statistics", 
            "title": "Interpreting odds ratios in logistic models", 
            "url": "https://www.reddit.com/r/statistics/comments/6zz4yg/interpreting_odds_ratios_in_logistic_models/"
        }, 
        {
            "author": "bubbachuck", 
            "created_utc": 1505352325.0, 
            "domain": "self.statistics", 
            "id": "6zz3v4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zz3v4/how_to_present_regression_results_when_outcomes/", 
            "score": 2, 
            "selftext": "The outputs I'm trying to predict are binary (i.e., alive or dead) but my model will output a probability (i.e., probability of death). Let's say there are multiple models I want to compare. What's the best way to determine which one is the best?\n\nOne way would be to threshold the probabilities at say 0.5 and convert the model outputs into binary, and then do classification error. And create an AUC out of that by varying the threshold.\n\nBut are there other ways? I feel like I'm losing information by thresholding but not sure what better way would be.", 
            "subreddit": "statistics", 
            "title": "How to present regression results when outcomes are binary?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zz3v4/how_to_present_regression_results_when_outcomes/"
        }, 
        {
            "author": "blockhead123", 
            "created_utc": 1505341711.0, 
            "domain": "self.statistics", 
            "id": "6zy3b8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zy3b8/assessing_the_accuracy_of_predictions_of_a_mixed/", 
            "score": 3, 
            "selftext": "I've been using lmer to fit a model with one data set, and I want to tests its accuracy using several similar data sets that we already have. I've gotten to the point where I have predicted values of my (continuous) DV as well as the actual values we collected. \n\nWhat's the best way to assess the accuracy of the prediction? R-squared? RMSE? If it helps, I intend the model to be predictive, not just explanatory.", 
            "subreddit": "statistics", 
            "title": "Assessing the accuracy of predictions of a mixed model?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zy3b8/assessing_the_accuracy_of_predictions_of_a_mixed/"
        }, 
        {
            "author": "Aqwis", 
            "created_utc": 1505340586.0, 
            "domain": "stats.stackexchange.com", 
            "id": "6zxz4f", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zxz4f/handling_incomplete_information_properly_when/", 
            "score": 3, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Handling incomplete information properly when using Bayes' theorem (specifically in the context of medical diagnosis)", 
            "url": "https://stats.stackexchange.com/questions/303031/handling-incomplete-information-properly-when-using-bayes-theorem"
        }, 
        {
            "author": "coffeecoffeecoffeee", 
            "created_utc": 1505338941.0, 
            "domain": "self.statistics", 
            "id": "6zxswi", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zxswi/looking_for_good_resources_on_working_with_cell/", 
            "score": 2, 
            "selftext": "I'd post this in /r/DSP, but it's pretty much dead.\n\nI'm working with sensor (accelerometer/gyroscope/magnetometer) data gathered from a mobile device.  In a nutshell, we have k drivers and we want to build a model that can determine which of the k drivers is driving.  There's a paper I went through that does a similar analysis, but there are some glaring flaws in it (e.g. they didn't take temporality into account when doing crossvalidation.)  It also uses data that's far less noisy than what we have.\n\nWhat good books or resources can you recommend for building this kind of a model?  I looked into time series classification, and while it seems useful, it primarily deals with detecting events within a signal, rather than determining which of k individuals produced a given signal.\n\nWhat are some good resources for learning to work with this kind of data?  There seems to be a dearth of information on the internet.  My ideal situation would be a few Applied Predictive Modeling-style example walkthroughs.", 
            "subreddit": "statistics", 
            "title": "Looking for good resources on working with cell phone sensor data.", 
            "url": "https://www.reddit.com/r/statistics/comments/6zxswi/looking_for_good_resources_on_working_with_cell/"
        }, 
        {
            "author": "BatsuGame13", 
            "created_utc": 1505337688.0, 
            "domain": "self.statistics", 
            "id": "6zxo2a", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zxo2a/phd_students_what_is_your_focus_when_did_you/", 
            "score": 13, 
            "selftext": "Title says it all, but for further clarification:\n\nI'm currently in the process of applying to PhD programs. Should I have any idea at the moment what I want to focus on when/if I pass qualifying exams? There are a lot of things I'm interested in, but I don't feel like I would even know where to start to narrow down a focus. Is this normal? (For the record, my undergrad is in stats as well.)\n\nAnyway, I'd also love to hear what people are studying now and what it entails!", 
            "subreddit": "statistics", 
            "title": "PhD students, what is your focus, when did you figure it out?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zxo2a/phd_students_what_is_your_focus_when_did_you/"
        }, 
        {
            "author": "Hellkyte", 
            "created_utc": 1505334852.0, 
            "domain": "self.statistics", 
            "id": "6zxcyl", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zxcyl/277/", 
            "score": 5, 
            "selftext": "I'm working on a stats project that is killing me.  Basically our organization has been using these kind of obscure ASTM tools for managing a gage variance monitoring system.\n\nMost of it makes sense.  For instance there is standard deviation broken out by different levels.  Multiple site, multiple operators, etc..  good so far.\n\nWhat doesn't make sense is this factor, 2.77, that is hidden inside one of the formulas.  It is (effectively) the z-score for one of those different levels of variance, and they say it's for the 95th percentile.  On it's face of course that makes no sense.  \n\nNow in statistical process control there are these correction factors you use to unbias sigma.   So the smaller your sample size the more you (potentially) underestimate sigma, so you use this correction factor to unbias it.\n\nBut that is just too big for a correction factor to get there.  In an N=2 situation it only goes up to a factor of 2.5.  \n\nBut then there's this part of me that hears 2.77 and I feel like I know it from somewhere.  I actually spent some time this afternoon just googling that number.\n\nAnyways.  Any thoughts on how you get a 95% area from sigma (or a biased point estimator of sigma) with a z score of 2.77? \n\n\n", 
            "subreddit": "statistics", 
            "title": "2.77", 
            "url": "https://www.reddit.com/r/statistics/comments/6zxcyl/277/"
        }, 
        {
            "author": "__compactsupport__", 
            "created_utc": 1505330553.0, 
            "domain": "self.statistics", 
            "id": "6zwvgy", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zwvgy/supplements_on_liklihood_theory/", 
            "score": 1, 
            "selftext": "I'm a new Ph.D student in biostats coming from applied math.  Consequently,  likelihood theory is new but not completely foreign to me.\n\nCan someone suggest some readings to helo supplement my understanding?  Specifically, I'd like to read more on properties of the MLE (i.e. consistency, efficientcy etc) and regularity conditions.", 
            "subreddit": "statistics", 
            "title": "Supplements on Liklihood Theory", 
            "url": "https://www.reddit.com/r/statistics/comments/6zwvgy/supplements_on_liklihood_theory/"
        }, 
        {
            "author": "givnv", 
            "created_utc": 1505323298.0, 
            "domain": "self.statistics", 
            "id": "6zw1kv", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zw1kv/how_to_find_the_increase_factor_between_two/", 
            "score": 1, 
            "selftext": "Hello, \n\nI have a question related to one of my school assignments. I have two variables- time and temperature. I want to know how much temperature is increasing for each n+1 in time. What method should I use to get to the solution?\n\nTime(s) | Temperature(C)\n--------- | ----------------\n0 | 20\n4 | 57\n8 | 89\n12 | 108\n16 | 131\n20 | 145\n30 | 176\n40 | 191\n50 | 204\n60 | 210\n70 | 215\n80 | 217\n120 | 220\n\nThank you very much in advance!!!", 
            "subreddit": "statistics", 
            "title": "How to find the increase factor between two non-linear variables.", 
            "url": "https://www.reddit.com/r/statistics/comments/6zw1kv/how_to_find_the_increase_factor_between_two/"
        }, 
        {
            "author": "ChesterEnergy", 
            "created_utc": 1505315971.0, 
            "domain": "chesterenergyandpolicy.com", 
            "id": "6zv7sm", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zv7sm/getting_us_energy_data_from_the_energy/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Getting U.S. Energy Data from the Energy Information Administration", 
            "url": "http://chesterenergyandpolicy.com/2017/09/13/navigating-the-vast-eia-datasets/"
        }, 
        {
            "author": "mmmmmchild", 
            "created_utc": 1505311597.0, 
            "domain": "self.statistics", 
            "id": "6zuqts", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zuqts/textbook_suggestions/", 
            "score": 0, 
            "selftext": "I come from a an undergrad background in Math and consider myself well-versed in probability, but I haven't done much in terms of statistics. Currently, I'm beefing up my R, SQL, and Tableau skills, but I was wondering if anyone had any suggestions for textbooks (paid or otherwise) that don't shy away from the theory or math. I've done digging online, but most of the resources seem catered to people who don't like math or haven't had much formal education in it. Long story short, any book suggestions that are more on the applied side with theory baked in and isn't programming-focused would be greatly appreciated!", 
            "subreddit": "statistics", 
            "title": "Textbook Suggestions", 
            "url": "https://www.reddit.com/r/statistics/comments/6zuqts/textbook_suggestions/"
        }, 
        {
            "author": "redditmaster21", 
            "created_utc": 1505280440.0, 
            "domain": "self.statistics", 
            "id": "6zsmbw", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zsmbw/correlation_causation_then_are_correlated/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Correlation =/= causation. Then are correlated variables still considered as explanatory variables?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zsmbw/correlation_causation_then_are_correlated/"
        }, 
        {
            "author": "becboo", 
            "created_utc": 1505279401.0, 
            "domain": "self.statistics", 
            "id": "6zsjt1", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zsjt1/statisticians_and_biostatisticians_in_industry/", 
            "score": 4, 
            "selftext": "I always hear that the quality of your school only matters for your first job. When I browse linkedin profiles and team pages of companies that I would like to work for I see A LOT of well regarded schools (think top 25 USNews rankings, and especially top 5 biostatistics when it comes to biotech).\n\n\n Is mobility actually limited by your alma mater or is it more the confounding of the types of people that get into those schools in the first place?", 
            "subreddit": "statistics", 
            "title": "Statisticians and Biostatisticians in industry, how much did/does the ranking of your school play a role?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zsjt1/statisticians_and_biostatisticians_in_industry/"
        }, 
        {
            "author": "XXXtaxation", 
            "created_utc": 1505274751.0, 
            "domain": "self.statistics", 
            "id": "6zs70y", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zs70y/questions_about_linearization_of_a_non_linear/", 
            "score": 1, 
            "selftext": "I took statistics last year and I just wanted to refresh my self because we're linearizing data in Chemistry and had some questions to add on to it.\n\n1. If you log just the y value but keep the x values the same, is it a power model or exponential model?\n\n2. If you log both x and y, is it a power model or exponential model?\n\n3. Why does using log (or ln????) linearize data? \n\n4. Is there any significant difference between -log and log of a model? (this may sound like a stupid question but I had to choose between which one was a better model in chemistry and it was -log because it made the regression formula positive not negative, -x)\n", 
            "subreddit": "statistics", 
            "title": "Questions about linearization of a non- linear model?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zs70y/questions_about_linearization_of_a_non_linear/"
        }, 
        {
            "author": "oreo_fanboy", 
            "created_utc": 1505271827.0, 
            "domain": "replicationindex.wordpress.com", 
            "id": "6zryqo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zryqo/daniel_kahneman_replies_to_a_critique_of_studies/", 
            "score": 81, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Daniel Kahneman replies to a critique of studies on priming", 
            "url": "https://replicationindex.wordpress.com/2017/02/02/reconstruction-of-a-train-wreck-how-priming-research-went-of-the-rails/#comment-1454"
        }, 
        {
            "author": "zefyear", 
            "created_utc": 1505267458.0, 
            "domain": "self.statistics", 
            "id": "6zrl3n", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zrl3n/does_there_exist_someone_faster_than_usain_bolt/", 
            "score": 0, 
            "selftext": "There are (serious) problems with estimating the likelihood of faster runners than Mr. Bolt from record-setting dash times that are both obvious and subtle. Humor me by imagining none of them exist.\n\n--------------\n\nUsain Bolt is the fastest human measured for the 100m dash. However, given the small number of athletes, it seems likely that the \"true\" fastest human alive is sitting on a couch somewhere and has never attempted a competitive running career.\n\nI am trying to use the fact that the difference between samples at the tails of the normal distribution become smaller and smaller. I'm using this to compute the likelihood there exists someone faster than Usain Bolt by comparing Usain to the 2nd fastest, 3rd fastest and so on.\n\nTo do this, I'm trying to compute the largest value that exists beyond \"Usain Bolt\" by taking the derivative of the normal distribution's CDF with respect to y, raising that to the nth (where n is about 7,000,000,000 or the number of samples less than the \"maximum\" - the logic behind this is described in the [German Tank Problem](https://en.wikipedia.org/wiki/German_tank_problem) Wikipedia page which generalizes among different distributions), e.g:\n\n    $\\int_{0}^{\\infty}y f_{Y_N} (y)dy = \\lambda n \\int_{0}^{\\infty} y \\left [ \\tfrac12\\left[1 + \\operatorname{erf}\\left( \\frac{y-\\mu}{\\sigma\\sqrt{2}}\\right)\\right]  \\right ]^{n-1} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\, e^{-\\frac{(y - \\mu)^2}{2 \\sigma^2}}dy$\n\n1. Is this a valid way to compute the probability that there exists someone faster than Usain Bolt?\n\n2. Is there a name for this sort of question outside of \"German Tank Problem for other distributions\"\n\n3. Is there a good way to estimate standard deviation from the extreme samples of a distribution? Finding information about the fastest 100m dashes of all time is easy, finding averages & variance is hard)\n\nThank you for your patience in dealing with a programmer without a background in the topic.\n\n", 
            "subreddit": "statistics", 
            "title": "Does there exist someone faster than Usain Bolt today?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zrl3n/does_there_exist_someone_faster_than_usain_bolt/"
        }, 
        {
            "author": "ArbitraryMathGuy", 
            "created_utc": 1505259407.0, 
            "domain": "self.statistics", 
            "id": "6zqul8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 23, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zqul8/getting_into_grad_school_for_applied_stats/", 
            "score": 6, 
            "selftext": "Hello /r/statistics. I am a recent graduate who has been struggling and trying to break into the data analysis field with no luck. I have since decided to try and get into the data entry field and no luck either. So I have decided to go get my Masters in the Stats for Fall 2018.\n\nI got my BA in Applied Mathematics with a Concentration in Probability and Stats. What types of challenges will I face in making this jump? I graduated with a 3.0 and want to go to a state school. Will scoring higher on the GRE help me out even though the program I want to go to doesn't require it? I am currently working at a dead end movie theatre job where the Operations Manager is making it pretty unbearable to work at. My girlfriend wants me to stay but I want to leave and find employment elsewhere while getting my degree.\n\nAny advice would be appreciated. Thank you.", 
            "subreddit": "statistics", 
            "title": "Getting into Grad School for Applied Stats", 
            "url": "https://www.reddit.com/r/statistics/comments/6zqul8/getting_into_grad_school_for_applied_stats/"
        }, 
        {
            "author": "CsStan", 
            "created_utc": 1505258807.0, 
            "domain": "self.statistics", 
            "id": "6zqsiw", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zqsiw/understanding_power_and_its_relationship_to_type/", 
            "score": 1, 
            "selftext": "Hey guys,\n\nSo I was wondering what the intuition was between these two events and how to understand them. I know what a type 2 error is: probability of incorrectly accepting the null and I know what power is: probability that you correctly reject the null.\n\nOne thing I would like to know is the intuition between the two and how to visualize it. I know there are formulas but is there some explanation which ties these together?\n\nThanks for reading", 
            "subreddit": "statistics", 
            "title": "Understanding power and its relationship to Type 2 Error?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zqsiw/understanding_power_and_its_relationship_to_type/"
        }, 
        {
            "author": "trumtra", 
            "created_utc": 1505225007.0, 
            "domain": "blog.statsbot.co", 
            "id": "6zn4vn", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zn4vn/guide_on_building_your_own_neural_conversational/", 
            "score": 14, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Guide on building your own neural conversational agent", 
            "url": "https://blog.statsbot.co/chatbots-machine-learning-e83698b1a91e"
        }, 
        {
            "author": "statred", 
            "created_utc": 1505217710.0, 
            "domain": "self.statistics", 
            "id": "6zmhvz", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zmhvz/what_are_my_chances_of_getting_into_grad_school/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "What are my chances of getting into grad school for Statistics with a 3.2?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zmhvz/what_are_my_chances_of_getting_into_grad_school/"
        }, 
        {
            "author": "OverflowDs", 
            "created_utc": 1505215618.0, 
            "domain": "census.gov", 
            "id": "6zmc6u", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zmc6u/2016_american_community_survey_content_test/", 
            "score": 2, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "2016 American Community Survey Content Test: Telephone Service", 
            "url": "https://www.census.gov/library/working-papers/2017/acs/2017_Mazur_01.html"
        }, 
        {
            "author": "alabasterheart", 
            "created_utc": 1505207418.0, 
            "domain": "self.statistics", 
            "id": "6zltzu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zltzu/data_on_where_stanford_statistics_department/", 
            "score": 28, 
            "selftext": "If you frequent the math subreddit, you may have seen some posts about collecting data on where faculty members in math departments received their PhDs (if you're curious, here is the data for [Stanford](https://www.reddit.com/r/math/comments/5rqz9u/collected_data_on_where_stanford_mathematics/), and here is the data for [MIT](https://www.reddit.com/r/math/comments/6tyqp0/data_on_where_mit_math_department_faculty_members/)). I did these mainly because I was bored, I like data, and PhD data is easily accessible using the [Math Genealogy Project](https://genealogy.math.ndsu.nodak.edu/). I recently learned that the Math Genealogy project has data not just for Math departments, but also for Statistics and other fields, so I thought it would be fitting to do this data collection for a Statistics department. I chose Stanford to do this because it has been widely regarded as the 'top-ranked' Statistics Department for a long time, and I think it's interesting to see which universities produce the most faculty members that end up at top departments. Doing this sort of analysis for math departments also shed some light for me on just how difficult it is to become a tenure-track professor at a top institution. \n\nThe following table shows data on where all tenure-track faculty (including Emeritus faculty) in the Statistics Department at Stanford received their PhDs (foreign universities have their country listed next to them):\n\nInstitution|Number of PhDs\n:--|:-- \nStanford|7\nBerkeley|4\nHarvard|3\nColumbia|2\nCornell|2\nMcGill (Canada)|1\nMontpellier (France)|1\nPerugia (Italy)|1\nTechnion (Israel)|1\nWashington|1\nWisconsin-Madison|1\n\nIf we include all post-docs, research associates, adjunct faculty, and other non-tenure track academic staff, we add the following numbers:\n\nInstitution|Number of PhDs\n:--|:-- \nBerkeley|2\nStanford|2\nBern (Switzerland)|1\nChinese Academy of Sciences (China)|1\nDuke|1\nFlorida State|1\nHarvard|1\nIowa State|1\nLeiden (Netherlands)|1\nMIT|1\nPrinceton|1\nTexas Tech|1\nTsinghua (China)|1\nUChicago|1\nUPenn|1\nUSC|1\nUT Austin|1\nYale|1\n\nSome comments:\n\n1. Statistics Departments seem in general to be a lot smaller than Mathematics Departments, so there was less data for me to collect than when I did this analysis for math departments. For instance, Stanford only has 24 tenure-track faculty members in the Stat Department, but when I did this analysis for math departments, Stanford had 38 faculty members while MIT had 68. \n\n2. It probably isn't a surprise to you that Stanford and Berkeley, usually considered to be the top two Statistics Departments, are the two schools that produced the most Statistics faculty members at Stanford, with Stanford itself producing the highest number of faculty members. Harvard also has a very well-regarded Statistics Department, so it probably isn't too surprising to see Harvard in third place. However, I think there are a lot of fantastic Statistics Departments that are underrepresented/absent. For instance, UChicago, Carnegie Mellon, Duke, UPenn, etc. all have very respected and well-ranked Statistics Departments (all top 10 according to US News), but there are no faculty members from any of these schools. Columbia and Cornell are well-represented despite being ranked 20 and 24 respectively. \n\n3. Several faculty members/academic staff actually didn't get a PhD in Statistics. There were a fair number of people who got their PhDs in other affiliated fields, like Mathematics, Operations Research, Computer Science, etc. For instance, on the second table, there are schools like Princeton, MIT, etc. listed, and these schools don't even have a Statistics Department! One research associate in particular got a PhD in Government. There seems to be a wide range of disciplines that can lead to doing work in a Statistics Department.\n\n3. The non-tenure track chart seems to have a more diverse selection of schools than the tenure-track chart. The tenure-track chart is a lot more 'top heavy' with mostly elite schools represented. This probably isn't surprising to you.\n\n4. This is only one \"snapshot\" in time of the statistics faculty at Stanford. I make no claims about the historical trends of the PhD programs that faculty members there have attended. There aren't a great deal of data points due to the smaller faculty size, and the data set is probably too small to make any generalizations. I just did this mostly because I enjoy collecting and seeing this data, and I put it out there in case this data is interesting for you too. Someone better at programming than me could probably do this sort of data collection on a much wider scale, which I think it would be very interesting to see. ", 
            "subreddit": "statistics", 
            "title": "Data on where Stanford Statistics Department faculty members received their PhDs", 
            "url": "https://www.reddit.com/r/statistics/comments/6zltzu/data_on_where_stanford_statistics_department/"
        }, 
        {
            "author": "Nanonaut", 
            "created_utc": 1505203542.0, 
            "domain": "self.statistics", 
            "id": "6zlm48", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 21, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zlm48/can_i_combine_probabilities_negative_predictive/", 
            "score": 2, 
            "selftext": "Imagine I have two tests. One can detect diabetes in general, but doesn't give information about the type of diabetes. It has a negative predictive value (NPV) of 85%. I have another test that can detect diabetes type II with an NPV of 80%. \n\nIf both tests are to be used, is there some way to combine these NPV probabilities in terms of diabetes in general? If both tests are negative, it seems like the NPV for \"diabetes\" would bit a bit higher than just 85%. But I'm not sure, since the 2nd test says nothing about type I diabetes. \n\nThis is a theoretical question so you can also imagine it being applied for something where test 1 tests for \"leukemia\" and test 2 tests for \"leukemia of the AML type\" - basically any pair of tests where the 2nd test is for a subgroup of the first.", 
            "subreddit": "statistics", 
            "title": "Can I combine probabilities (negative predictive values) in this scenario?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zlm48/can_i_combine_probabilities_negative_predictive/"
        }, 
        {
            "author": "hellyeaharya", 
            "created_utc": 1505177767.0, 
            "domain": "self.statistics", 
            "id": "6zjnkx", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zjnkx/project_in_advance_statistical_methods/", 
            "score": 0, 
            "selftext": "Hello guys, I am seeking some opinions for project in Advance statistical methods?\n\nThanks in advance ", 
            "subreddit": "statistics", 
            "title": "Project in Advance statistical methods", 
            "url": "https://www.reddit.com/r/statistics/comments/6zjnkx/project_in_advance_statistical_methods/"
        }, 
        {
            "author": "Optimizeplus", 
            "created_utc": 1505163531.0, 
            "domain": "self.statistics", 
            "id": "6zi9mm", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zi9mm/does_anyone_know_of_a_good_source_to_learn_about/", 
            "score": 8, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Does anyone know of a good source to learn about fuzzy hypothesis testing?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zi9mm/does_anyone_know_of_a_good_source_to_learn_about/"
        }, 
        {
            "author": "Adampowicz", 
            "created_utc": 1505158547.0, 
            "domain": "self.statistics", 
            "id": "6zhq94", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zhq94/confusion_regarding_pvalues_in_hypothesis_test/", 
            "score": 3, 
            "selftext": "So say I'm performing a hypothesis test, with 95% confidence, which gives me the test statistic 1,8 (P=0,0359).\n\nThis information can tell me that i am NOT 95% confident that \u00b5 \u2260 x . Because that requires the p-value to be less than 0,025\n\nBUT at the same time i can say that \"I am 95% confident that \u00b5 > x) , because for some reason the p-value just have to be less than 0,05\n\nDoesn't that contradict itself? I mean, if its \"most likely more than x\", would that not also mean that its most likely (even more likely) to \"not equal\" to x? Been looking around on internet for a decent explanation but haven't found much. (I don't know what to search for honestly) \n\nVery thankful for any explanation or link helping me out here! ", 
            "subreddit": "statistics", 
            "title": "Confusion regarding P-values in hypothesis test", 
            "url": "https://www.reddit.com/r/statistics/comments/6zhq94/confusion_regarding_pvalues_in_hypothesis_test/"
        }, 
        {
            "author": "RudePenguin", 
            "created_utc": 1505141931.0, 
            "domain": "self.statistics", 
            "id": "6zfwno", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zfwno/how_do_you_pick_which_schools_to_apply_to_for_an/", 
            "score": 0, 
            "selftext": "So I'm applying for Masters in statistics this upcoming cycle and was wondering what criteria I should use to pick schools to apply to.\n\nHere are my stats\n\nProgram: Statistics. Masters for the most part\n\nInterests: Sports statistics and geospatial data for the most part, but I'm generally open to various topics.\n\nUndergrad Institution: UNC Chapel Hill. Graduated in 2017\n\nUndergrad GPA: 3.792\n\nUndergrad Major: Statistics, Economics. Minor in Hispanic Literature and Cultures\n\nGRE: Have not taken yet, but I'm a good test taker and did well on SATs in HS.\n\nQuantitative Courses: Calc 1-3 (A), Discrete Math w/ proofs (A-), Differential Equations (A), Linear Algebra (B, the professor was the worst one I've ever had), Object Oriented Programming (A), Game Theory (A-), Deterministic Models (A), Probability (A), Econometrics (A-), Stochastic Modeling (A), Mathematical Statistics (B), R Programming (A), Time Series (A), Game Theory (A-).\n\nI'm missing Real Analysis, which could be a big deal.\n\nResearch: I had one research assistant position at Duke, but it was completely unrelated to statistics. I also wrote a Senior Economics Thesis that is fairly quantitatively heavy and got highest honors. \n\nLetters of Rec: Two econ professors, who should be good. 1 stats professor that I'm not too sure about", 
            "subreddit": "statistics", 
            "title": "How do you pick which schools to apply to for an MS in Stats?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zfwno/how_do_you_pick_which_schools_to_apply_to_for_an/"
        }, 
        {
            "author": "cookie_monster2017", 
            "created_utc": 1505120598.0, 
            "domain": "self.statistics", 
            "id": "6ze874", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ze874/how_would_i_justify_the_use_of_intervals_in/", 
            "score": 0, 
            "selftext": "I have the following intervals for classifying an individual as no evidence of nomophobia, at risk of nomophobia and nomophobia?\n\nas no evidence of nomophobia: 0 - 10\nat risk of nomophobia: 11-25\nand nomophobia: 26-50\n\nI used a pilot graph and then made these intervals based on evaluation. I checked what other people may have done, but it was clear that they just made some intervals up. Can someone recommend a way to justify these intervals?\n\nBy the way, an individual is given a ten-item questionnaire from a 6-likert scale from 0-5. Individuals then choose answers and it is then added up. From the adding up of scores, they are then placed in one of these intervals. Someone with an added up score of 23 is in the risk of nomophobia category. \n\nThanks.", 
            "subreddit": "statistics", 
            "title": "How would I justify the use of intervals in classifying an individual as no evidence of nomophobia, at risk of nomophobia and nomophobia?", 
            "url": "https://www.reddit.com/r/statistics/comments/6ze874/how_would_i_justify_the_use_of_intervals_in/"
        }, 
        {
            "author": "royalslayer", 
            "created_utc": 1505116457.0, 
            "domain": "self.statistics", 
            "id": "6ze02x", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ze02x/bayesian_updating_given_uniform_prior/", 
            "score": 1, 
            "selftext": "Hey everyone, I had a question from my grad class that confused me a bunch. I know about and have googled Bayesian updating with priors, but everything I found so far has shown me how to update given the result of a few coin tosses, for instance. In the question i have, that part is missing and it is confusing me about how to proceed. Here's the question:\n\nSuppose that a biased coin is thrown once. Estimate the probability of heads, theta, under the assumption of a uniform prior distribution on theta. (i.e. the pdf of prior distribution is g(theta) = 1 if 0 <= theta <= 1, and g(theta) = 0 otherwise.)\n\nAs you can see, it says the coin is tossed once but it doesn't say anything about whether it is heads or tails. If anyone can clarify what to do here, I would really appreciate it.", 
            "subreddit": "statistics", 
            "title": "Bayesian Updating Given Uniform Prior", 
            "url": "https://www.reddit.com/r/statistics/comments/6ze02x/bayesian_updating_given_uniform_prior/"
        }, 
        {
            "author": "mathmare", 
            "created_utc": 1505113206.0, 
            "domain": "theatlantic.com", 
            "id": "6zdt5z", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zdt5z/when_correlation_is_not_causation_but_something/", 
            "score": 70, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "When Correlation Is Not Causation, But Something Much More Screwy", 
            "url": "https://www.theatlantic.com/business/archive/2012/05/when-correlation-is-not-causation-but-something-much-more-screwy/256918/"
        }, 
        {
            "author": "summerwinterautumn", 
            "created_utc": 1505102652.0, 
            "domain": "self.statistics", 
            "id": "6zd3qh", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zd3qh/eli5_covariance_i_keep_getting_differing_ideas/", 
            "score": 9, 
            "selftext": "As far as I have been explained covariance (in terms of correlation and regression), the following have been explained to me:\n\nCovariance is how two variables change together - if one goes up, the other does too. If one goes down, the other does, too.\n\nCovariance is a third confounding factor. Like age when comparing gender to depression.\n\nCovariance is also known as a covariant, which doesn't tell me anything.\n\nI've googled, youtubed, and am still stuck. Sure, I may be a dumdum, but I would love anyone's opinion.", 
            "subreddit": "statistics", 
            "title": "ELI5: Covariance (I keep getting differing ideas)", 
            "url": "https://www.reddit.com/r/statistics/comments/6zd3qh/eli5_covariance_i_keep_getting_differing_ideas/"
        }, 
        {
            "author": "alex628", 
            "created_utc": 1505095181.0, 
            "domain": "self.statistics", 
            "id": "6zchr6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zchr6/data_scientist_marketing_journal_suggestions/", 
            "score": 7, 
            "selftext": "I got a position in marketing data science. I am curious what some of the best journals in this field are. I am only aware of the asa journals. I would love to hear of a good journal to keep me sharp.", 
            "subreddit": "statistics", 
            "title": "Data Scientist Marketing Journal Suggestions", 
            "url": "https://www.reddit.com/r/statistics/comments/6zchr6/data_scientist_marketing_journal_suggestions/"
        }, 
        {
            "author": "skilledninja", 
            "created_utc": 1505090596.0, 
            "domain": "self.statistics", 
            "id": "6zc3ea", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zc3ea/how_do_i_calculate_overall_probability_of/", 
            "score": 0, 
            "selftext": "I am supposed to calculate the following: \nThe overall probability of students graduating at each of the three universities.\nThe overall probability of students having a publication at each of the three universities.\n\nJ.W. Blake\t2560\t2304\t876\nK.R. Cunningham\t599\t599\t300\nR.H. Doughty\t472\t425\t162\nL.M. Edwards\t1027\t904\t434\nW.H. Greiner\t138\t121\t44\nI.D. Jackson\t817\t809\t307\nO.P. Lawson\t2218\t2018\t726\n\nThe first number after the name is standard for the # of students taught, the second one is students graduated and the third one is for publications.\n\n\nI am typing the following information into the cell sheet =prob(c2:c15) but I keep getting an error. Can you help?", 
            "subreddit": "statistics", 
            "title": "How do I calculate overall probability of students in an excel sheet? I keep getting an error....", 
            "url": "https://www.reddit.com/r/statistics/comments/6zc3ea/how_do_i_calculate_overall_probability_of/"
        }, 
        {
            "author": "dividerall", 
            "created_utc": 1505089659.0, 
            "domain": "self.statistics", 
            "id": "6zc0j7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zc0j7/whats_wrong_with_using_just_trend_for_time_series/", 
            "score": 1, 
            "selftext": "Been a while since I took a time series class... So here goes.\n\nI remember in my old time series assignments I would remove trend and seasonality, do the model fitting and what not, then add the trend and seasonality back to see my results.\n\nFor my current analysis, I want to use one time series to predict another... one time series is popularity of a food item in one forum versus popularity of the same food item based on google trends.\n\nSo for example, one clothing item is a particular kind of frozen dessert. Obviously, there's seasonality here, so I remove that, but I'm actually interested in the trend - for this data I found that the google trends time series is fairly similar to the forum time series, just lagged a year later.\n\n\nBonus points if you have suggestions on how I can use one series to predict another...", 
            "subreddit": "statistics", 
            "title": "What's wrong with using just trend for time series analysis?", 
            "url": "https://www.reddit.com/r/statistics/comments/6zc0j7/whats_wrong_with_using_just_trend_for_time_series/"
        }, 
        {
            "author": "Eldwod", 
            "created_utc": 1505068510.0, 
            "domain": "self.statistics", 
            "id": "6z9yt7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6z9yt7/other_than_machinedeep_learning_what_cs_courses/", 
            "score": 16, 
            "selftext": "Barring introductory programming/data structures, obviously. \n\nI'm interested in doing applied statistics and machine learning, and thus I plan on taking CS courses in machine learning/deep learning. I'm still only a Sophomore in college, so I have plenty of time to do more CS classes. A lot of them seem at least somewhat interesting, but I'm not sure which ones are the best to take (both for direct usefulness in stats, and looking good to employers).\nThe CS courses are roughly broken down into three categories:\nTheoretical, e.g. Theory of Computation/Algorithms: Seems interesting, though they seem less practical (especially the theory of computation).\n\nSystems, e.g. Intro to Systems, Distributed, Operating, Security : These seem the most directly applied, though they don't seem to be directly useful for anything stats related. They seem the least interesting, and are by far the largest time commitment (20-30 hrs/week).\n\nOther A.I. Courses, e.g. Computer Vision, Natural Language Processing: Seem semi-related to ML, but I'm not sure if the connection is strong enough to warrant taking them over others.\n\nI'm not entirely sure what I should take, any thoughts?", 
            "subreddit": "statistics", 
            "title": "Other than Machine/Deep learning, what CS courses are best for applied stats/data analysis?", 
            "url": "https://www.reddit.com/r/statistics/comments/6z9yt7/other_than_machinedeep_learning_what_cs_courses/"
        }, 
        {
            "author": "redditmaster21", 
            "created_utc": 1505058157.0, 
            "domain": "self.statistics", 
            "id": "6z8x0n", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "College Advice", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6z8x0n/undergrad_advice_statsecons_or_statscompsci/", 
            "score": 0, 
            "selftext": "I think I have seen something like this before, but I don't think the advice really applies to me.\n\nBasically, I'm majoring in stats. I'm currently thinking of either picking up a second major in econs or minoring in CompSci. You might ask, why major in one or minor in the other? Stats graduates often perform much better when they are proficient in a programming language (Python, R, SAS etc.) but I feel like the CompSci major at a university level focuses on more of the theory parts, such as understanding the workings of computer processes itself, and not learning languages which I would be more interested in.\n\nLong story short, I'm leaning more towards taking up the Stats / Econs double major (which would greatly benefit me as I'm planning to go into actuarial science in the future, and self-learn the aforementioned programming languages) as opposed to Stats / CompSci. But then again, I'm open to all suggestions, so pls send them my way.\n\nThanks in advance guys, this community is awesome!", 
            "subreddit": "statistics", 
            "title": "Undergrad advice: Stats/Econs or Stats/CompSci?", 
            "url": "https://www.reddit.com/r/statistics/comments/6z8x0n/undergrad_advice_statsecons_or_statscompsci/"
        }, 
        {
            "author": "LetsEndSuffering", 
            "created_utc": 1505014199.0, 
            "domain": "self.statistics", 
            "id": "6z64y9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6z64y9/how_helpful_would_a_statistics_masters_degree_be/", 
            "score": 0, 
            "selftext": "I will be graduating with a B.S degree in Finance from a no-name school and will move to live in San Francisco forever after graduation (parents live there). How helpful would a masters degree in statistics be for me to land jobs in BI, product analysis, market research analysis, risk analysis, corporate finance ,etc?  ", 
            "subreddit": "statistics", 
            "title": "how helpful would a statistics Masters degree be in landing jobs?", 
            "url": "https://www.reddit.com/r/statistics/comments/6z64y9/how_helpful_would_a_statistics_masters_degree_be/"
        }, 
        {
            "author": "Chicagodivemaster", 
            "created_utc": 1505011137.0, 
            "domain": "self.statistics", 
            "id": "6z5wpn", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6z5wpn/is_there_a_best_practice_for_selecting_random/", 
            "score": 12, 
            "selftext": "I've learned a bit about mixed models recently, but one subject that still eludes me is the \"best\" way to determine what random effects to include. I have a mountain of variables that *could* improve the model I'm working with, but I don't want to do anything until I have a better idea of what to do with them. Right now the only thing I've included as a random intercept and slope is subject. I've gathered that that's an appropriate thing to do in a repeated measure design. \n\nDoes it make sense to construct a model from the bottom up, testing variables one at a time, rejecting ones that fail to improve the model? Do I test every permutation of model and select the one that best balances accuracy and complexity? Is there a package for R that helps test random effects? Finally, are there any major pitfalls that I should know to avoid before I start this endeavor?\n\nthanks!", 
            "subreddit": "statistics", 
            "title": "Is there a \"best practice\" for selecting random effects variables in a mixed model?", 
            "url": "https://www.reddit.com/r/statistics/comments/6z5wpn/is_there_a_best_practice_for_selecting_random/"
        }, 
        {
            "author": "wernz", 
            "created_utc": 1504995084.0, 
            "domain": "bl.ocks.org", 
            "id": "6z4l1v", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6z4l1v/learning_the_normal_distribution_through/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Learning the normal distribution through interactive sampling", 
            "url": "http://bl.ocks.org/wernersa/135f652c2f82125c2167d508dfba9685"
        }, 
        {
            "author": "Spickay", 
            "created_utc": 1504986567.0, 
            "domain": "self.statistics", 
            "id": "6z3spk", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6z3spk/are_the_linear_regression_coefficients_for_a/", 
            "score": 14, 
            "selftext": "from what I understand, the coefficients in linear regression are calculated using OLS and the fixed effect estimates from a mixed model are determined using ML (which gives the same results as OLS depending on the assumptions), so assuming I'm using the same dataset and specifying the correct model then I should be getting the same values for the coefficients right?\n", 
            "subreddit": "statistics", 
            "title": "are the linear regression coefficients for a factor in a two way anova identical (or related in anyway) to the fixed effect estimates for that same factor in a mixed model?", 
            "url": "https://www.reddit.com/r/statistics/comments/6z3spk/are_the_linear_regression_coefficients_for_a/"
        }, 
        {
            "author": "pragmatic-man", 
            "created_utc": 1504986038.0, 
            "domain": "self.statistics", 
            "id": "6z3qvi", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6z3qvi/multicollinearity_and_interaction_terms/", 
            "score": 1, 
            "selftext": "I have a few models where I am testing for moderation by using interaction terms, where y=x1+x2+x1*x2. Unfortunately, the interaction term is showing a negative sign. This appears to be because of multicollinearity with x1 and x2. When I run a model just with the interaction term the sign is positive as predicted. I have already centered the variables by standardizing. What are some strategies for dealing with multicollinearity? ", 
            "subreddit": "statistics", 
            "title": "multicollinearity and interaction terms", 
            "url": "https://www.reddit.com/r/statistics/comments/6z3qvi/multicollinearity_and_interaction_terms/"
        }, 
        {
            "author": "cloud9ineteen", 
            "created_utc": 1504930449.0, 
            "domain": "self.statistics", 
            "id": "6yzrup", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yzrup/comparing_whether_a_second_test_is_as_good_as_the/", 
            "score": 1, 
            "selftext": "If I have two tests, with test A being treated as the gold standard aka the truth, and I have the number of true positives, true negatives, false positives, and false negatives, how do I figure out how good the second test is in comparison to the first test aka the gold standard?", 
            "subreddit": "statistics", 
            "title": "Comparing whether a second test is as good as the first one", 
            "url": "https://www.reddit.com/r/statistics/comments/6yzrup/comparing_whether_a_second_test_is_as_good_as_the/"
        }, 
        {
            "author": "Spickay", 
            "created_utc": 1504928122.0, 
            "domain": "self.statistics", 
            "id": "6yzlx0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yzlx0/resources_for_understanding_the_goals_and/", 
            "score": 23, 
            "selftext": "Hi all, would anyone have any recommended books, lectures or anything of the sort to getting into and understanding what linear and mixed effect models (emphasis on the mixed) are?", 
            "subreddit": "statistics", 
            "title": "Resources for understanding the goals and reasoning behind linear and mixed effect models for someone with basic stats background", 
            "url": "https://www.reddit.com/r/statistics/comments/6yzlx0/resources_for_understanding_the_goals_and/"
        }, 
        {
            "author": "alex628", 
            "created_utc": 1504923747.0, 
            "domain": "self.statistics", 
            "id": "6yza20", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yza20/alternative_names_to_risk_risk_difference_and/", 
            "score": 2, 
            "selftext": "I am curious if anyone has seen alternative names for risk, risk difference, and odds ratio when looking at data that takes the 2x2 table form. For me it seems the \"risk\" does not capture the generality of the formula. \"Risk\" makes sense in biostats, but is an odd word for marketing. It seems that a more appropriate word would be rate. Has anyone seen alternative terms?", 
            "subreddit": "statistics", 
            "title": "Alternative names to risk, risk difference, and odds ratio", 
            "url": "https://www.reddit.com/r/statistics/comments/6yza20/alternative_names_to_risk_risk_difference_and/"
        }, 
        {
            "author": "ThilebanTheEngineer", 
            "created_utc": 1504914615.0, 
            "domain": "youtube.com", 
            "id": "6yyjg0", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": {
                "oembed": {
                    "author_name": "Engineer Thileban Explains", 
                    "author_url": "https://www.youtube.com/channel/UCcnz9s70vXWoqErYJgsTzCA", 
                    "description": "Uploaded by Engineer Thileban Explains on 2017-09-08.", 
                    "height": 338, 
                    "html": "<iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F9i6Vy9B-nbg%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D9i6Vy9B-nbg&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F9i6Vy9B-nbg%2Fhqdefault.jpg&key=522baf40bd3911e08d854040d3dc5c07&type=text%2Fhtml&schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "provider_name": "YouTube", 
                    "provider_url": "https://www.youtube.com/", 
                    "thumbnail_height": 360, 
                    "thumbnail_url": "https://i.ytimg.com/vi/9i6Vy9B-nbg/hqdefault.jpg", 
                    "thumbnail_width": 480, 
                    "title": "Elementary Statistics : Probability Rules : Treatment of Experimental Data #3", 
                    "type": "video", 
                    "url": "http://www.youtube.com/watch?v=9i6Vy9B-nbg", 
                    "version": "1.0", 
                    "width": 600
                }, 
                "type": "youtube.com"
            }, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yyjg0/elementary_statistics_probability_rules_treatment/", 
            "score": 1, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Elementary Statistics : Probability Rules : Treatment of Experimental Da...", 
            "url": "https://www.youtube.com/attribution_link?a=wplVf-9Ro5A&u=%2Fwatch%3Fv%3D9i6Vy9B-nbg%26feature%3Dshare"
        }, 
        {
            "author": "rafters08", 
            "created_utc": 1504914508.0, 
            "domain": "self.statistics", 
            "id": "6yyj3p", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yyj3p/unbalanced_factorial_designanova/", 
            "score": 3, 
            "selftext": "I have a 2x2x2 factorial design. However,  I am missing a treatment data altogether.  My study is store x type of fat x presence of stirring on the amount of crystals. I am missing the data for a fat type from one store. How do I analyze this data? I cannot redo the experiment. I have used a 3-way ANOVA for the other balanced experiment. There is no literature data that I can assume. \n\nIf I were to individually compare the each fat type (in this case 3, since one is missing) by one way ANOVA? or arrange the data as 6 data sets and analyze the entire set by one-WAY ANOVA ?\nStore1-fat2-stirring1\nStore1-fat2-stirring3\nStore2-fat1-stirring1\nStore2-fat2-stirring2\nStore3-fat1-stirring1\nStore3-fat2-stirring2\nI am missing Store1-fat1 data", 
            "subreddit": "statistics", 
            "title": "Unbalanced factorial design-ANOVA?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yyj3p/unbalanced_factorial_designanova/"
        }, 
        {
            "author": "JavascriptFanboy", 
            "created_utc": 1504905742.0, 
            "domain": "self.statistics", 
            "id": "6yxp2g", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yxp2g/two_different_groups_have_different_shaped/", 
            "score": 7, 
            "selftext": "So today I gathered a whole lot of data. I have two groups and I want to determine if those two sets of data are significantly different from each other. Based on the experimental design, I felt that t-test would be most appropriate. So, I checked the \"rules\" before using t-test and as it's probably well known, \"A t-test is most commonly applied when the test statistic would follow a normal distribution\". However, in my case, the data in one group conforms to normal distribution, while the data in other does not. As such, I believe I am not to use the t-test. What is the correct approach, then? ", 
            "subreddit": "statistics", 
            "title": "Two different groups have different shaped distributions - which test to use?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yxp2g/two_different_groups_have_different_shaped/"
        }, 
        {
            "author": "torontolife997", 
            "created_utc": 1504885805.0, 
            "domain": "self.statistics", 
            "id": "6yvlry", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yvlry/assistanceadvice_needed_for_oneway_anova/", 
            "score": 3, 
            "selftext": "Hello,\n\nI've been having trouble figuring out how to analyze the data I have collected in an experiment. I can't find any instance suggesting how to treat replicates in my data set.\n\nIn brief, I have a continuous output variable, four treatment groups, and four blocks. Within each block there is a full/balanced data set (3, 3, 3, 3 or else 4, 4, 4, 4 measurements). The problem I face is how to deal with these replicate measurements. I note that it's very straightforward when there's only 1 measurement per treatment per block.\n\nI think it's reasonable to go ahead with calculating the mean of the replicates and then using these means to 'pretend' there was only one measurement each. However, I'm wondering if there is a better way of doing this that makes better use of all the data I've collected? Is there a fair way I can make my F-test degrees of freedom go from F(3,9) to F(3,49) or something like that?\n\nThanks so much.", 
            "subreddit": "statistics", 
            "title": "Assistance/advice needed for one-way ANOVA, randomized block design, with multiple measures per treatment in each block", 
            "url": "https://www.reddit.com/r/statistics/comments/6yvlry/assistanceadvice_needed_for_oneway_anova/"
        }, 
        {
            "author": "chainor", 
            "created_utc": 1504871620.0, 
            "domain": "self.statistics", 
            "id": "6yua9n", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yua9n/what_is_a_term_for_the_data_that_stem_from_the/", 
            "score": 0, 
            "selftext": "I am going to analyze data over a period of two years. I am planning to not implement the first 6 months, as the particular field I am studying is just fielded, and I dont want my data to be affected by the initial low experience level. How can I describe this? Are there better academic terms to use?", 
            "subreddit": "statistics", 
            "title": "What is a term for the data that stem from the first phase of fielding something?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yua9n/what_is_a_term_for_the_data_that_stem_from_the/"
        }, 
        {
            "author": "atomofconsumption", 
            "created_utc": 1504838668.0, 
            "domain": "self.statistics", 
            "id": "6ys25s", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ys25s/using_bootstraps_to_get_cvs_in_sas_r_or_stata/", 
            "score": 2, 
            "selftext": "I have survey data with 1000 bootstraps already attached.\n\nSo far I have extremely complex programs in SAS to get the CVs of the **change** between the means of two monthly estimates.\n\nI know there's proc surveymeans but that doesn't seem to work for proportions.\n\n**Anyone know if R or STATA is better than SAS for conducting these types of bootstrap tests?**", 
            "subreddit": "statistics", 
            "title": "Using bootstraps to get CVs in SAS, R, or STATA, which is best?", 
            "url": "https://www.reddit.com/r/statistics/comments/6ys25s/using_bootstraps_to_get_cvs_in_sas_r_or_stata/"
        }, 
        {
            "author": "nellasbean", 
            "created_utc": 1504821442.0, 
            "domain": "self.statistics", 
            "id": "6yqgd4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yqgd4/probability_theory_help/", 
            "score": 1, 
            "selftext": "There's another post on here that sounds similar to my situation. I got into my PhD program for biostats through unconventional means (molecular biology background, but I'm decent at programming and am technically-oriented). So now I'm taking an intro to theory of statistics course and it feels like the carpet has been whipped out from underneath me. I did a minor in math in undergrad (which was, now, several years ago), I've always been ok at math, but these probability theory questions are curve balls from every direction. \n\nWe are using Introduction to Probability by Blitzstein and Hwang. I've been doing all the reading, watching his lectures on Youtube, and trying to go through and prove to myself the concepts that he shows in the text. I can mostly follow when a problem is explained retrospectively, but I have trouble applying these concepts to isomorphic problems. I've spoken to the professor and he says he'll sit with me during office hours to go through questions I have, but we go so fast that I don't think this will be enough. I've already failed the first two quizzes of the class, and there is a screening exam next summer that will test my understanding of these things.\n\nDo you know of resources online that have practice questions with solutions that break down why a problem was solved in a certain way? I think I just need to practice the same kinds of questions over and over until it sticks. I'll do some Googling on my own, but I figured you all would probably know better than I.\n\nAny help is appreciated. ", 
            "subreddit": "statistics", 
            "title": "Probability theory help", 
            "url": "https://www.reddit.com/r/statistics/comments/6yqgd4/probability_theory_help/"
        }, 
        {
            "author": "lc929", 
            "created_utc": 1504821156.0, 
            "domain": "self.statistics", 
            "id": "6yqf99", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yqf99/is_there_a_way_to_generate_pvalue_given_two_means/", 
            "score": 6, 
            "selftext": "I don't have access to the raw data - just the mean with a 95% low/high confidence internval. Is there a way I can generate a p-value or some sort of metric like the p-value?", 
            "subreddit": "statistics", 
            "title": "Is there a way to generate p-value given two means, along with their 95% confidence intervals?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yqf99/is_there_a_way_to_generate_pvalue_given_two_means/"
        }, 
        {
            "author": "DickDraper", 
            "created_utc": 1504815546.0, 
            "domain": "self.statistics", 
            "id": "6yptt5", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yptt5/i_need_help_determining_if_the_following_example/", 
            "score": 1, 
            "selftext": "I have come across this example and I am having difficulty determining. This is not for homework, but is more a philosophical question about the nature of designs. If you have an answer I would love a source so I can read up on it. I have found myself enjoying philosophy behind statistics more than anything else. Here it goes. \n\n\nIV: Racial non-minorities vs racial minorities; Gender; Time\nCV: Years of having been a therapist \nDV: Depression reduction in symptoms\n\n\nDesign: people are assigned to therapists that either match or not match their gender and racial status, with the possibility of 4 groups. \n\n\nGender Match and Race Match\nGender Match and Race mismatch\nGender Mismatch and Race Match\nGender Mismatch and Race Mismatch\n\n\nThey are given a pre and post test and years of having been a therapist is held constant. \n\nI am scratching my head. My gut tells me non-experimental but I could be wrong. \n\nAgain I appreciate you taking the time to read. And sources would be much appreciated as I am very much intrigued. \n", 
            "subreddit": "statistics", 
            "title": "I need help determining if the following example is Experimental, or quasi-experimental", 
            "url": "https://www.reddit.com/r/statistics/comments/6yptt5/i_need_help_determining_if_the_following_example/"
        }, 
        {
            "author": "Hellkyte", 
            "created_utc": 1504805177.0, 
            "domain": "self.statistics", 
            "id": "6yop27", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yop27/jmp_vs_minitab_for_scripting/", 
            "score": 1, 
            "selftext": "I need to script some routine statistical reporting for some control charts.  Anyone have experience with both of these scripting languages and have an opinion on which they prefer?  I vaguely prefer Minitab in this case because I find it's quality control stuff a bit more robust, but I have other business reasons for considering JMP.\n\nThe script won't be doing anything too crazy, just pull data from a server excel file which will also include constants for limits/point estimators\n\nI have a decent amount of experience coding.\n\nAlso, for business reasons R and Python are not alternatives.", 
            "subreddit": "statistics", 
            "title": "JMP vs Minitab (for scripting)", 
            "url": "https://www.reddit.com/r/statistics/comments/6yop27/jmp_vs_minitab_for_scripting/"
        }, 
        {
            "author": "skammaks", 
            "created_utc": 1504803959.0, 
            "domain": "self.statistics", 
            "id": "6yok72", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 44, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yok72/i_am_not_smart_enough_for_graduate_school_in/", 
            "score": 32, 
            "selftext": "This week I\u2019ve had my two graduate courses (Applied Statistics \u2013 using the book: Introduction to the Practice of Statistics by Moore, McCabe, Craig) and (Probability Distributions \u2013 using the book: Introduction to Mathematical Statistics by Hogg, Allen, Craig). I\u2019m a bit worried about the Probability Distributions class since I have no experience with mathematical proofs. In undergraduate, I\u2019ve done a minor in Mathematics (Calculus 1-3, Linear Algebra, Differential Equations, Numerical Analysis, Probability and Statistics), but none of it was taught with proofs, only applications. Now I\u2019m worried about graduate school because the professor gave us 3/5 proof-based questions for homework. One question about unions and intersections and limits of nondecreasing and nonincreasing sets. Another question about a set integrated over a sample space in 3-dimensions (triple integral with spherical coordinates). Another question about proving the inclusion-exclusion formula for the union of three sets and also a generalized version for a finite amount of sets. One question on probability and a final one on a proof of the \u201ccontinuity theorem of probability\u201d. \n\n\nIt\u2019s hard for me. I get lost and confused with all of the notation, especially with the generalized inclusion-exclusion formula. Professor is not very helpful, he just said to use induction, but I was never really taught all that. I tried to look this up online how to do the problem and there are always more than one solutions or no solutions at all. I am ashamed to say that it took me over 10 hours to understand and prove Demorgan\u2019s Laws. Maybe I don\u2019t deserve to do a master degree in Applied Statistics. I worry that if it took me so long to understand something simple like Demorgan\u2019s Laws then it will be next to impossible to understand complicated topics in statistics. I don\u2019t want to fail out of graduate school and disappoint my parents and waste their money. I am sad thinking about this. \n", 
            "subreddit": "statistics", 
            "title": "I am not smart enough for graduate school in Applied Statistics", 
            "url": "https://www.reddit.com/r/statistics/comments/6yok72/i_am_not_smart_enough_for_graduate_school_in/"
        }, 
        {
            "author": "YellowMaterCustard", 
            "created_utc": 1504801735.0, 
            "domain": "self.statistics", 
            "id": "6yoben", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yoben/where_can_i_find_the_most_comprehensive_set_of/", 
            "score": 1, 
            "selftext": "\nI am looking to do a statistical analysis on Deaths going back as far as possible, with respect to day, time, age of death, and cause.  Does anyone know where I would find such data?", 
            "subreddit": "statistics", 
            "title": "Where can I find the most comprehensive set of data on Deaths Demographics?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yoben/where_can_i_find_the_most_comprehensive_set_of/"
        }, 
        {
            "author": "Kimchiandmilk", 
            "created_utc": 1504794947.0, 
            "domain": "self.statistics", 
            "id": "6ynl46", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ynl46/proper_test_to_compare_two_data_sets_of_center_of/", 
            "score": 1, 
            "selftext": "I am looking for the best analysis for comparing two different devices that record measures related to balance (center of pressure sway) over time. \n\nEssentially I would have two data sets in say a 30 second period with coordinates, and I want to see how each coordinate correlates to each other but over that entire period. I know a Pearson correlation can be used, but I feel like I've heard of a rather specific test that can be used to compare two large data sets that compare point by point over time. \n\nSo for example data point 1 in device A vs point 1 in point B at say .01 seconds, 2 to 2 at .02 seconds, 3 to 3 at .03 seconds, etc. ", 
            "subreddit": "statistics", 
            "title": "Proper test to compare two data sets of center of pressure distribution over time?", 
            "url": "https://www.reddit.com/r/statistics/comments/6ynl46/proper_test_to_compare_two_data_sets_of_center_of/"
        }, 
        {
            "author": "j_complex", 
            "created_utc": 1504771968.0, 
            "domain": "self.statistics", 
            "id": "6ylvo8", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 51, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ylvo8/if_correlation_doesnt_imply_causation_how_can_you/", 
            "score": 16, 
            "selftext": "I did a first year stats course a few years ago, didn't find it too hard or put in too much effort (I'm a maths major so it's a similar style of thinking), but I can't remember much at all from it. \n\nHelp please?", 
            "subreddit": "statistics", 
            "title": "If correlation doesn't imply causation, how can you determine that the correlation does imply causation?", 
            "url": "https://www.reddit.com/r/statistics/comments/6ylvo8/if_correlation_doesnt_imply_causation_how_can_you/"
        }, 
        {
            "author": "trippyelephantx", 
            "created_utc": 1504760698.0, 
            "domain": "self.statistics", 
            "id": "6yl5nx", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yl5nx/extra_course_work_for_ms_in_statistics/", 
            "score": 1, 
            "selftext": "I'm currently a senior Finance major about to graduate in the spring. I want to go on into a graduate degree in statistics eventually, however I don't think I have the mathematical background to get into any good programs. I have 2 years of programming experience during undergrad, as I was a double major for a while, but somehow I have not taken linear algebra or any advanced calculus. Only discrete math and a basic statistics course. My question is, in order to make my application more acceptable, should I enroll in some of these classes I need at a University after I graduate, and before I prepare my application? Or will this not have an effect? Also, are there specific statistics programs that best prepare someone for the field of data science? I've researched the top schools for statistics, but don't exactly know how well their programs translate into the real world. Thanks!", 
            "subreddit": "statistics", 
            "title": "Extra Course Work for MS in Statistics?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yl5nx/extra_course_work_for_ms_in_statistics/"
        }, 
        {
            "author": "LordTumTum", 
            "created_utc": 1504753265.0, 
            "domain": "self.statistics", 
            "id": "6ykk88", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ykk88/supplemental_materials_for_text/", 
            "score": 1, 
            "selftext": "I'm taking a course using the text \"probability and statistics: the science of uncertianty\" by evans and rosenthal and i find that the text lacks explanation. Can anyone suggest supplimental materials to help fill in the holes? \n\nDuring calculus i really enjoyed using pauls notes from lamar uni. If you know of and could share something similar i would really appreciate it! \n\nYour's, at least until shit gets weird, trully\n\nL to the tum tum ", 
            "subreddit": "statistics", 
            "title": "Supplemental materials for text", 
            "url": "https://www.reddit.com/r/statistics/comments/6ykk88/supplemental_materials_for_text/"
        }, 
        {
            "author": "DominatingDrew", 
            "created_utc": 1504747191.0, 
            "domain": "self.statistics", 
            "id": "6yk0eg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 13, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yk0eg/how_long_do_you_have_to_go_without_causing_a_car/", 
            "score": 7, 
            "selftext": "Basically if a 16 year old got their license yesterday, they (hopefully) won't have ever caused an accident.  However, their lack of accidents is just because they don't have enough \"data points\" yet rather than their superior driving skills.  At what age do you have enough driving history to be a better than average driver if you've never caused an accident?\n\nI don't know much about statistics, but I would think it could be answered in two ways.  One would be once you pass the average time between accidents.  \n\nAnother answer could be once you pass the average age for people's first accident.  Assuming you become a better driver with age you might at the current moment still be worse than the \"average\" driver but on track to be a better \"lifetime driver\" if that makes sense.  ", 
            "subreddit": "statistics", 
            "title": "How long do you have to go without causing a car accident to be a statistically better than average driver?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yk0eg/how_long_do_you_have_to_go_without_causing_a_car/"
        }, 
        {
            "author": "Colossal_Eh", 
            "created_utc": 1504741132.0, 
            "domain": "self.statistics", 
            "id": "6yjg2z", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 18, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yjg2z/settle_an_argument/", 
            "score": 1, 
            "selftext": "I'm going to preface this by mentioning that I know very little statistics. Obviously arguing about whether or not god exists is pointless, but my friend made a claim that bothered me so I thought I'd post it. He said that all religions are equally likely, including the absence of a god. Is that accurate? We were assuming infinite religions, not just the ones practiced currently, if that makes any difference.", 
            "subreddit": "statistics", 
            "title": "Settle an argument?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yjg2z/settle_an_argument/"
        }, 
        {
            "author": "SacoETrampa", 
            "created_utc": 1504735920.0, 
            "domain": "self.statistics", 
            "id": "6yixkb", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yixkb/how_to_choose_parametric_family_of_a_parametric/", 
            "score": 4, 
            "selftext": "Hey everyone, I'm currently in Puerto Rico holding on for dear life. I have been reading about survival analysis because it's the perfect tool for a problem in tackling currently. \n\n\nInitially, I thought a Cox proportional hazards model would be great because it seems to be the favorite model out there. However, I intend to use the results to project currently censored individuals to the future and that is not possible with Cox  (since baseline is never really specified).\n\n\nBecause I am interested in inputing covariates getting output \"probability of failure at time t\", it seems a parametric survival model will be optimal. Most of the literature I have found indicates that a simple exponential distribution works remarkably well for tha vast majority of uses, so I'm inclined to try that. However, for my precise problem, there will be competing risks and I believe that they work fundamentally different across time. \n\n\nA bit of background on the issue. I am trying to estimate probability of departure from a company for each individual in it given a vector of covariates which include: division, wage, age, position...The competing risks are that a person can leave by will, be booted or retire. I expect retirement hazard specifically to be very low at the beginning of tenure and to spike up at the end of the span. \n\n\nWhat parametric family works for what type of hazard ? Is there a systematic way of determining the specification based off the data, or is it just intuition ? \n\n\nIm sorry if this is stupid, I started reading about survival analysis yesterday. Also, sorry for the spelling mistakes, I have no electricity and typed this on my phone. \n\n\nThanks a lot for your help. Hope everyone affected by Irma stays safe. \n\n", 
            "subreddit": "statistics", 
            "title": "How to choose parametric family of a parametric survival model's baseline hazard?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yixkb/how_to_choose_parametric_family_of_a_parametric/"
        }, 
        {
            "author": "Mr_Madao", 
            "created_utc": 1504726664.0, 
            "domain": "self.statistics", 
            "id": "6yhyci", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 22, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yhyci/advice_for_recent_grad_who_is_bad_at_programming/", 
            "score": 12, 
            "selftext": "I just recently got my Bachelors in Stats and most of the stat related jobs I can find require heavy programming abilities. Unfortunately besides having taken not that many programming classes, I'm in general just bad at programming. Any advice for any other jobs or careers I should look into?", 
            "subreddit": "statistics", 
            "title": "Advice for recent grad who is bad at programming", 
            "url": "https://www.reddit.com/r/statistics/comments/6yhyci/advice_for_recent_grad_who_is_bad_at_programming/"
        }, 
        {
            "author": "PsychShake", 
            "created_utc": 1504724333.0, 
            "domain": "self.statistics", 
            "id": "6yhp18", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yhp18/does_my_method_work_for_controlling_variables_in/", 
            "score": 1, 
            "selftext": "Say you have IQ and Academic Achievement (AA) and you create a partial correlation with both of those variables and covary them with Socioeconomic Status (SES). \n\nFrom this video ( https://www.youtube.com/watch?v=GUNPXLRk_60 ), I learned that, if you want to graph that partial regression in SPSS, then you can use the residuals of two regressions, IQ with SES and AA with SES. \n\nThus, if I am understanding this correctly, the residual of each of those regressions is the value of the dependent variables (IQ and AA) controlled for SES, correct?\n\nThis leads me to my current question. Can you control for variables in a linear discriminant analysis? \nIn SPSS, there is no option to, which makes sense because of the nature of the classification function. However, what if I regressed each of the individual dependent variables in the LDA on typical covariates, such as age, sex, and race, then used those residuals of the dependent variables for the discriminant function? \n\nWould that technically control for the variables or is there some problem with this that I am not seeing?", 
            "subreddit": "statistics", 
            "title": "Does my method work for controlling variables in a Linear Discriminant Analysis?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yhp18/does_my_method_work_for_controlling_variables_in/"
        }, 
        {
            "author": "lucky94", 
            "created_utc": 1504720197.0, 
            "domain": "luckytoilet.wordpress.com", 
            "id": "6yh8nz", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 11, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yh8nz/whats_the_difference_between_mathematics_and/", 
            "score": 0, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "What's the difference between Mathematics and Statistics?", 
            "url": "https://luckytoilet.wordpress.com/2017/09/06/whats-the-difference-between-mathematics-and-statistics/"
        }, 
        {
            "author": "IM_BOAT", 
            "created_utc": 1504708516.0, 
            "domain": "self.statistics", 
            "id": "6yfzo1", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 12, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yfzo1/can_someone_eli5_on_the_difference_between/", 
            "score": 13, 
            "selftext": "I think I understand causation pretty good. It's when one variable directly impacts the outcome of another variable? \n\nIn some text I find says association and correlation mean the same thing. But the way my professor was talking about them in class made it seem as if they have different meaning. \n\nPlease eli5 thanks in advance. ", 
            "subreddit": "statistics", 
            "title": "Can someone ELI5 on the difference between Association, Causation, and Correlation?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yfzo1/can_someone_eli5_on_the_difference_between/"
        }, 
        {
            "author": "Tongciu", 
            "created_utc": 1504707466.0, 
            "domain": "self.statistics", 
            "id": "6yfvz9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yfvz9/how_can_i_determine_whether_or_not_to_apply_the/", 
            "score": 1, 
            "selftext": "If the sample size n is 20,and the sampling distribution X~Bin(n=20,p=0.8),need to know P(x>14).So I can't calculate the sample proportion and use the Z table to get the probability?CLT should only be applied when np and np(1-p) all larger than 5?What if the sample distribution is uniformed or something else?I'm a little bit confused. ", 
            "subreddit": "statistics", 
            "title": "How can I determine whether or not to apply the central limit theorem?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yfvz9/how_can_i_determine_whether_or_not_to_apply_the/"
        }, 
        {
            "author": "Thinex", 
            "created_utc": 1504689576.0, 
            "domain": "self.statistics", 
            "id": "6yeju7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 19, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yeju7/help_with_deciding_on_what_statistical_test_to_use/", 
            "score": 5, 
            "selftext": "Hello /r/statistics,\n\nI am currently doing my internship at a company, and they have asked me to determine the influence of various variable parameters in the productionprocess on the particle-size of the end-product.\nUnfortunatly, during my education statistics has not really been a widely covered subject, which is why I do not have a clue on which statistical test to use.\nCan anyone suggest which statistical test is most suitable for my project and if possible, where I can find more information regarding this test so that I can educate myself on it.\n\nThanks in advance!", 
            "subreddit": "statistics", 
            "title": "Help with deciding on what statistical test to use", 
            "url": "https://www.reddit.com/r/statistics/comments/6yeju7/help_with_deciding_on_what_statistical_test_to_use/"
        }, 
        {
            "author": "michiel195", 
            "created_utc": 1504675370.0, 
            "domain": "self.statistics", 
            "id": "6ydogt", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ydogt/alternatives_to_auc/", 
            "score": 7, 
            "selftext": "Hi all,\n\nI am currently working on a model trying to reproduce the presence/absence pattern of an organism on a landscape. My model produces a raster of population growth rate, strictly positive. I validate the model with a raster of presence/absence, binary, using AUC. \n\nMy issue is is that AUC doesn't tell the whole story: the model performs okay, but reproduces the pattern on the landscape very well, except in a single discrete area on the landscape, which drives down AUC. \n\nI of course will be reporting the mediocre AUC value, but I'd like to see if there's any alternative validation measures I can report as well. For example, is there a metric that summarizes pattern fidelity? \n\nThanks!", 
            "subreddit": "statistics", 
            "title": "Alternatives to AUC", 
            "url": "https://www.reddit.com/r/statistics/comments/6ydogt/alternatives_to_auc/"
        }, 
        {
            "author": "HSMatt", 
            "created_utc": 1504653105.0, 
            "domain": "self.statistics", 
            "id": "6ybqil", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6ybqil/looking_for_crash_course_in_standard_deviations/", 
            "score": 19, 
            "selftext": "I've built a website for players of a collectible card game to track their match stats and compare those with the general population. A player chooses their deck each matchup is against a random opponent with their own deck. \n\nIn a sample world, on my site, a player would log a match where they played Deck A against Deck B and won.\n\nThen the aggregated data would show that Deck A is 40% of all decks played and has a win % of 50% against another Deck A (by default), a 55% chance against Deck B and 48% against Deck C. Then Deck B is 30% of \"the meta\" and has a 60% win % against Deck C.\n\nNow, I know how to calculate an expected win % based on that data (and I vaguely understand that those \"meta\" percentages should move towards equilibrium, but the nature of the game makes people irrational actors).\n\nHowever, the object isn't always to maximize win percentage. There are 15 round tournaments where, in order to win, one must first advance to the top 8, which typically requires a 12-3 record or better. Having a 67% win rate doesn't amount to much with 0 variance, as a 10-5 record is ultimately as good as an 0-15 record.\n\nMy understanding to this point is that I can calculate how many standard deviations a deck is away from 12 wins as use that as a guide for which deck has the best chance to advance.\n\nHowever, I'm not sure how to convert that to a odds.\n\nHere's is what I've worked through so far:\n\n* I calculated expected win rate and variance for each deck\n\n* I calculated standard deviations to 12 wins with this math:\n\n(12 wins needed - (Win % * 15 rounds))/(variance * 15 rounds)\n\n* I calculated the mean and standard deviation of the set of standard deviations\n\n* I used those to calculate a z-score.\n\nThe fewest standard deviations to 12 wins was 1.10 and that correlated to a z-score of -2.71. Looking at [this chart](https://en.wikipedia.org/wiki/Standard_score#/media/File:Normal_distribution_and_scales.gif), I'm guessing that means something like 1% to advance. I'm not sure how to get there mathematically and I'm not sure if that changes based on the size of the tournament or if the \"meta\" being incorporated in the expected win rate means that it scales perfectly.\n\nSo first, am I on the right track? If so, how do I get to the finish line? And if I'm not... help, please.", 
            "subreddit": "statistics", 
            "title": "Looking for crash course in standard deviations, z-scores and odds", 
            "url": "https://www.reddit.com/r/statistics/comments/6ybqil/looking_for_crash_course_in_standard_deviations/"
        }, 
        {
            "author": "HolisticReductionist", 
            "created_utc": 1504643032.0, 
            "domain": "self.statistics", 
            "id": "6yapkg", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6yapkg/what_is_the_accuracy_of_the_spaghetti_hurricane/", 
            "score": 1, 
            "selftext": "In other words, is there some kind of a statistical curve in which accuracy and temporal distance into the future are inversely related for these?", 
            "subreddit": "statistics", 
            "title": "What is the accuracy of the Spaghetti hurricane models as a function of temporal distance into the future?", 
            "url": "https://www.reddit.com/r/statistics/comments/6yapkg/what_is_the_accuracy_of_the_spaghetti_hurricane/"
        }, 
        {
            "author": "UnderwaterDialect", 
            "created_utc": 1504633402.0, 
            "domain": "self.statistics", 
            "id": "6y9mse", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y9mse/wondering_about_the_best_way_to_see_if_people/", 
            "score": 1, 
            "selftext": "Let's say I present participants with pairs of animals (this isn't what I did in reality, but it will serve as an example). One member of each pair is always a cat, the other member is always a dog.\n\nTheir task is to choose the member of the pair that is the best match for a certain adjective. Let's say there are three different kinds of adjectives: those describing positive physical traits, those describing positive personality traits, and those describing positive mental traits.\n\nMy original intention was to analyze this data with mixed effects logistic regression. That is, I would separate the data into the three different subsets based on which kind of adjective was shown. For each adjective type I would run an intercepts only model in which choosing a dog was coded as 1, and choosing a cat was coded as 0. I would have a random subject intercept and a random item intercept (i.e., which pair they were shown; the same cats and dogs were always paired together). This would tell me if participants were more likely to choose dogs vs. cats as having positive physical traits, positive personality traits and positive mental traits.\n\nI would then also be interested in adding predictors to the model to see how they affect things (e.g., whether the participant is a cat or a dog owner).\n\nBut, this strikes me as a very simple analysis. Is there a better way for me to analyze this data?\n\nIn addition, if this is the best approach, is there a way to see if the additional predictors remove the overall effect? Would their addition making the intercept insignificant tell me this?", 
            "subreddit": "statistics", 
            "title": "Wondering about the best way to see if people were more likely to choose Option A vs Option B, for multiple types of questions.", 
            "url": "https://www.reddit.com/r/statistics/comments/6y9mse/wondering_about_the_best_way_to_see_if_people/"
        }, 
        {
            "author": "Zenom7", 
            "created_utc": 1504632443.0, 
            "domain": "self.statistics", 
            "id": "6y9iwn", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y9iwn/cant_land_an_internship_would_independent/", 
            "score": 10, 
            "selftext": "I'm currently in my last semester of an Applied Statistics M.S. degree at Purdue. I have great grades and a good looking resume and cover letter. However, I have had no luck in landing any sort of practical job experience. I'm thinking of doing a few independent projects to showcase my practical abilities. Do you think this would be effective for landing a job after I graduate? Or do you guys think I should keep trying for internships? Any advice would be appreciated! ", 
            "subreddit": "statistics", 
            "title": "Can't land an internship, would independent projects boost my resume?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y9iwn/cant_land_an_internship_would_independent/"
        }, 
        {
            "author": "frequentlybayes", 
            "created_utc": 1504627100.0, 
            "domain": "self.statistics", 
            "id": "6y8wrm", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y8wrm/what_websites_do_you_read_daily_or_weekly/", 
            "score": 5, 
            "selftext": "Everyone has blogs, news sites, or content aggregators that they follow daily.  I typically follow Hacker News every day, look at Andrew Gelman's blog once or twice a week, and look at Arxiv.org when I remember to.  It's been a while since I've seen this asked on r/statistics and I want to find some new sources of good content to break my normal habits!", 
            "subreddit": "statistics", 
            "title": "What websites do you read daily, or weekly?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y8wrm/what_websites_do_you_read_daily_or_weekly/"
        }, 
        {
            "author": "liftyMcLiftFace", 
            "created_utc": 1504612187.0, 
            "domain": "self.statistics", 
            "id": "6y7gg0", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y7gg0/interpolated_median_but_not/", 
            "score": 0, 
            "selftext": "I have been looking at trying to calculate a median from a function for my distribution of two continuous variables. In this case, personal income by age.\n\nI wish to calculate the median for different age groups but long story short, I cant use the sample data outside of my work environment due to confidentiality and producing medians for every conceivable age breakdown is inconvenient.\n\nIn R I have just been producing a function and integrating the age range of interest and solving for half the integral to find a median/midpoint in that section of the curve. It seems similar to an interpolated median because I can generate a median at an interpolated point on the function rather than using an actual sample value.\n\nI realise I havent created a new method so my question is what the heck is this called ?\n\nI cant seem to find reference to this technique anywhere.\n\nTL;DR - What is it called when you use integrals to estimate the median of your distribution rather than use actual samples?", 
            "subreddit": "statistics", 
            "title": "Interpolated median... but not", 
            "url": "https://www.reddit.com/r/statistics/comments/6y7gg0/interpolated_median_but_not/"
        }, 
        {
            "author": "SacoETrampa", 
            "created_utc": 1504577293.0, 
            "domain": "self.statistics", 
            "id": "6y4xjc", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y4xjc/how_to_intepret_survival_analysis_output/", 
            "score": 1, 
            "selftext": "Hello all. I have been playing around with a STATA breast cancer dataset. I am focusing on the commands: \n\n    clear\n     \n    use http://www.stata-press.com/data/cggm3/bc_compete\n     \n    stset time, failure(status = 1) \n    stcrreg drug age, compete(status = 2)\n    \n    sts list\n\nThe regression gives me the output: \n\n    Competing-risks regression                       No. of obs       =        423\n                                                     No. of subjects  =        423\n    Failure event  : status == 1                     No. failed       =        135\n    Competing event: status == 2                     No. competing    =         78\n                                                     No. censored     =        210\n    \n                                                     Wald chi2(2)     =       5.07\n    Log pseudolikelihood = -794.79099                Prob > chi2      =     0.0794\n    \n    ------------------------------------------------------------------------------\n                 |               Robust\n              _t |        SHR   Std. Err.      z    P>|z|     [95% Conf. Interval]\n    -------------+----------------------------------------------------------------\n            drug |   .6845084   .1215754    -2.13   0.033      .483278    .9695282\n             age |   1.006517   .0107129     0.61   0.542     .9857373    1.027734\n    ------------------------------------------------------------------------------\n\nAnd I am interested in understanding how the coefficients affect what I assume is the baseline survival function ?\n\n\n             failure _d:  status == 1\n       analysis time _t:  time\n    \n               Beg.          Net            Survivor      Std.\n      Time    Total   Fail   Lost           Function     Error     [95% Conf. Int.]\n    -------------------------------------------------------------------------------\n         3      423     38     11             0.9102    0.0139     0.8786    0.9338\n         6      374     35     21             0.8250    0.0186     0.7850    0.8582\n         9      318     27     10             0.7549    0.0214     0.7100    0.7939\n        12      281     20     10             0.7012    0.0230     0.6535    0.7436\n        15      251      2      2             0.6956    0.0231     0.6477    0.7384\n        18      247      2      6             0.6900    0.0233     0.6418    0.7331\n        21      239      2      2             0.6842    0.0234     0.6357    0.7277\n        24      235      0      1             0.6842    0.0234     0.6357    0.7277\n        27      234      1      5             0.6813    0.0235     0.6327    0.7249\n        30      228      1      0             0.6783    0.0236     0.6295    0.7221\n        33      227      0      2             0.6783    0.0236     0.6295    0.7221\n        36      225      2      2             0.6723    0.0238     0.6232    0.7164\n        39      221      1      1             0.6692    0.0239     0.6200    0.7136\n        45      219      1      3             0.6662    0.0240     0.6168    0.7107\n        48      215      2      0             0.6600    0.0241     0.6103    0.7049\n        54      213      0      1             0.6600    0.0241     0.6103    0.7049\n        60      212      1    211             0.6569    0.0242     0.6071    0.7020\n    -------------------------------------------------------------------------------\n\n  Essentially, now that I have estimated the parameters, how do I input data from an individual's characteristics to obtain a probability of him failing in time t ? \n\nThanks !", 
            "subreddit": "statistics", 
            "title": "How to Intepret Survival Analysis Output ?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y4xjc/how_to_intepret_survival_analysis_output/"
        }, 
        {
            "author": "yneos", 
            "created_utc": 1504573691.0, 
            "domain": "self.statistics", 
            "id": "6y4lds", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 3, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y4lds/can_someone_explain_this_risk_statistic/", 
            "score": 1, 
            "selftext": "From https://en.wikipedia.org/wiki/Heart_failure\n\n> In the year after diagnosis the risk of death is about 35% after which it decreases to below 10% each year.\n\nI can read that a few different ways. Does it mean that after one year, the risk is below 10% and continues decreasing?\n\n", 
            "subreddit": "statistics", 
            "title": "Can someone explain this 'risk' statistic?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y4lds/can_someone_explain_this_risk_statistic/"
        }, 
        {
            "author": "jnetelle", 
            "created_utc": 1504571099.0, 
            "domain": "self.statistics", 
            "id": "6y4cj4", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 7, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y4cj4/recommended_statistic_moocs/", 
            "score": 19, 
            "selftext": "Hi. Anyone use MOOCs to as a refresher for statistics? If so, which ones would you recommend? Thanks!", 
            "subreddit": "statistics", 
            "title": "Recommended Statistic MOOCs?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y4cj4/recommended_statistic_moocs/"
        }, 
        {
            "author": "salvia_d", 
            "created_utc": 1504567790.0, 
            "domain": "self.statistics", 
            "id": "6y41p6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Software", 
            "media": null, 
            "num_comments": 8, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y41p6/looking_for_a_good_open_source_stats_program_or/", 
            "score": 4, 
            "selftext": "Thanks for any advice. ", 
            "subreddit": "statistics", 
            "title": "Looking for a good open source stats program or spreadsheet. Any recommendations?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y41p6/looking_for_a_good_open_source_stats_program_or/"
        }, 
        {
            "author": "urmyheartBeatStopR", 
            "created_utc": 1504567066.0, 
            "domain": "self.statistics", 
            "id": "6y3zat", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y3zat/gradient_boosting_machine_book_or_paper/", 
            "score": 1, 
            "selftext": "Title.\n\nStudying decision trees and such. I got random forest and cart down but GBM is a bit difficult would love a book for it.\n\nThanks!\n\nupdate:\n\nsolved.", 
            "subreddit": "statistics", 
            "title": "Gradient Boosting Machine book or paper recommendation?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y3zat/gradient_boosting_machine_book_or_paper/"
        }, 
        {
            "author": "Manonano", 
            "created_utc": 1504563371.0, 
            "domain": "self.statistics", 
            "id": "6y3mbp", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y3mbp/calculating_reliability_of_subjects_standard/", 
            "score": 1, 
            "selftext": "Hi all. I am trying to determine test-retest reliability for a test that involves 6 trials where subjects responses are angle from 0 degrees (ie, +1 degree or -1 degree). I have 35 subjects who completed the test two times (total of 12 trials, 6 trials for each time tested). However, mean response does not give much information on this test; someone who responds +6 and -6 (avg = 0 degrees) will look the same as someone who scored +1 and -1, despite performing completely differently on the task. Likewise, taking \"absolute value\" average performance collapses standard deviations so now someone who scored +6 and -6 looks like someone who scored +6 and +6 (more consistent). Therefore, I cannot rely on the test/retest reliability of the mean; I would like to calculate test/retest reliability of the subjects' standard deviations. All formulas for Standard Error of Measurement / ICC utilize Mean and not standard deviation; does anyone know what equations I can use to estimate a subjects' expected consistency instead of their expected accuracy?\n\nTLDR: Average does not give as much information as standard deviation on a task, would like to calculate/estimate expected test/retest standard deviation, if possible.", 
            "subreddit": "statistics", 
            "title": "Calculating Reliability of Subjects' Standard Deviation on a Test (Not Average)", 
            "url": "https://www.reddit.com/r/statistics/comments/6y3mbp/calculating_reliability_of_subjects_standard/"
        }, 
        {
            "author": "Apolliyon", 
            "created_utc": 1504550561.0, 
            "domain": "self.statistics", 
            "id": "6y2976", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y2976/how_to_handle_data_higher_than_measurement_range/", 
            "score": 2, 
            "selftext": "I have a dataset where one outcome I am measuring is a \"range\" - this will be either a number (between 0.1 and 20 microns), or \"greater than 20 microns\" because the range is bigger than our instrument is able to measure.\n\nHow should I handle this variable statistically? Should I make the entire variable categorical with binning, e.g. '0 to 5 microns', '5 to 10 microns', etc? Or is there a better way to handle this? \n\n(I am a grad student with access to R or other stats tools if need be.) Thank you so much in advance!", 
            "subreddit": "statistics", 
            "title": "How to handle data higher than measurement range", 
            "url": "https://www.reddit.com/r/statistics/comments/6y2976/how_to_handle_data_higher_than_measurement_range/"
        }, 
        {
            "author": "videek", 
            "created_utc": 1504544912.0, 
            "domain": "self.statistics", 
            "id": "6y1mm2", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6y1mm2/i_suck_at_maths_does_this_mean_i_will_suck_at/", 
            "score": 8, 
            "selftext": "I have a background in *medical sciences*. Actually I have an MPharm but given the curriculum I'd place myself more towards the interdisciplinary side of the field.\n\nWhereas I have a little knowledge about statistical inference and dabbled on my own with exploratory statistics, I do not feel I fully understand the underlying theory behind the how and when - I know how and when to apply some statistical methods but lack in understanding why use them and what exactly do their results tell me.\n\nIn the Uni our maths covered the subject up to the differential equations and singular integrals. And whereas I feel at home while doing anything molecular I feel...overwhelmed when I see equations in front of me, even though they represent nothing more or less than the molecular structures and/or formulae - pattern recognition and deduction.\n\nI did start to work on understanding the more advanced topics of maths namely linear algebra series from 3blue1brown but the question whose answer eludes me still is - just how much math does one need to understand in order to understand even basic statistics? Are there any auxiliary branches / competences one should strengthen alongside pure statistics?\n\n\nCheers!", 
            "subreddit": "statistics", 
            "title": "I suck at maths. Does this mean I will suck at statistics as well?", 
            "url": "https://www.reddit.com/r/statistics/comments/6y1mm2/i_suck_at_maths_does_this_mean_i_will_suck_at/"
        }, 
        {
            "author": "mbarnhart14", 
            "created_utc": 1504487640.0, 
            "domain": "self.statistics", 
            "id": "6xx5u6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xx5u6/eli5_how_do_i_interpret_a_likelihood_ratio/", 
            "score": 3, 
            "selftext": "I'm going through research articles and I don't understand what likelihood ratios are nor do I understand positive/negative likelihood ratios. Please help. ", 
            "subreddit": "statistics", 
            "title": "ELI5: How do I interpret a likelihood ratio", 
            "url": "https://www.reddit.com/r/statistics/comments/6xx5u6/eli5_how_do_i_interpret_a_likelihood_ratio/"
        }, 
        {
            "author": "iwontmakeyoursammich", 
            "created_utc": 1504476713.0, 
            "domain": "self.statistics", 
            "id": "6xw6q6", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xw6q6/question_about_equivalence_testing_with/", 
            "score": 3, 
            "selftext": "Hi everyone,\n\nI've been struggling with some analyses for a while and was hoping I could get some advice. I need to run some equivalence tests to test our hypothesis that our IV is not correlated with or predictive of our DV. However, I can't seem to find any info about how to do this when accounting for covariates. Has anyone had any experience with this that could offer some direction? Thanks!", 
            "subreddit": "statistics", 
            "title": "Question about equivalence testing with covariates for partial correlations and regression", 
            "url": "https://www.reddit.com/r/statistics/comments/6xw6q6/question_about_equivalence_testing_with/"
        }, 
        {
            "author": "gwern", 
            "created_utc": 1504473366.0, 
            "domain": "arxiv.org", 
            "id": "6xvuwo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xvuwo/getting_started_with_particle_metropolishastings/", 
            "score": 24, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "\"Getting Started with Particle Metropolis-Hastings for Inference in Nonlinear Dynamical Models\", Dahlin & Sch\u00f6n 2017", 
            "url": "https://arxiv.org/abs/1511.01707"
        }, 
        {
            "author": "hugokhf", 
            "created_utc": 1504457037.0, 
            "domain": "self.statistics", 
            "id": "6xu7ar", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 6, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xu7ar/finding_the_spread_for_binary_result_what_formula/", 
            "score": 10, 
            "selftext": "Sorry I am not able to explain clearly in the title, as I don't really know the term.\n\nSo I have a bunch of binary results (true and false) with X amount of test data.\nExample: method A showed 50% accuracy and method B showed 60% accuracy, but there are only 10 test data. How do I find the 'spread' of each method to say if the difference is actually significant??\n\nThanks, sorry for the noob question", 
            "subreddit": "statistics", 
            "title": "finding the 'spread' for binary result, what 'formula' should i be looking at?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xu7ar/finding_the_spread_for_binary_result_what_formula/"
        }, 
        {
            "author": "the-nerdy-dude", 
            "created_utc": 1504410923.0, 
            "domain": "self.statistics", 
            "id": "6xr261", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 16, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xr261/regression_unadjustednonsignificant_but/", 
            "score": 7, 
            "selftext": "I am working on a paper looking at level of suicidal thoughts  (outcome) by HIV status (positive vs negative). so my outcome is two ways a composite score (range from 0-6, higher=higher level of suicidal thoughts) and I am looking at a yes/no item (have you thought of committing suicide). Basically my research question is are HIV-positive patients more likely to have suicidal thoughts than HIV-negative?\n\nSo for the score, I am using ANOVA/ANCOVA for the score as an outcome variable and  the item I am using Logistic Regression.\n\nFor both cases, the unadjusted model (so predictor is just HIV status), it is non-significant/borderline significant \n\nBut when I adjusted for stuff (Age, sex, education, depression..and more)... HIV status becomes significant.\n\nI am unsure how to explain that. Can someone help me understand what is going on behind SAS that after accounting for covariates, the independent variable became significant? \n\nis this a case of confounding?\n", 
            "subreddit": "statistics", 
            "title": "Regression: Unadjusted=non-significant but adjusted=significant. Explanation?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xr261/regression_unadjustednonsignificant_but/"
        }, 
        {
            "author": "achillesrhyme", 
            "created_utc": 1504407395.0, 
            "domain": "self.statistics", 
            "id": "6xqsvl", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xqsvl/2_separate_regressions_with_the_same_dependent/", 
            "score": 0, 
            "selftext": "I conducted two separate regressions using the same dependent variable but different independent variables. Let's call the independent variable in the first regression as X1 and the independent variable for the second as X2. The regression results provide me with a regression coefficient for X1 in the first regression. And a regression coefficient for X2 at the end of the second regression. Assume the regression coefficient for X1 is 0.02 and the regression coefficient for X2 is 0.003. Can I say that X1 is a better predictor for the dependent variable compared to X2 based on the regression coefficients? Can I push this even further and say that X1 is probably 6 times (0.02/0.003 = ~6) the better predictor compared to X2?\n\n**For further clarification**\n\nI'm essentially trying to see if revenue (dependent variable or y) is affected more by market A (independent variable in the first regression). Or, if it's affected more by market B (independent variable in the second regression). I'm looking at revenue and market potential (in terms of $) values across 13 years to determine this. I know based on industry experience that market 1 and market 2 have some influence on company revenue. I'm wondering if I can use the slopes of each test to make a statement as to whether market 2 has a feebler impact on revenue compared to market 1. Is that reasonable?\n\n**Update - multivariate regression**\n\nI tried doing a multiple regression using market A and market B as independent variables. Noticed that the regression coefficient for market B had a high p-value. Plus the coefficient for market B (while small just like the regular regression) was negative! But the very same coefficient was positive under the regular regression. Market B has a high p-value of 0.92 while Market A has a p-value of 0.047. Can I use this to say that Market A works as a better indicator of the company revenue as opposed to Market B - which happens to be noisy?", 
            "subreddit": "statistics", 
            "title": "2 separate regressions with the same dependent variable but different independent variables, what can I infer from the regression coefficients?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xqsvl/2_separate_regressions_with_the_same_dependent/"
        }, 
        {
            "author": "ecourtney31415", 
            "created_utc": 1504405508.0, 
            "domain": "self.statistics", 
            "id": "6xqnpu", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xqnpu/bracket_that_ranks_all_losers/", 
            "score": 2, 
            "selftext": "I know this may not be exactly what this sub is for but its the only thing I could think of\n I'm looking to make a list ranking around 100 things by competing songs 2 at a time to find an ordered list of my top 100. Does anyone know how to make a bracket/any programs that do this, without me having to manually make like 20 losers brackets. \nThanks ", 
            "subreddit": "statistics", 
            "title": "Bracket that ranks all losers", 
            "url": "https://www.reddit.com/r/statistics/comments/6xqnpu/bracket_that_ranks_all_losers/"
        }, 
        {
            "author": "Scrab22", 
            "created_utc": 1504384801.0, 
            "domain": "self.statistics", 
            "id": "6xotut", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Career Advice", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xotut/how_can_i_tell_if_ill_love_this_field/", 
            "score": 5, 
            "selftext": "I'm going to study Math along with Statistics+Operations Research this year.\n\n**Background:**\nMathematics have always been my strong side. It's a compensation for my weaker language side.\nWhile I don't do Math (pun intended) in my free time, I've always loved it and loved taking tests for the fun and challenge of it.\nI learned programming in high school, and creating the final project independently was difficult so while I didn't end up creating what I had in vision, my project won as one of the 3 best in our school. I've been skeptical about programming, and it's hard to be accepted due to a very high PET demand.\nSo I thought to myself \"Perhaps I will try Statistics+Operations Research where I can also advance to programming and data mining if I wish to\".\n\n**TL;DR:**\nBut aside from articles over the net, this profession is a stranger to me, so I'd like to know the best way to get to know it. If there's any info or personal experiences you can provide me with, I'd be most glad.\n\nThanks", 
            "subreddit": "statistics", 
            "title": "How can I tell if I'll love this field?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xotut/how_can_i_tell_if_ill_love_this_field/"
        }, 
        {
            "author": "miggafoo", 
            "created_utc": 1504370253.0, 
            "domain": "self.statistics", 
            "id": "6xn42d", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xn42d/know_of_any_cool_examples_of_boxplot_usage/", 
            "score": 9, 
            "selftext": "I'm not a statistician and was hoping for trivial/unusual/surprising/fun papers. More specifically, I'm trying to find poor or fantastic usage of boxplots... ", 
            "subreddit": "statistics", 
            "title": "Know of any cool examples of boxplot usage?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xn42d/know_of_any_cool_examples_of_boxplot_usage/"
        }, 
        {
            "author": "SandipanDeyUMBC", 
            "created_utc": 1504311677.0, 
            "domain": "sandipanweb.wordpress.com", 
            "id": "6xilfa", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 1, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xilfa/using_particle_filter_for_tracking_objects_in_2d/", 
            "score": 12, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Using Particle Filter for tracking objects in 2D (in R)", 
            "url": "https://sandipanweb.wordpress.com/2017/09/02/using-particle-filter-for-tracking-objects-in-2d-in-r/"
        }, 
        {
            "author": "adamrossnelson", 
            "created_utc": 1504305169.0, 
            "domain": "self.statistics", 
            "id": "6xi02s", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xi02s/help_finding_examples_solutions/", 
            "score": 4, 
            "selftext": "Hi Folks, anyone out there in statistics land that would know of a good book that has examples and solutions for Bayesian classification. I'm looking for more examples similar to those presented in these videos:\n\nhttps://www.youtube.com/watch?time_continue=5&v=GlmS_jox08s\n\nhttps://www.youtube.com/watch?time_continue=66&v=CNpSrdnYvbo\n\nThanks in advance folks!", 
            "subreddit": "statistics", 
            "title": "Help Finding Examples & Solutions", 
            "url": "https://www.reddit.com/r/statistics/comments/6xi02s/help_finding_examples_solutions/"
        }, 
        {
            "author": "thuja_plicata", 
            "created_utc": 1504302471.0, 
            "domain": "self.statistics", 
            "id": "6xhq8g", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 5, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xhq8g/adjusting_bias_in_random_forest_response_values/", 
            "score": 9, 
            "selftext": "I have a question about the statistical validity of something I'm thinking of doing - it seems too simple to be valid.\n\nRandom forests (and all bagging approaches?) perform fairly poorly in the extremes for continuous prediction, under and over predicting at the high and low end.  This is a natural outcome of their lumping data together.  Understood.  Even with nodesize set to the minimum (1), and fiddling around with other parameters, that tendency is still there due to the random predictors utilized (I assume).\n\nI'd like to continue to use random forests for my problem (tree densities) for other reasons.  But, I'd also like to address this issue.  The predicted values are, as expected, higher than my validation points on the low end (e.g., it predicts 100 trees for a given site, that site only has 80) and lower on the high end (e.g., predicts 2000 trees when there was 2200).\n\nThe plot between predicted and observed is quite linear, running from over prediction at the low end to well predicted in the middle to under prediction on the high end.\n\nThe simplest way seems to be simply fit a linear regression to the predicted vs. observed plot and adjust that way (not extrapolating).  The linear regression gets r2 of >0.95, all the diagnostic plots look great.  \n\nI can then apply the linear model \"adjustment\" to the random forest prediction, which has the effect of mostly eliminating that bias (it's of course not perfect, but better).  It doesn't artificially reduce the variance, which is good.  I'm essentially just adjusting the slope and intercept of the random forest model outputs.\n\nSo what is wrong with that ad hoc sort of approach?  It's not very satisfying for sure, but I'm not finding a better option online - seems like this under/over bias is an inherent part of the method. Is there a better way?\n\n\nEdit:  The question is also posed here by somebody else: https://stats.stackexchange.com/questions/28732/response-distribution-dependent-bias-in-random-forest-regression\n\nThe regression adjustment is called \"naive\" but then sort of walked back.  I guess I'm looking for a second opinion.  I am not extrapolating anything, so it avoids that problem.", 
            "subreddit": "statistics", 
            "title": "Adjusting bias in random forest response values", 
            "url": "https://www.reddit.com/r/statistics/comments/6xhq8g/adjusting_bias_in_random_forest_response_values/"
        }, 
        {
            "author": "Noxty", 
            "created_utc": 1504300128.0, 
            "domain": "self.statistics", 
            "id": "6xhhla", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xhhla/where_to_start_for_workforce_intraday_analysis/", 
            "score": 3, 
            "selftext": "So just graduated from college with my BA in Statistics and Economics, been applying to some jobs, got an offer for a workforce intraday analyst position at a call center. After my meeting/interview I find out that this team is actually just one person doing all this forecasting and analysis which is crazy! Turns out this one person doesn't even have a statistics degree but she is in charge of doing all this analysis. But she knows a decent amount of what she's talking about and what she's doing. I guess the problem is her explaining to me what she wants from me to help her figure out somethings. \n\n\nThinking back at my education I don't think we ever talked about intraday analysis, but I get it, your just essentially forecasting call volume to determine how to schedule our workers on a daily basis per hour. \n\nAnyways enough rambling, I just wanted to post here to see what kind of resources can I find online to help me on this adventure into intraday analysis. Like I definitely want to do my own studying about it at home so that way I can impress the company.", 
            "subreddit": "statistics", 
            "title": "Where to start for workforce intraday analysis?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xhhla/where_to_start_for_workforce_intraday_analysis/"
        }, 
        {
            "author": "analoveschocolate", 
            "created_utc": 1504291529.0, 
            "domain": "self.statistics", 
            "id": "6xgklz", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xgklz/data_in_one_of_my_treatment_conditions_is_not/", 
            "score": 1, 
            "selftext": "Hi guys, \n\nI'm doing a bivariate correlation between the delta of 2 variables and I have 2 treatment groups. The deltas I'm using are from the original values (Time A and Time B). Time A in one of my treatment groups is not normally distributed and I want to see how my results would change if it was normally distributed. So I take it that that would require a log transformation.\n\nI'm using SPSS and the deltas were calculated from excel and then uploaded into SPSS. So not sure if I would have to do a log transformation on excel? \n\nI'm new to R but I can also handle this in R if someone could guide me.\n\nThanks. \n\n", 
            "subreddit": "statistics", 
            "title": "Data in one of my treatment conditions is not normally distributed. Question about how to handle this", 
            "url": "https://www.reddit.com/r/statistics/comments/6xgklz/data_in_one_of_my_treatment_conditions_is_not/"
        }, 
        {
            "author": "Jmzwck", 
            "created_utc": 1504291513.0, 
            "domain": "self.statistics", 
            "id": "6xgkjf", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 9, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xgkjf/what_is_the_expectation_of_y_given_a_joint/", 
            "score": 5, 
            "selftext": "Just failed an exam from this...I know how to calculate E(Y) given the pdf of y, but what if you are given a joint distribution of X and Y? I calculated the marginal densities of X and Y and noticed they are not independent (since their product did not equal the joint density), so it did not seem to me that it would make sense to calculate E(Y) by taking E(marginal Y)...since Y has a dependency on X. So I tried to take E(Y) by taking int (joint pdf * y) dy - but apparently this is wrong. \n\nNo internet searches are proving helpful either. It's just people asking about how to find the expectation of a joint density.\n", 
            "subreddit": "statistics", 
            "title": "What is the expectation of Y given a joint distribution of f(X,Y)?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xgkjf/what_is_the_expectation_of_y_given_a_joint/"
        }, 
        {
            "author": "summerwinterautumn", 
            "created_utc": 1504285668.0, 
            "domain": "self.statistics", 
            "id": "6xfx5y", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 2, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xfx5y/best_tests_for_exploratory_data/", 
            "score": 6, 
            "selftext": "I am in the social/behavioral sciences and am curious for those that have done exploratory studies, what statistical tests do you commonly use and why?", 
            "subreddit": "statistics", 
            "title": "Best tests for EXPLORATORY data?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xfx5y/best_tests_for_exploratory_data/"
        }, 
        {
            "author": "gmh1977", 
            "created_utc": 1504260588.0, 
            "domain": "self.statistics", 
            "id": "6xdlix", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xdlix/is_the_central_limit_thereon_correct_application/", 
            "score": 3, 
            "selftext": "I have have run a AB Test and identified independence using a CHI Squared test.  Now that  I have a result. I have been asked to give a probability on whether the results of the AB test will be lower than the findings.  So, essentially, the AB test revealed 62% conversion rate for a population of 80333. IF this variation is released on 500,000 customers what is the probability that the version will be under 62%.", 
            "subreddit": "statistics", 
            "title": "Is the Central Limit Thereon Correct application", 
            "url": "https://www.reddit.com/r/statistics/comments/6xdlix/is_the_central_limit_thereon_correct_application/"
        }, 
        {
            "author": "ImInTheFutureAlso", 
            "created_utc": 1504232195.0, 
            "domain": "self.statistics", 
            "id": "6xbmk7", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Statistics Question", 
            "media": null, 
            "num_comments": 0, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6xbmk7/manova_or_multiple_regression/", 
            "score": 1, 
            "selftext": "Hi everybody! I'm not sure I should use MANOVA or multiple regression.\n\nI'm looking at how four different health behaviors change after an intervention is administered. I have pre and post measures of the health behaviors. I think any change in behavior is likely related to changes in psychological symptoms. I've given one measure for this. I thought I would use MANOVA because I have one independent variable (the psych measure) and four dependent variables (the health behaviors). I have made a \"change variable\" for the psych measure by subtracting pre scores from post scores to get one number, which is how much scores on the measure changed. I planned to use this change variable as the independent variable in my analysis.\n\nIt feels like I'm on the right rack, but something isn't quite right here. Should I be using regression instead? Conceptually, I want to predict change in health behaviors based on how the psych measure changed (i.e., people who improved on the psych measure show decreases in the health behaviors, while people who didn't get better don't show those changes).\n\nThank you for help and advice!\n\ncross-post r/askstatistics", 
            "subreddit": "statistics", 
            "title": "MANOVA or multiple regression?", 
            "url": "https://www.reddit.com/r/statistics/comments/6xbmk7/manova_or_multiple_regression/"
        }
    ], 
    "subreddit_creation_utc": 1205473198.0, 
    "subscribers": 44071, 
    "title": "statistics", 
    "title_word_count_occurrences": {
        " r ": 4, 
        "data science": 3, 
        "deep learning": 1, 
        "dell": 1, 
        "facebook": 1, 
        "google": 1, 
        "ios": 4, 
        "perl": 1, 
        "python": 2, 
        "sql": 1, 
        "tex": 3
    }, 
    "top_score_submissions": [
        {
            "author": "oreo_fanboy", 
            "created_utc": 1505271827.0, 
            "domain": "replicationindex.wordpress.com", 
            "id": "6zryqo", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 15, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zryqo/daniel_kahneman_replies_to_a_critique_of_studies/", 
            "score": 81, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "Daniel Kahneman replies to a critique of studies on priming", 
            "url": "https://replicationindex.wordpress.com/2017/02/02/reconstruction-of-a-train-wreck-how-priming-research-went-of-the-rails/#comment-1454"
        }, 
        {
            "author": "qabadai", 
            "created_utc": 1506496724.0, 
            "domain": "self.statistics", 
            "id": "72qon9", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 20, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/72qon9/lpt_everything_you_do_must_be_reproducible/", 
            "score": 79, 
            "selftext": "I don't really come from a statistics/data science background, and don't have a lot of formal training outside online classes, but have ended up in a position where I do a ton of data analysis on a daily basis. And goddamn do I wish someone sat me down and explained this to me sooner.\n\nThe reality is it is SO EASY to mess something up and end up with data that looks right but isn't. Forgetting to enable the weights in a survey, messing up the filter in a SQL query, using count instead of distinct count in a pivot table, etc.\n\nI've had clients come back to me four months after a project saying they got different results when they ran the data. Fuck if I know, time to spend 5 hours re-running everything. SAVE YOUR WORK.\n\nI once got to the end of 6 weeks worth of data runs and analyses for a client. Went to double check the data in the final report and couldn't reproduce it. I had no idea what I had run and didn't know if I messed up early or was messing up now. Turns out it was earlier and had to re-do everything. SAVE YOUR WORK.\n\nMost of my day to day work is in excel and SPSS. Unfortunately that means a lot of it is in GUIs that can't be reproduced, so I've been slowly integrating more and more R.\n\nI know is one of things that most of you are going to be like \"duh,\" but on a deadline or just doing exploratory analysis, it's so easy to take shortcuts and forget to do it.", 
            "subreddit": "statistics", 
            "title": "LPT: Everything you do must be reproducible", 
            "url": "https://www.reddit.com/r/statistics/comments/72qon9/lpt_everything_you_do_must_be_reproducible/"
        }, 
        {
            "author": "ChesterEnergy", 
            "created_utc": 1506087150.0, 
            "domain": "fivethirtyeight.com", 
            "id": "71qzud", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Research/Article", 
            "media": null, 
            "num_comments": 22, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/71qzud/the_media_has_a_probability_problem/", 
            "score": 75, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "The Media Has A Probability Problem", 
            "url": "https://fivethirtyeight.com/features/the-media-has-a-probability-problem/"
        }, 
        {
            "author": "mathmare", 
            "created_utc": 1505113206.0, 
            "domain": "theatlantic.com", 
            "id": "6zdt5z", 
            "is_reddit_media_domain": false, 
            "is_self": false, 
            "is_video": false, 
            "link_flair_css_class": null, 
            "link_flair_text": null, 
            "media": null, 
            "num_comments": 4, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/6zdt5z/when_correlation_is_not_causation_but_something/", 
            "score": 70, 
            "selftext": "", 
            "subreddit": "statistics", 
            "title": "When Correlation Is Not Causation, But Something Much More Screwy", 
            "url": "https://www.theatlantic.com/business/archive/2012/05/when-correlation-is-not-causation-but-something-much-more-screwy/256918/"
        }, 
        {
            "author": "wordboyhere", 
            "created_utc": 1505441414.0, 
            "domain": "self.statistics", 
            "id": "706y0f", 
            "is_reddit_media_domain": false, 
            "is_self": true, 
            "is_video": false, 
            "link_flair_css_class": "", 
            "link_flair_text": "Discussion", 
            "media": null, 
            "num_comments": 153, 
            "num_crossposts": 0, 
            "permalink": "/r/statistics/comments/706y0f/what_is_a_controversial_opinion_that_you_have/", 
            "score": 62, 
            "selftext": "Anything vaguely related to statistics. ", 
            "subreddit": "statistics", 
            "title": "What is a controversial opinion that you have?", 
            "url": "https://www.reddit.com/r/statistics/comments/706y0f/what_is_a_controversial_opinion_that_you_have/"
        }
    ], 
    "total_submissions": 247, 
    "utc_of_data_collection_completion": "2017-10-16 18:52:39", 
    "utc_of_data_collection_start": "2017-10-16 18:52:37"
}